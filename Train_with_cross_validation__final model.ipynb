{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IfQ3xP6-EY5r"
      },
      "source": [
        "# **Preparation**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpy4A1Vpi9jH"
      },
      "source": [
        "## Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PWlbrkMCx-W-"
      },
      "outputs": [],
      "source": [
        "# !pip install -q opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KTkttSWnUi1Q"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import cv2\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import tqdm\n",
        "import math\n",
        "import numpy.linalg as LA\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dH_yWnJ9QRLs"
      },
      "source": [
        "## Code to run pose estimation using MoveNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUGTJ5XRTP7W",
        "outputId": "14cbe41b-8f46-4554-8b78-5e900a78b735"
      },
      "outputs": [],
      "source": [
        "# # Download model from TF Hub and check out inference code from GitHub\n",
        "# # Model movenet\n",
        "# !wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "\n",
        "# # Github tensorflow (có chứa thư viện utils, data, ml...)\n",
        "# !git clone https://github.com/tensorflow/examples.git\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aN-W7SEpnLoe"
      },
      "source": [
        "#### Load MoveNet Thunder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "48kW1c2F5l1R"
      },
      "outputs": [],
      "source": [
        "from pose_estimation import utils\n",
        "from pose_estimation.data import BodyPart\n",
        "from pose_estimation.ml import Movenet\n",
        "movenet = Movenet('movenet_thunder')\n",
        "\n",
        "# Gọi hàm nhận dạng của movenet nhiều lần để cải thiện độ chính xác nhận dạng khung xương\n",
        "def detect(input_tensor, inference_count=3):\n",
        "  \"\"\"Runs detection on an input image.\n",
        " \n",
        "  Args:\n",
        "    input_tensor: A [height, width, 3] Tensor of type tf.float32.\n",
        "    inference_count: Số lần lặp.\n",
        " \n",
        "  Returns:\n",
        "    A Person entity detected by the MoveNet.SinglePose.\n",
        "  \"\"\"\n",
        "  image_height, image_width, channel = input_tensor.shape\n",
        " \n",
        "  # Detect pose using the full input image\n",
        "  movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
        " \n",
        "  # Repeatedly using previous detection result to identify the region of\n",
        "  # interest and only croping that region to improve detection accuracy\n",
        "  for _ in range(inference_count - 1):\n",
        "    person = movenet.detect(input_tensor.numpy(), \n",
        "                            reset_crop_region=False)\n",
        "\n",
        "\n",
        "  return person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgPath = \"./Image/_/output4_0330.png\"\n",
        "image = tf.io.read_file(imgPath)\n",
        "img = tf.io.decode_jpeg(image)\n",
        "pers = detect(img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pNiIeG5PnUHm"
      },
      "source": [
        "#### Trực quan hoá kết quả nhận dạng khung xương"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fKo0NzwQJ5Rm"
      },
      "outputs": [],
      "source": [
        "def draw_prediction_on_image(\n",
        "    image, person, crop_region=None, close_figure=True,\n",
        "    keep_input_size=False):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        " \n",
        "  Args:\n",
        "    image: An numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    person: A person entity returned from the MoveNet.SinglePose model.\n",
        "    close_figure: Whether to close the plt figure after the function returns.\n",
        "    keep_input_size: Whether to keep the size of the input image.\n",
        " \n",
        "  Returns:\n",
        "    An numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  # Draw the detection result on top of the image.\n",
        "  image_np = utils.visualize(image, [person])\n",
        "  \n",
        "  # Plot the image with detection results.\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  im = ax.imshow(image_np)\n",
        " \n",
        "  if close_figure:\n",
        "    plt.close(fig)\n",
        " \n",
        "  if not keep_input_size:\n",
        "    image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))\n",
        "\n",
        "  return image_np"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dVn3fgInndqY"
      },
      "source": [
        "#### Code to load the images, detect pose landmarks and save them into a CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QUkOW_26S6K-"
      },
      "outputs": [],
      "source": [
        "class MoveNetPreprocessor(object): \n",
        "  def __init__(self,\n",
        "               images_in_folder,\n",
        "               images_out_folder,\n",
        "               csvs_out_path):\n",
        "    \"\"\"Creates a preprocessor to detection pose from images and save as CSV.\n",
        "\n",
        "    Args:\n",
        "      images_in_folder: Path to the folder with the input images. It has structure:\n",
        "        \n",
        "        train/test\n",
        "        |__ humpbacked\n",
        "            |______ 00000128.jpg\n",
        "            |______ 00000181.bmp\n",
        "            |______ ...\n",
        "        |__ leaning back\n",
        "            |______ 00000243.jpg\n",
        "            |______ 00000306.jpg\n",
        "            |______ ...\n",
        "        ...\n",
        "\n",
        "      images_out_folder: Path to write detected landmarks\n",
        "\n",
        "      csvs_out_path: Path to write the CSV containing the detected landmark\n",
        "        coordinates and label of each image that can be used to train a pose\n",
        "        classification model.\n",
        "    \"\"\"\n",
        "    self._images_in_folder = images_in_folder\n",
        "    self._images_out_folder = images_out_folder\n",
        "    self._csvs_out_path = csvs_out_path\n",
        "    self._messages = []\n",
        "\n",
        "    # Create a temp dir to store the pose CSVs per class\n",
        "    self._csvs_out_folder_per_class = tempfile.mkdtemp()\n",
        " \n",
        "    # Get list of pose classes\n",
        "    self._pose_class_names = sorted(\n",
        "        [n for n in os.listdir(self._images_in_folder)]\n",
        "        )\n",
        "    \n",
        "  def process(self, per_pose_class_limit=None, detection_threshold=0.1):\n",
        "    \"\"\"Preprocesses images in the given folder.\n",
        "    Args:\n",
        "      per_pose_class_limit: Number of images to load in each class directory. \n",
        "      detection_threshold: Only keep images with all landmark confidence score\n",
        "        above this threshold.\n",
        "    \"\"\"\n",
        "    # Loop through the class directory and preprocess its images\n",
        "    for pose_class_name in self._pose_class_names:\n",
        "      print('Preprocessing', pose_class_name, file=sys.stderr)\n",
        "\n",
        "      # Paths for the pose class.\n",
        "      images_in_folder = os.path.join(self._images_in_folder, pose_class_name)    # Tạo đường dẫn\n",
        "      images_out_folder = os.path.join(self._images_out_folder, pose_class_name)\n",
        "      csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
        "                                  pose_class_name + '.csv')\n",
        "      if not os.path.exists(images_out_folder):\n",
        "        os.makedirs(images_out_folder)\n",
        " \n",
        "      # Detect landmarks in each image and write it to a CSV file\n",
        "      with open(csv_out_path, 'w') as csv_out_file:\n",
        "        csv_out_writer = csv.writer(csv_out_file, \n",
        "                                    delimiter=',', \n",
        "                                    quoting=csv.QUOTE_MINIMAL)\n",
        "        # Get list of images\n",
        "        image_names = sorted(\n",
        "            [n for n in os.listdir(images_in_folder) if not n.startswith('.')])\n",
        "        if per_pose_class_limit is not None:\n",
        "          image_names = image_names[:per_pose_class_limit]\n",
        "\n",
        "        valid_image_count = 0\n",
        " \n",
        "        # Detect pose landmarks from each image\n",
        "        for image_name in tqdm.tqdm(image_names):\n",
        "          image_path = os.path.join(images_in_folder, image_name)\n",
        "\n",
        "          try:\n",
        "            image = tf.io.read_file(image_path)\n",
        "            image = tf.io.decode_jpeg(image)\n",
        "          except:\n",
        "            self._messages.append('Skipped ' + image_path + '. Invalid image.')\n",
        "            continue\n",
        "          else:\n",
        "            image = tf.io.read_file(image_path)\n",
        "            image = tf.io.decode_jpeg(image)\n",
        "            image_height, image_width, channel = image.shape\n",
        "          \n",
        "          # Skip images that isn't RGB because Movenet requires RGB images\n",
        "          if channel != 3:\n",
        "            self._messages.append('Skipped ' + image_path +\n",
        "                                  '. Image isn\\'t in RGB format.')\n",
        "            continue\n",
        "          person = detect(image)\n",
        "          \n",
        "          # Save landmarks if all landmarks were detected\n",
        "          min_landmark_score = min(\n",
        "              [keypoint.score for keypoint in person.keypoints])\n",
        "          should_keep_image = min_landmark_score >= detection_threshold\n",
        "          if not should_keep_image:\n",
        "            self._messages.append('Skipped ' + image_path +\n",
        "                                  '. No pose was confidentlly detected.')\n",
        "            continue\n",
        "\n",
        "          valid_image_count += 1\n",
        "\n",
        "          # Draw the prediction result on top of the image for debugging later\n",
        "          output_overlay = draw_prediction_on_image(\n",
        "              image.numpy().astype(np.uint8), person, \n",
        "              close_figure=True, keep_input_size=True)\n",
        "        \n",
        "          # Write detection result into an image file\n",
        "          output_frame = cv2.cvtColor(output_overlay, cv2.COLOR_RGB2BGR)\n",
        "          cv2.imwrite(os.path.join(images_out_folder, image_name), output_frame)\n",
        "        \n",
        "          # Get landmarks and scale it to the same size as the input image\n",
        "          pose_landmarks = np.array(\n",
        "              [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]\n",
        "                for keypoint in person.keypoints],\n",
        "              dtype=np.float32)\n",
        "\n",
        "          # Write the landmark coordinates to its per-class CSV file\n",
        "          coordinates = pose_landmarks.flatten().astype(str).tolist()\n",
        "          csv_out_writer.writerow([image_name] + coordinates)\n",
        "\n",
        "        if not valid_image_count:\n",
        "          raise RuntimeError(\n",
        "              'No valid images found for the \"{}\" class.'\n",
        "              .format(pose_class_name))\n",
        "      \n",
        "    # Print the error message collected during preprocessing.\n",
        "    print('\\n'.join(self._messages))\n",
        "\n",
        "    # Combine all per-class CSVs into a single output file\n",
        "    all_landmarks_df = self._all_landmarks_as_dataframe()\n",
        "    all_landmarks_df.to_csv(self._csvs_out_path, index=False)\n",
        "\n",
        "  def class_names(self):\n",
        "    \"\"\"List of classes found in the training dataset.\"\"\"\n",
        "    return self._pose_class_names\n",
        "  \n",
        "  def _all_landmarks_as_dataframe(self):\n",
        "    \"\"\"Merge all per-class CSVs into a single dataframe.\"\"\"\n",
        "    total_df = None\n",
        "    for class_index, class_name in enumerate(self._pose_class_names):\n",
        "      csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
        "                                  class_name + '.csv')\n",
        "      per_class_df = pd.read_csv(csv_out_path, header=None)\n",
        "      \n",
        "      # Add the labels\n",
        "      per_class_df['class_no'] = [class_index]*len(per_class_df)\n",
        "      per_class_df['class_name'] = [class_name]*len(per_class_df)\n",
        "\n",
        "      # Append the folder name to the filename column (first column)\n",
        "      per_class_df[per_class_df.columns[0]] = (os.path.join(class_name, '') \n",
        "        + per_class_df[per_class_df.columns[0]].astype(str))\n",
        "\n",
        "      if total_df is None:\n",
        "        # For the first class, assign its data to the total dataframe\n",
        "        total_df = per_class_df\n",
        "      else:\n",
        "        # Concatenate each class's data into the total dataframe\n",
        "        total_df = pd.concat([total_df, per_class_df], axis=0)\n",
        " \n",
        "    list_name = [[bodypart.name + '_x', bodypart.name + '_y', \n",
        "                  bodypart.name + '_score'] for bodypart in BodyPart] \n",
        "    \n",
        "    # Đổi tên cột dataframe\n",
        "    header_name = []\n",
        "    for columns_name in list_name:\n",
        "      header_name += columns_name\n",
        "    header_name = ['file_name'] + header_name\n",
        "    header_map = {total_df.columns[i]: header_name[i] \n",
        "                  for i in range(len(header_name))}\n",
        " \n",
        "    total_df.rename(header_map, axis=1, inplace=True)\n",
        "\n",
        "    return total_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YvA67LZDnijz"
      },
      "source": [
        "#### Test pose estimate"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L24GWhgo4WAl"
      },
      "source": [
        "# **Part 1: Preprocess the input images**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TJXSR2CQhm-z"
      },
      "source": [
        "### Upload your own pose dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iEnjgeKeS_VP"
      },
      "outputs": [],
      "source": [
        "dataset_is_split = True #@param [\"False\", \"True\"] {type:\"raw\"}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YiqF3sRf3LLC"
      },
      "source": [
        "\n",
        "  If you've already split your dataset into train and test sets, then set `dataset_is_split` to **True**. \n",
        "  That is, your images folder must include \"train\" and \"test\" directories like this:\n",
        "```\n",
        "    sittingPoses/\n",
        "    |__ train/\n",
        "        |__ humpbacked/\n",
        "            |______ 00000128.jpg\n",
        "            |______ ...\n",
        "        |__ leaning back/\n",
        "            |______ 00000180.jpg\n",
        "            |______ ...\n",
        "    |__ test/\n",
        "        |__ leaning back/\n",
        "            |______ 00000181.jpg\n",
        "            |______ ...\n",
        "```\n",
        "\n",
        "Or, if your dataset is NOT split yet, then set\n",
        "`dataset_is_split` to **False** and we'll split it up based\n",
        "on a specified split fraction. That is, your uploaded images\n",
        "folder should look like this:\n",
        "\n",
        "```    \n",
        "    sittingPoses/\n",
        "    |__ humpbacked/\n",
        "        |______ 00000128.jpg\n",
        "        |______ 00000181.jpg\n",
        "        |______ ...\n",
        "    |__ leaning back/\n",
        "        |______ 00000243.jpg\n",
        "        |______ 00000306.jpg\n",
        "        |______ ...\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BoXaN5nUeU8d"
      },
      "source": [
        "### Phân chia tập dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "joAHy_r62dsI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_into_train_test(images_origin, images_dest, test_split):\n",
        "  \"\"\"Splits a directory of sorted images into training and test sets.\n",
        "\n",
        "  Args:\n",
        "    images_origin: Path to the directory with your images. This directory\n",
        "      must include subdirectories for each of your labeled classes.\n",
        "\n",
        "    images_dest: Path to a directory where you want the split dataset to be\n",
        "      saved\n",
        "\n",
        "    test_split: Fraction of data to reserve for test (float between 0 and 1).\n",
        "  \"\"\"\n",
        "  _, dirs, _ = next(os.walk(images_origin))\n",
        "\n",
        "  TRAIN_DIR = os.path.join(images_dest, 'train')\n",
        "  TEST_DIR = os.path.join(images_dest, 'test')\n",
        "  os.makedirs(TRAIN_DIR, exist_ok=True)\n",
        "  os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "  for dir in dirs:\n",
        "    # Get all filenames for this dir, filtered by filetype\n",
        "    filenames = os.listdir(os.path.join(images_origin, dir))\n",
        "    filenames = [os.path.join(images_origin, dir, f) for f in filenames if (\n",
        "        f.endswith('.png') or f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.bmp'))]\n",
        "    # Shuffle the files, deterministically\n",
        "    filenames.sort()\n",
        "    random.seed(42)\n",
        "    random.shuffle(filenames)\n",
        "    # Divide them into train/test dirs\n",
        "    os.makedirs(os.path.join(TEST_DIR, dir), exist_ok=True)\n",
        "    os.makedirs(os.path.join(TRAIN_DIR, dir), exist_ok=True)\n",
        "    test_count = int(len(filenames) * test_split)\n",
        "    for i, file in enumerate(filenames):\n",
        "      if i < test_count:\n",
        "        destination = os.path.join(TEST_DIR, dir, os.path.split(file)[1])\n",
        "      else:\n",
        "        destination = os.path.join(TRAIN_DIR, dir, os.path.split(file)[1])\n",
        "      shutil.copyfile(file, destination)\n",
        "    print(f'Moved {test_count} of {len(filenames)} from class \"{dir}\" into test.')\n",
        "  print(f'Your split dataset is in \"{images_dest}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IfpNIjAmR0lp"
      },
      "outputs": [],
      "source": [
        "# You can leave the rest alone:\n",
        "dataset_in = './image_new'\n",
        "if not os.path.isdir(dataset_in):\n",
        "  raise Exception(\"dataset_in is not a valid directory\")\n",
        "if dataset_is_split:\n",
        "  IMAGES_ROOT = dataset_in\n",
        "else:\n",
        "  dataset_out = dataset_in + '_split'\n",
        "  split_into_train_test(dataset_in, dataset_out, test_split=0.2)\n",
        "  IMAGES_ROOT = dataset_out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IPkTA5-sNF7W"
      },
      "source": [
        "**Note:** If you're using `split_into_train_test()` to split the dataset, it expects all images to be PNG, JPEG, or BMP—it ignores other file types."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Process single dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "isProcessed = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "images_out_test_folder = 'Nhat_pose'\n",
        "\n",
        "if isProcessed: pass\n",
        "else:\n",
        "    images_in_test_folder = os.path.join(IMAGES_ROOT, 'Nhat')\n",
        "    csvs_out_test_path = 'N.csv'\n",
        "\n",
        "    preprocessor = MoveNetPreprocessor(\n",
        "        images_in_folder=images_in_test_folder,\n",
        "        images_out_folder=images_out_test_folder,\n",
        "        csvs_out_path=csvs_out_test_path,\n",
        "    )\n",
        "\n",
        "    preprocessor.process(per_pose_class_limit=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "images_out_test_folder = 'Nhi_pose'\n",
        "\n",
        "if isProcessed: pass\n",
        "else:\n",
        "    images_in_test_folder = os.path.join(IMAGES_ROOT, 'Nhi')\n",
        "    csvs_out_test_path = 'NH.csv'\n",
        "\n",
        "    preprocessor = MoveNetPreprocessor(\n",
        "        images_in_folder=images_in_test_folder,\n",
        "        images_out_folder=images_out_test_folder,\n",
        "        csvs_out_path=csvs_out_test_path,\n",
        "    )\n",
        "\n",
        "    preprocessor.process(per_pose_class_limit=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "images_out_test_folder = 'Hoan_pose'\n",
        "\n",
        "if isProcessed: pass\n",
        "else:\n",
        "    images_in_test_folder = os.path.join(IMAGES_ROOT, 'Hoan')\n",
        "    csvs_out_test_path = 'H.csv'\n",
        "\n",
        "    preprocessor = MoveNetPreprocessor(\n",
        "        images_in_folder=images_in_test_folder,\n",
        "        images_out_folder=images_out_test_folder,\n",
        "        csvs_out_path=csvs_out_test_path,\n",
        "    )\n",
        "\n",
        "    preprocessor.process(per_pose_class_limit=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "images_out_test_folder = 'Khoi_pose'\n",
        "\n",
        "if isProcessed: pass\n",
        "else:\n",
        "    images_in_test_folder = os.path.join(IMAGES_ROOT, 'Khoi')\n",
        "    csvs_out_test_path = 'K.csv'\n",
        "\n",
        "    preprocessor = MoveNetPreprocessor(\n",
        "        images_in_folder=images_in_test_folder,\n",
        "        images_out_folder=images_out_test_folder,\n",
        "        csvs_out_path=csvs_out_test_path,\n",
        "    )\n",
        "\n",
        "    preprocessor.process(per_pose_class_limit=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UevEKViRT_6J"
      },
      "source": [
        "# **Part 2: Train a pose classification model that takes the landmark coordinates as input, and output the predicted labels.**\n",
        "\n",
        "You'll build a TensorFlow model that takes the landmark coordinates and predicts the pose class that the person in the input image performs. The model consists of two submodels:\n",
        "\n",
        "* Submodel 1 calculates a pose embedding (a.k.a feature vector) from the detected landmark coordinates.\n",
        "* Submodel 2 feeds pose embedding through several `Dense` layer to predict the pose class.\n",
        "\n",
        "You'll then train the model based on the dataset that were preprocessed in part 1."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iGMcoSwLwRSD"
      },
      "source": [
        "### Load the preprocessed CSVs into `train` and `test` variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pOUcc8EL5rrj"
      },
      "outputs": [],
      "source": [
        "def load_pose_landmarks(csv_path):\n",
        "  \"\"\"Loads a CSV created by MoveNetPreprocessor.\n",
        "  \n",
        "  Returns:\n",
        "    X: Detected landmark coordinates and scores of shape (N, 17 * 3)\n",
        "    y: Ground truth labels of shape (N, label_count)\n",
        "    classes: The list of all class names found in the dataset\n",
        "    dataframe: The CSV loaded as a Pandas dataframe features (X) and ground\n",
        "      truth labels (y) to use later to train a pose classification model.\n",
        "  \"\"\"\n",
        "  # Load the CSV file\n",
        "  dataframe = pd.DataFrame()\n",
        "  if type(csv_path) is np.ndarray:\n",
        "    for path in csv_path:\n",
        "      dataframe = pd.concat([dataframe, pd.read_csv(path)],\n",
        "                            axis=0, ignore_index=True)\n",
        "  else:\n",
        "    dataframe = pd.read_csv(csv_path)\n",
        "\n",
        "  df_to_process = dataframe.copy()\n",
        "\n",
        "  # Drop the file_name columns as you don't need it during training.\n",
        "  df_to_process.drop(columns=['file_name'], inplace=True)\n",
        "\n",
        "  # Extract the list of class names\n",
        "  classes = df_to_process.pop('class_name').unique()\n",
        "\n",
        "  # Extract the labels\n",
        "  y = df_to_process.pop('class_no')\n",
        "\n",
        "  # Convert the input features and labels into the correct format for training.\n",
        "  X = df_to_process.astype('float64')\n",
        "  y = keras.utils.to_categorical(y)\n",
        "\n",
        "  return X, y, classes, dataframe"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ydb-bd_UWXMq"
      },
      "source": [
        "### Define functions to convert the pose landmarks to a pose embedding (a.k.a. feature vector) for pose classification\n",
        "\n",
        "Next, convert the landmark coordinates to a feature vector by:\n",
        "1. Moving the pose center to the origin.\n",
        "2. Scaling the pose so that the pose size becomes 1\n",
        "3. Flattening these coordinates into a feature vector\n",
        "\n",
        "Then use this feature vector to train a neural-network based pose classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HgQMdfeT65Z5"
      },
      "outputs": [],
      "source": [
        "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
        "    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
        "\n",
        "    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
        "    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
        "    center = left * 0.5 + right * 0.5\n",
        "    return center\n",
        "\n",
        "\n",
        "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
        "    \"\"\"Calculates pose size.\n",
        "\n",
        "    It is the maximum of two values:\n",
        "      * Torso size multiplied by `torso_size_multiplier`\n",
        "      * Maximum distance from pose center to any pose landmark\n",
        "    \"\"\"\n",
        "    # Hips center\n",
        "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP,\n",
        "                                   BodyPart.RIGHT_HIP)\n",
        "\n",
        "    # Shoulders center\n",
        "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
        "                                        BodyPart.RIGHT_SHOULDER)\n",
        "\n",
        "    # Torso size as the minimum body size\n",
        "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
        "\n",
        "    # Pose center\n",
        "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP,\n",
        "                                       BodyPart.RIGHT_HIP)\n",
        "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
        "    # Broadcast the pose center to the same size as the landmark vector to\n",
        "    # perform substraction\n",
        "    pose_center_new = tf.broadcast_to(pose_center_new,\n",
        "                                      [tf.size(landmarks) // (17*2), 17, 2])\n",
        "\n",
        "    # Dist to pose center\n",
        "    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
        "                  name=\"dist_to_pose_center\")\n",
        "    # Max dist to pose center\n",
        "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
        "\n",
        "    # Normalize scale\n",
        "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
        "\n",
        "    return pose_size\n",
        "\n",
        "\n",
        "def feature_pose(landmarks):\n",
        "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
        "    scaling it to a constant pose size.\n",
        "    \"\"\"\n",
        "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP,\n",
        "                                BodyPart.RIGHT_HIP)\n",
        "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
        "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
        "    # substraction\n",
        "    pose_center = tf.broadcast_to(pose_center,\n",
        "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
        "    landmarks = landmarks - pose_center\n",
        "\n",
        "    # Scale the landmarks to a constant pose size\n",
        "    pose_size = get_pose_size(landmarks)\n",
        "\n",
        "    xVal = tf.gather(landmarks, BodyPart.RIGHT_KNEE.value, axis=1).numpy()[0]\n",
        "    tempList = landmarks.numpy().tolist()\n",
        "\n",
        "    if xVal[0] < 0:\n",
        "        for coordinate in tempList[0]:\n",
        "            coordinate[0] = -coordinate[0]\n",
        "        landmarks = tf.constant(tempList, dtype=np.float32)\n",
        "\n",
        "    landmarks /= pose_size\n",
        "\n",
        "    return landmarks\n",
        "\n",
        "\n",
        "def normalize_drop_score(landmarks_and_scores):\n",
        "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
        "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
        "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
        "\n",
        "    # Normalize landmarks 2D\n",
        "    norm_landmarks = feature_pose(reshaped_inputs[:, :, :2])\n",
        "\n",
        "    return norm_landmarks\n",
        "\n",
        "\n",
        "def angle_between_two_vector(a, b):\n",
        "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
        "\n",
        "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
        "            1.5707963267948966\n",
        "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
        "            0.0\n",
        "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
        "            3.141592653589793\n",
        "    \"\"\"\n",
        "    inner = np.inner(a, b)\n",
        "    norms = LA.norm(a) * LA.norm(b)\n",
        "    cos = inner / norms\n",
        "    return np.arccos(np.clip(cos, -1.0, 1.0))\n",
        "\n",
        "\n",
        "def flatten(x):\n",
        "    return x.numpy().flatten()\n",
        "\n",
        "\n",
        "def angle_between_three_point(landmarks, bodypart1, bodypart2, bodypart3, isBodyPart=True):\n",
        "    if isBodyPart:\n",
        "        bodypart1 = tf.gather(landmarks, bodypart1.value, axis=1)\n",
        "        bodypart2 = tf.gather(landmarks, bodypart2.value, axis=1)\n",
        "        bodypart3 = tf.gather(landmarks, bodypart3.value, axis=1)\n",
        "\n",
        "    v21 = bodypart1 - bodypart2\n",
        "    v23 = bodypart3 - bodypart2\n",
        "\n",
        "    # đơn vị radian (góc đúng cả 2 bên)\n",
        "    return angle_between_two_vector(flatten(v21), flatten(v23))\n",
        "\n",
        "\n",
        "def feature_angle(landmarks):\n",
        "    center_ear = get_center_point(\n",
        "        landmarks, BodyPart.LEFT_EAR, BodyPart.RIGHT_EAR)\n",
        "    center_shoulder = get_center_point(\n",
        "        landmarks, BodyPart.LEFT_SHOULDER, BodyPart.RIGHT_SHOULDER)\n",
        "    center_hip = get_center_point(\n",
        "        landmarks, BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP)\n",
        "\n",
        "    angle_ear_shoulder = angle_between_three_point(\n",
        "        landmarks, center_ear, center_shoulder, center_hip, False)\n",
        "\n",
        "    angle_left_torso_thighs = angle_between_three_point(\n",
        "        landmarks, BodyPart.LEFT_SHOULDER, BodyPart.LEFT_HIP, BodyPart.LEFT_KNEE)\n",
        "\n",
        "    angle_right_torso_thighs = angle_between_three_point(\n",
        "        landmarks, BodyPart.RIGHT_SHOULDER, BodyPart.RIGHT_HIP, BodyPart.RIGHT_KNEE)\n",
        "\n",
        "    angle_left_thighs_tibia = angle_between_three_point(\n",
        "        landmarks, BodyPart.LEFT_HIP, BodyPart.LEFT_KNEE, BodyPart.LEFT_ANKLE)\n",
        "\n",
        "    angle_right_thighs_tibia = angle_between_three_point(\n",
        "        landmarks, BodyPart.RIGHT_HIP, BodyPart.RIGHT_KNEE, BodyPart.RIGHT_ANKLE)\n",
        "    \n",
        "    backbone_vector = flatten(center_shoulder - center_hip)\n",
        "    horizontal_backbone_angle = angle_between_two_vector(backbone_vector, (1,0))\n",
        "\n",
        "    # print('angle between left torso and thighs: ',\n",
        "    #       np.rad2deg(angle_left_torso_thighs))\n",
        "\n",
        "    return [horizontal_backbone_angle*2, angle_ear_shoulder, angle_left_torso_thighs, angle_left_thighs_tibia, angle_right_torso_thighs, angle_right_thighs_tibia]\n",
        "\n",
        "\n",
        "def get_distance(landmarks, left, right):\n",
        "    left = tf.gather(landmarks, left.value, axis=1)\n",
        "    right = tf.gather(landmarks, right.value, axis=1)\n",
        "\n",
        "    vector2 = (right - left).numpy()[0] ** 2\n",
        "\n",
        "    return math.sqrt(vector2[0] + vector2[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_preprocess_single(landmarks):\n",
        "    landmarks = normalize_drop_score(landmarks)\n",
        "    feature_vector = feature_angle(landmarks)\n",
        "    feature_vector.append(get_distance(landmarks, BodyPart.LEFT_SHOULDER, BodyPart.RIGHT_SHOULDER))\n",
        "    pose = feature_pose(landmarks).numpy().tolist()\n",
        "\n",
        "    unwanted = [BodyPart.LEFT_ELBOW.value, BodyPart.RIGHT_ELBOW.value, BodyPart.LEFT_WRIST.value, BodyPart.RIGHT_WRIST.value]\n",
        "    for index in sorted(unwanted, reverse = True): \n",
        "        del pose[0][index]\n",
        "\n",
        "    wanted_point = []\n",
        "\n",
        "    for item in pose[0]:\n",
        "        for i in item:\n",
        "            wanted_point.append(i)\n",
        "\n",
        "    feature_vector.extend(wanted_point)\n",
        "    # feature_vector = feature_pose(landmarks).numpy().flatten().tolist()\n",
        "    return feature_vector\n",
        "\n",
        "def data_preprocess(X: pd.DataFrame):\n",
        "    data = X.copy()\n",
        "    listFeature = []\n",
        "    for landmark in data.values:\n",
        "        landmark = tf.constant([landmark])\n",
        "        feature = data_preprocess_single(landmark)\n",
        "        listFeature.append(feature)\n",
        "\n",
        "    return pd.DataFrame(listFeature)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a Keras model for pose classification\n",
        "\n",
        "Our Keras model takes the detected pose landmarks, then calculates the pose embedding and predicts the pose class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the model\n",
        "def trainModel(X_train, y_train, X_val, y_val, X_test, y_test, class_names):\n",
        "    inputs = tf.keras.Input(shape=(33))\n",
        "\n",
        "    layer = keras.layers.Dense(64, activation=tf.nn.relu6)(inputs)\n",
        "    layer = keras.layers.Dropout(0.5)(layer)\n",
        "    layer = keras.layers.Dense(32, activation=tf.nn.relu6)(layer)\n",
        "    layer = keras.layers.Dropout(0.5)(layer)\n",
        "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    model.summary();          # ';' discards the output\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Add a checkpoint callback to store the checkpoint that has the highest\n",
        "    # validation accuracy.\n",
        "    checkpoint_path = \"weights.best.hdf5\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                monitor='val_accuracy',\n",
        "                                verbose=1,\n",
        "                                save_best_only=True,\n",
        "                                mode='max')\n",
        "    earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
        "                                                patience=20)\n",
        "\n",
        "    # Start training\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        epochs=200,\n",
        "                        batch_size=16,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks=[checkpoint, earlystopping]);          # ';' discards the output\n",
        "\n",
        "\n",
        "    # Evaluate the model using the TEST dataset\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    trainLastAccuracy = history.history['accuracy'][len(history.history['accuracy']) - 1]\n",
        "    \n",
        "    validateLastAccuracy = history.history['val_accuracy'][len(\n",
        "        history.history['val_accuracy']) - 1]\n",
        "\n",
        "    return trainLastAccuracy, validateLastAccuracy, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_N, y_N, class_names, test_N = load_pose_landmarks(\"N.csv\")\n",
        "X_H, y_H, _, test_H = load_pose_landmarks(\"H.csv\")\n",
        "X_NH, y_NH, _, test_NH = load_pose_landmarks(\"NH.csv\")\n",
        "X_K, y_K, _, test_K = load_pose_landmarks(\"K.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_N = data_preprocess(X_N)\n",
        "X_H = data_preprocess(X_H)\n",
        "X_NH = data_preprocess(X_NH)\n",
        "X_K = data_preprocess(X_K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "    'N': (X_N, y_N, test_N),\n",
        "    'NH': (X_NH, y_NH, test_NH),\n",
        "    'H': (X_H, y_H, test_H),\n",
        "    'K': (X_K, y_K, test_K)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "173/174 [============================>.] - ETA: 0s - loss: 1.8892 - accuracy: 0.2561\n",
            "Epoch 1: val_accuracy improved from -inf to 0.53821, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 1.8888 - accuracy: 0.2557 - val_loss: 1.6208 - val_accuracy: 0.5382\n",
            "Epoch 2/200\n",
            "155/174 [=========================>....] - ETA: 0s - loss: 1.4039 - accuracy: 0.4290\n",
            "Epoch 2: val_accuracy improved from 0.53821 to 0.55464, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 1.3826 - accuracy: 0.4400 - val_loss: 1.5464 - val_accuracy: 0.5546\n",
            "Epoch 3/200\n",
            "150/174 [========================>.....] - ETA: 0s - loss: 1.1469 - accuracy: 0.5258\n",
            "Epoch 3: val_accuracy improved from 0.55464 to 0.59244, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 1.1378 - accuracy: 0.5283 - val_loss: 1.6171 - val_accuracy: 0.5924\n",
            "Epoch 4/200\n",
            "146/174 [========================>.....] - ETA: 0s - loss: 0.9933 - accuracy: 0.5912\n",
            "Epoch 4: val_accuracy did not improve from 0.59244\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.9942 - accuracy: 0.5877 - val_loss: 1.6703 - val_accuracy: 0.5645\n",
            "Epoch 5/200\n",
            "162/174 [==========================>...] - ETA: 0s - loss: 0.9026 - accuracy: 0.6269\n",
            "Epoch 5: val_accuracy did not improve from 0.59244\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.9065 - accuracy: 0.6230 - val_loss: 1.8730 - val_accuracy: 0.5070\n",
            "Epoch 6/200\n",
            "158/174 [==========================>...] - ETA: 0s - loss: 0.8416 - accuracy: 0.6424\n",
            "Epoch 6: val_accuracy did not improve from 0.59244\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.8415 - accuracy: 0.6439 - val_loss: 1.9313 - val_accuracy: 0.5834\n",
            "Epoch 7/200\n",
            "170/174 [============================>.] - ETA: 0s - loss: 0.7597 - accuracy: 0.6938\n",
            "Epoch 7: val_accuracy improved from 0.59244 to 0.66886, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.7561 - accuracy: 0.6954 - val_loss: 2.0962 - val_accuracy: 0.6689\n",
            "Epoch 8/200\n",
            "155/174 [=========================>....] - ETA: 0s - loss: 0.7208 - accuracy: 0.7048\n",
            "Epoch 8: val_accuracy improved from 0.66886 to 0.69762, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 0.7173 - accuracy: 0.7087 - val_loss: 2.0870 - val_accuracy: 0.6976\n",
            "Epoch 9/200\n",
            "127/174 [====================>.........] - ETA: 0s - loss: 0.6574 - accuracy: 0.7402\n",
            "Epoch 9: val_accuracy did not improve from 0.69762\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 0.6520 - accuracy: 0.7436 - val_loss: 2.2812 - val_accuracy: 0.6976\n",
            "Epoch 10/200\n",
            "166/174 [===========================>..] - ETA: 0s - loss: 0.6191 - accuracy: 0.7451\n",
            "Epoch 10: val_accuracy did not improve from 0.69762\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.7476 - val_loss: 2.3802 - val_accuracy: 0.6771\n",
            "Epoch 11/200\n",
            "147/174 [========================>.....] - ETA: 0s - loss: 0.5910 - accuracy: 0.7636\n",
            "Epoch 11: val_accuracy did not improve from 0.69762\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 0.5912 - accuracy: 0.7649 - val_loss: 2.3856 - val_accuracy: 0.6804\n",
            "Epoch 12/200\n",
            "142/174 [=======================>......] - ETA: 0s - loss: 0.5506 - accuracy: 0.7768\n",
            "Epoch 12: val_accuracy did not improve from 0.69762\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 0.5464 - accuracy: 0.7829 - val_loss: 2.5671 - val_accuracy: 0.6763\n",
            "Epoch 13/200\n",
            "147/174 [========================>.....] - ETA: 0s - loss: 0.5316 - accuracy: 0.7912\n",
            "Epoch 13: val_accuracy did not improve from 0.69762\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7868 - val_loss: 2.6591 - val_accuracy: 0.6664\n",
            "Epoch 14/200\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.7965\n",
            "Epoch 14: val_accuracy improved from 0.69762 to 0.70419, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.5179 - accuracy: 0.7965 - val_loss: 2.7443 - val_accuracy: 0.7042\n",
            "Epoch 15/200\n",
            "143/174 [=======================>......] - ETA: 0s - loss: 0.5019 - accuracy: 0.7920\n",
            "Epoch 15: val_accuracy improved from 0.70419 to 0.70748, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7944 - val_loss: 2.9043 - val_accuracy: 0.7075\n",
            "Epoch 16/200\n",
            "156/174 [=========================>....] - ETA: 0s - loss: 0.4741 - accuracy: 0.8097\n",
            "Epoch 16: val_accuracy improved from 0.70748 to 0.73706, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.8127 - val_loss: 2.9696 - val_accuracy: 0.7371\n",
            "Epoch 17/200\n",
            "157/174 [==========================>...] - ETA: 0s - loss: 0.4753 - accuracy: 0.8173\n",
            "Epoch 17: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4675 - accuracy: 0.8203 - val_loss: 2.9533 - val_accuracy: 0.6968\n",
            "Epoch 18/200\n",
            "171/174 [============================>.] - ETA: 0s - loss: 0.4649 - accuracy: 0.8180\n",
            "Epoch 18: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.8178 - val_loss: 3.2219 - val_accuracy: 0.6721\n",
            "Epoch 19/200\n",
            "164/174 [===========================>..] - ETA: 0s - loss: 0.4293 - accuracy: 0.8258\n",
            "Epoch 19: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8275 - val_loss: 3.2606 - val_accuracy: 0.6606\n",
            "Epoch 20/200\n",
            "156/174 [=========================>....] - ETA: 0s - loss: 0.4227 - accuracy: 0.8361\n",
            "Epoch 20: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4262 - accuracy: 0.8326 - val_loss: 3.0782 - val_accuracy: 0.6623\n",
            "Epoch 21/200\n",
            "162/174 [==========================>...] - ETA: 0s - loss: 0.4050 - accuracy: 0.8426\n",
            "Epoch 21: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8444 - val_loss: 3.4079 - val_accuracy: 0.6763\n",
            "Epoch 22/200\n",
            "157/174 [==========================>...] - ETA: 0s - loss: 0.3961 - accuracy: 0.8523\n",
            "Epoch 22: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8477 - val_loss: 3.4043 - val_accuracy: 0.6532\n",
            "Epoch 23/200\n",
            "166/174 [===========================>..] - ETA: 0s - loss: 0.4004 - accuracy: 0.8524\n",
            "Epoch 23: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.8531 - val_loss: 3.4905 - val_accuracy: 0.6327\n",
            "Epoch 24/200\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.8520\n",
            "Epoch 24: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4005 - accuracy: 0.8520 - val_loss: 3.5949 - val_accuracy: 0.6647\n",
            "Epoch 25/200\n",
            "167/174 [===========================>..] - ETA: 0s - loss: 0.3893 - accuracy: 0.8626\n",
            "Epoch 25: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3900 - accuracy: 0.8624 - val_loss: 3.4196 - val_accuracy: 0.6615\n",
            "Epoch 26/200\n",
            "167/174 [===========================>..] - ETA: 0s - loss: 0.3712 - accuracy: 0.8615\n",
            "Epoch 26: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8628 - val_loss: 3.5894 - val_accuracy: 0.7050\n",
            "Epoch 27/200\n",
            "173/174 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8652\n",
            "Epoch 27: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8653 - val_loss: 3.4851 - val_accuracy: 0.6754\n",
            "Epoch 28/200\n",
            "167/174 [===========================>..] - ETA: 0s - loss: 0.3633 - accuracy: 0.8686\n",
            "Epoch 28: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8664 - val_loss: 3.7718 - val_accuracy: 0.6689\n",
            "Epoch 29/200\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8740\n",
            "Epoch 29: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8740 - val_loss: 3.6969 - val_accuracy: 0.6993\n",
            "Epoch 30/200\n",
            "170/174 [============================>.] - ETA: 0s - loss: 0.3614 - accuracy: 0.8757\n",
            "Epoch 30: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3597 - accuracy: 0.8765 - val_loss: 3.6935 - val_accuracy: 0.6565\n",
            "Epoch 31/200\n",
            "117/174 [===================>..........] - ETA: 0s - loss: 0.3361 - accuracy: 0.8803\n",
            "Epoch 31: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8840 - val_loss: 3.8040 - val_accuracy: 0.6902\n",
            "Epoch 32/200\n",
            "169/174 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8802\n",
            "Epoch 32: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8794 - val_loss: 4.0236 - val_accuracy: 0.6647\n",
            "Epoch 33/200\n",
            "147/174 [========================>.....] - ETA: 0s - loss: 0.2914 - accuracy: 0.9043\n",
            "Epoch 33: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.2956 - accuracy: 0.9021 - val_loss: 4.1447 - val_accuracy: 0.6541\n",
            "Epoch 34/200\n",
            "148/174 [========================>.....] - ETA: 0s - loss: 0.2942 - accuracy: 0.8957\n",
            "Epoch 34: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8945 - val_loss: 3.9525 - val_accuracy: 0.6697\n",
            "Epoch 35/200\n",
            "154/174 [=========================>....] - ETA: 0s - loss: 0.3074 - accuracy: 0.8916\n",
            "Epoch 35: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.8902 - val_loss: 3.8250 - val_accuracy: 0.6697\n",
            "Epoch 36/200\n",
            "165/174 [===========================>..] - ETA: 0s - loss: 0.2783 - accuracy: 0.9011\n",
            "Epoch 36: val_accuracy did not improve from 0.73706\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.2771 - accuracy: 0.9021 - val_loss: 4.2164 - val_accuracy: 0.6647\n",
            "52/52 [==============================] - 0s 922us/step - loss: 1.7616 - accuracy: 0.6643\n",
            "[0.9020525813102722, 0.6647493839263916, 0.6642512083053589]\n",
            "H\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "142/167 [========================>.....] - ETA: 0s - loss: 1.9017 - accuracy: 0.2430\n",
            "Epoch 1: val_accuracy improved from -inf to 0.45734, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 1s 3ms/step - loss: 1.8656 - accuracy: 0.2566 - val_loss: 1.5791 - val_accuracy: 0.4573\n",
            "Epoch 2/200\n",
            "127/167 [=====================>........] - ETA: 0s - loss: 1.4633 - accuracy: 0.4075\n",
            "Epoch 2: val_accuracy did not improve from 0.45734\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 1.4342 - accuracy: 0.4150 - val_loss: 1.2989 - val_accuracy: 0.3600\n",
            "Epoch 3/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 1.2300 - accuracy: 0.4735\n",
            "Epoch 3: val_accuracy improved from 0.45734 to 0.71931, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 1.2153 - accuracy: 0.4804 - val_loss: 1.1570 - val_accuracy: 0.7193\n",
            "Epoch 4/200\n",
            "156/167 [===========================>..] - ETA: 0s - loss: 1.0765 - accuracy: 0.5397\n",
            "Epoch 4: val_accuracy improved from 0.71931 to 0.72605, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 1.0777 - accuracy: 0.5418 - val_loss: 1.0756 - val_accuracy: 0.7260\n",
            "Epoch 5/200\n",
            "164/167 [============================>.] - ETA: 0s - loss: 0.9849 - accuracy: 0.5903\n",
            "Epoch 5: val_accuracy improved from 0.72605 to 0.74326, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.9831 - accuracy: 0.5910 - val_loss: 1.0342 - val_accuracy: 0.7433\n",
            "Epoch 6/200\n",
            "158/167 [===========================>..] - ETA: 0s - loss: 0.8926 - accuracy: 0.6151\n",
            "Epoch 6: val_accuracy did not improve from 0.74326\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.8903 - accuracy: 0.6178 - val_loss: 1.0292 - val_accuracy: 0.7178\n",
            "Epoch 7/200\n",
            "151/167 [==========================>...] - ETA: 0s - loss: 0.8057 - accuracy: 0.6689\n",
            "Epoch 7: val_accuracy did not improve from 0.74326\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.8089 - accuracy: 0.6655 - val_loss: 0.9697 - val_accuracy: 0.7403\n",
            "Epoch 8/200\n",
            "153/167 [==========================>...] - ETA: 0s - loss: 0.7781 - accuracy: 0.6810\n",
            "Epoch 8: val_accuracy did not improve from 0.74326\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.7765 - accuracy: 0.6825 - val_loss: 0.9476 - val_accuracy: 0.7088\n",
            "Epoch 9/200\n",
            "154/167 [==========================>...] - ETA: 0s - loss: 0.7054 - accuracy: 0.7029\n",
            "Epoch 9: val_accuracy improved from 0.74326 to 0.75000, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.7054 - accuracy: 0.7024 - val_loss: 1.0163 - val_accuracy: 0.7500\n",
            "Epoch 10/200\n",
            "140/167 [========================>.....] - ETA: 0s - loss: 0.6668 - accuracy: 0.7304\n",
            "Epoch 10: val_accuracy did not improve from 0.75000\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.7239 - val_loss: 0.9605 - val_accuracy: 0.7231\n",
            "Epoch 11/200\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.6287 - accuracy: 0.7254\n",
            "Epoch 11: val_accuracy did not improve from 0.75000\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.6287 - accuracy: 0.7254 - val_loss: 0.9720 - val_accuracy: 0.7081\n",
            "Epoch 12/200\n",
            "165/167 [============================>.] - ETA: 0s - loss: 0.6040 - accuracy: 0.7352\n",
            "Epoch 12: val_accuracy improved from 0.75000 to 0.76198, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.6036 - accuracy: 0.7355 - val_loss: 1.0362 - val_accuracy: 0.7620\n",
            "Epoch 13/200\n",
            "148/167 [=========================>....] - ETA: 0s - loss: 0.5708 - accuracy: 0.7707\n",
            "Epoch 13: val_accuracy did not improve from 0.76198\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.7724 - val_loss: 1.0010 - val_accuracy: 0.7403\n",
            "Epoch 14/200\n",
            "164/167 [============================>.] - ETA: 0s - loss: 0.5546 - accuracy: 0.7660\n",
            "Epoch 14: val_accuracy did not improve from 0.76198\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.5536 - accuracy: 0.7667 - val_loss: 1.0738 - val_accuracy: 0.7208\n",
            "Epoch 15/200\n",
            "151/167 [==========================>...] - ETA: 0s - loss: 0.5247 - accuracy: 0.7860\n",
            "Epoch 15: val_accuracy did not improve from 0.76198\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.5268 - accuracy: 0.7856 - val_loss: 1.0960 - val_accuracy: 0.7448\n",
            "Epoch 16/200\n",
            "162/167 [============================>.] - ETA: 0s - loss: 0.5124 - accuracy: 0.7851\n",
            "Epoch 16: val_accuracy did not improve from 0.76198\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.5142 - accuracy: 0.7837 - val_loss: 1.0640 - val_accuracy: 0.7313\n",
            "Epoch 17/200\n",
            "115/167 [===================>..........] - ETA: 0s - loss: 0.4735 - accuracy: 0.8065\n",
            "Epoch 17: val_accuracy did not improve from 0.76198\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4864 - accuracy: 0.7972 - val_loss: 1.1550 - val_accuracy: 0.7545\n",
            "Epoch 18/200\n",
            "160/167 [===========================>..] - ETA: 0s - loss: 0.4891 - accuracy: 0.7832\n",
            "Epoch 18: val_accuracy did not improve from 0.76198\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7807 - val_loss: 1.0992 - val_accuracy: 0.7328\n",
            "Epoch 19/200\n",
            "157/167 [===========================>..] - ETA: 0s - loss: 0.4691 - accuracy: 0.8021\n",
            "Epoch 19: val_accuracy improved from 0.76198 to 0.76796, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.7983 - val_loss: 1.1515 - val_accuracy: 0.7680\n",
            "Epoch 20/200\n",
            "108/167 [==================>...........] - ETA: 0s - loss: 0.4460 - accuracy: 0.8148\n",
            "Epoch 20: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.8119 - val_loss: 1.1314 - val_accuracy: 0.7537\n",
            "Epoch 21/200\n",
            "114/167 [===================>..........] - ETA: 0s - loss: 0.4576 - accuracy: 0.8103\n",
            "Epoch 21: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.8096 - val_loss: 1.1494 - val_accuracy: 0.7537\n",
            "Epoch 22/200\n",
            "124/167 [=====================>........] - ETA: 0s - loss: 0.4294 - accuracy: 0.8185\n",
            "Epoch 22: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8205 - val_loss: 1.1483 - val_accuracy: 0.7485\n",
            "Epoch 23/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.4203 - accuracy: 0.8151\n",
            "Epoch 23: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8187 - val_loss: 1.1994 - val_accuracy: 0.7560\n",
            "Epoch 24/200\n",
            "112/167 [===================>..........] - ETA: 0s - loss: 0.4365 - accuracy: 0.8147\n",
            "Epoch 24: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4310 - accuracy: 0.8202 - val_loss: 1.3242 - val_accuracy: 0.7238\n",
            "Epoch 25/200\n",
            "130/167 [======================>.......] - ETA: 0s - loss: 0.4257 - accuracy: 0.8202\n",
            "Epoch 25: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8205 - val_loss: 1.2616 - val_accuracy: 0.7575\n",
            "Epoch 26/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.4159 - accuracy: 0.8355\n",
            "Epoch 26: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4092 - accuracy: 0.8375 - val_loss: 1.2320 - val_accuracy: 0.7418\n",
            "Epoch 27/200\n",
            "129/167 [======================>.......] - ETA: 0s - loss: 0.4080 - accuracy: 0.8377\n",
            "Epoch 27: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4137 - accuracy: 0.8315 - val_loss: 1.3027 - val_accuracy: 0.7388\n",
            "Epoch 28/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.4178 - accuracy: 0.8202\n",
            "Epoch 28: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4026 - accuracy: 0.8303 - val_loss: 1.3440 - val_accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "126/167 [=====================>........] - ETA: 0s - loss: 0.3972 - accuracy: 0.8279\n",
            "Epoch 29: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3913 - accuracy: 0.8333 - val_loss: 1.3837 - val_accuracy: 0.7298\n",
            "Epoch 30/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.4032 - accuracy: 0.8340\n",
            "Epoch 30: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4000 - accuracy: 0.8341 - val_loss: 1.4210 - val_accuracy: 0.7500\n",
            "Epoch 31/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.3675 - accuracy: 0.8445\n",
            "Epoch 31: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8469 - val_loss: 1.4046 - val_accuracy: 0.7358\n",
            "Epoch 32/200\n",
            "127/167 [=====================>........] - ETA: 0s - loss: 0.3920 - accuracy: 0.8278\n",
            "Epoch 32: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3821 - accuracy: 0.8315 - val_loss: 1.4404 - val_accuracy: 0.7515\n",
            "Epoch 33/200\n",
            "126/167 [=====================>........] - ETA: 0s - loss: 0.3673 - accuracy: 0.8467\n",
            "Epoch 33: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3644 - accuracy: 0.8518 - val_loss: 1.3256 - val_accuracy: 0.7552\n",
            "Epoch 34/200\n",
            "116/167 [===================>..........] - ETA: 0s - loss: 0.3994 - accuracy: 0.8270\n",
            "Epoch 34: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 0.8435 - val_loss: 1.3828 - val_accuracy: 0.7552\n",
            "Epoch 35/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.3777 - accuracy: 0.8450\n",
            "Epoch 35: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3644 - accuracy: 0.8495 - val_loss: 1.3735 - val_accuracy: 0.7485\n",
            "Epoch 36/200\n",
            "127/167 [=====================>........] - ETA: 0s - loss: 0.3625 - accuracy: 0.8519\n",
            "Epoch 36: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3552 - accuracy: 0.8555 - val_loss: 1.3854 - val_accuracy: 0.7440\n",
            "Epoch 37/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.3385 - accuracy: 0.8665\n",
            "Epoch 37: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3566 - accuracy: 0.8574 - val_loss: 1.3655 - val_accuracy: 0.7627\n",
            "Epoch 38/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3675 - accuracy: 0.8501\n",
            "Epoch 38: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8574 - val_loss: 1.5493 - val_accuracy: 0.7178\n",
            "Epoch 39/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3239 - accuracy: 0.8613\n",
            "Epoch 39: val_accuracy did not improve from 0.76796\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8612 - val_loss: 1.5353 - val_accuracy: 0.7305\n",
            "52/52 [==============================] - 0s 745us/step - loss: 0.7716 - accuracy: 0.8454\n",
            "[0.8611738085746765, 0.7305389046669006, 0.8454106450080872]\n",
            "K\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 2.0168 - accuracy: 0.1964\n",
            "Epoch 1: val_accuracy improved from -inf to 0.49480, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 1s 2ms/step - loss: 1.9522 - accuracy: 0.2197 - val_loss: 1.6180 - val_accuracy: 0.4948\n",
            "Epoch 2/200\n",
            "113/160 [====================>.........] - ETA: 0s - loss: 1.5990 - accuracy: 0.3689\n",
            "Epoch 2: val_accuracy improved from 0.49480 to 0.50035, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.5550 - accuracy: 0.3807 - val_loss: 1.2804 - val_accuracy: 0.5003\n",
            "Epoch 3/200\n",
            "152/160 [===========================>..] - ETA: 0s - loss: 1.2936 - accuracy: 0.4852\n",
            "Epoch 3: val_accuracy improved from 0.50035 to 0.57946, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.2903 - accuracy: 0.4884 - val_loss: 1.1133 - val_accuracy: 0.5795\n",
            "Epoch 4/200\n",
            "103/160 [==================>...........] - ETA: 0s - loss: 1.1534 - accuracy: 0.5388\n",
            "Epoch 4: val_accuracy improved from 0.57946 to 0.62595, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.1162 - accuracy: 0.5601 - val_loss: 0.9721 - val_accuracy: 0.6260\n",
            "Epoch 5/200\n",
            "147/160 [==========================>...] - ETA: 0s - loss: 0.9861 - accuracy: 0.6093\n",
            "Epoch 5: val_accuracy improved from 0.62595 to 0.68980, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.9852 - accuracy: 0.6118 - val_loss: 0.8771 - val_accuracy: 0.6898\n",
            "Epoch 6/200\n",
            "116/160 [====================>.........] - ETA: 0s - loss: 0.8700 - accuracy: 0.6649\n",
            "Epoch 6: val_accuracy improved from 0.68980 to 0.69327, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.8729 - accuracy: 0.6678 - val_loss: 0.8268 - val_accuracy: 0.6933\n",
            "Epoch 7/200\n",
            "116/160 [====================>.........] - ETA: 0s - loss: 0.7445 - accuracy: 0.7457\n",
            "Epoch 7: val_accuracy did not improve from 0.69327\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.7402 - accuracy: 0.7446 - val_loss: 0.8033 - val_accuracy: 0.6849\n",
            "Epoch 8/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.7171 - accuracy: 0.7319\n",
            "Epoch 8: val_accuracy did not improve from 0.69327\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.6947 - accuracy: 0.7466 - val_loss: 0.7925 - val_accuracy: 0.6634\n",
            "Epoch 9/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.6542 - accuracy: 0.7692\n",
            "Epoch 9: val_accuracy improved from 0.69327 to 0.70507, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.6437 - accuracy: 0.7681 - val_loss: 0.7337 - val_accuracy: 0.7051\n",
            "Epoch 10/200\n",
            "117/160 [====================>.........] - ETA: 0s - loss: 0.5874 - accuracy: 0.7890\n",
            "Epoch 10: val_accuracy improved from 0.70507 to 0.70784, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5877 - accuracy: 0.7857 - val_loss: 0.7091 - val_accuracy: 0.7078\n",
            "Epoch 11/200\n",
            "109/160 [===================>..........] - ETA: 0s - loss: 0.5450 - accuracy: 0.8010\n",
            "Epoch 11: val_accuracy did not improve from 0.70784\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.8108 - val_loss: 0.7700 - val_accuracy: 0.6891\n",
            "Epoch 12/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.5095 - accuracy: 0.8170\n",
            "Epoch 12: val_accuracy improved from 0.70784 to 0.71964, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.8179 - val_loss: 0.7173 - val_accuracy: 0.7196\n",
            "Epoch 13/200\n",
            "108/160 [===================>..........] - ETA: 0s - loss: 0.4829 - accuracy: 0.8362\n",
            "Epoch 13: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.8351 - val_loss: 0.8162 - val_accuracy: 0.6974\n",
            "Epoch 14/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.4710 - accuracy: 0.8422\n",
            "Epoch 14: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.8484 - val_loss: 0.7667 - val_accuracy: 0.6884\n",
            "Epoch 15/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.4359 - accuracy: 0.8504\n",
            "Epoch 15: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4253 - accuracy: 0.8531 - val_loss: 0.7525 - val_accuracy: 0.7148\n",
            "Epoch 16/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.4000 - accuracy: 0.8650\n",
            "Epoch 16: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4094 - accuracy: 0.8633 - val_loss: 0.7756 - val_accuracy: 0.7120\n",
            "Epoch 17/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.4144 - accuracy: 0.8635\n",
            "Epoch 17: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.8656 - val_loss: 0.7924 - val_accuracy: 0.7002\n",
            "Epoch 18/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.3766 - accuracy: 0.8714\n",
            "Epoch 18: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3796 - accuracy: 0.8727 - val_loss: 0.7879 - val_accuracy: 0.7189\n",
            "Epoch 19/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.3786 - accuracy: 0.8735\n",
            "Epoch 19: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3753 - accuracy: 0.8723 - val_loss: 0.8672 - val_accuracy: 0.7141\n",
            "Epoch 20/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.3536 - accuracy: 0.8805\n",
            "Epoch 20: val_accuracy did not improve from 0.71964\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3542 - accuracy: 0.8829 - val_loss: 0.8661 - val_accuracy: 0.7002\n",
            "Epoch 21/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.3371 - accuracy: 0.8858\n",
            "Epoch 21: val_accuracy improved from 0.71964 to 0.73213, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8829 - val_loss: 0.8909 - val_accuracy: 0.7321\n",
            "Epoch 22/200\n",
            "110/160 [===================>..........] - ETA: 0s - loss: 0.3148 - accuracy: 0.8915\n",
            "Epoch 22: val_accuracy did not improve from 0.73213\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3270 - accuracy: 0.8860 - val_loss: 0.8250 - val_accuracy: 0.7162\n",
            "Epoch 23/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.3164 - accuracy: 0.8974\n",
            "Epoch 23: val_accuracy did not improve from 0.73213\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8966 - val_loss: 0.8229 - val_accuracy: 0.7065\n",
            "Epoch 24/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.3198 - accuracy: 0.8965\n",
            "Epoch 24: val_accuracy did not improve from 0.73213\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8997 - val_loss: 1.0103 - val_accuracy: 0.7071\n",
            "Epoch 25/200\n",
            "111/160 [===================>..........] - ETA: 0s - loss: 0.2976 - accuracy: 0.9065\n",
            "Epoch 25: val_accuracy did not improve from 0.73213\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.9060 - val_loss: 0.9934 - val_accuracy: 0.7196\n",
            "Epoch 26/200\n",
            "114/160 [====================>.........] - ETA: 0s - loss: 0.2812 - accuracy: 0.9073\n",
            "Epoch 26: val_accuracy did not improve from 0.73213\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.9056 - val_loss: 1.1025 - val_accuracy: 0.7217\n",
            "Epoch 27/200\n",
            "118/160 [=====================>........] - ETA: 0s - loss: 0.2757 - accuracy: 0.9126\n",
            "Epoch 27: val_accuracy did not improve from 0.73213\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.9119 - val_loss: 1.0306 - val_accuracy: 0.7231\n",
            "Epoch 28/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.2893 - accuracy: 0.9124\n",
            "Epoch 28: val_accuracy improved from 0.73213 to 0.74046, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.9127 - val_loss: 0.9677 - val_accuracy: 0.7405\n",
            "Epoch 29/200\n",
            "112/160 [====================>.........] - ETA: 0s - loss: 0.2751 - accuracy: 0.9169\n",
            "Epoch 29: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.9162 - val_loss: 0.8574 - val_accuracy: 0.7071\n",
            "Epoch 30/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.2653 - accuracy: 0.9168\n",
            "Epoch 30: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.9166 - val_loss: 1.0398 - val_accuracy: 0.7196\n",
            "Epoch 31/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.2592 - accuracy: 0.9205\n",
            "Epoch 31: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2598 - accuracy: 0.9181 - val_loss: 1.0503 - val_accuracy: 0.7217\n",
            "Epoch 32/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 0.2585 - accuracy: 0.9187\n",
            "Epoch 32: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.9197 - val_loss: 0.9689 - val_accuracy: 0.7328\n",
            "Epoch 33/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.2693 - accuracy: 0.9098\n",
            "Epoch 33: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2661 - accuracy: 0.9150 - val_loss: 1.1456 - val_accuracy: 0.7092\n",
            "Epoch 34/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.2417 - accuracy: 0.9246\n",
            "Epoch 34: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9291 - val_loss: 1.1003 - val_accuracy: 0.7217\n",
            "Epoch 35/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.2460 - accuracy: 0.9264\n",
            "Epoch 35: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2536 - accuracy: 0.9240 - val_loss: 1.1052 - val_accuracy: 0.7280\n",
            "Epoch 36/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.2068 - accuracy: 0.9396\n",
            "Epoch 36: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9342 - val_loss: 1.2546 - val_accuracy: 0.7314\n",
            "Epoch 37/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.2334 - accuracy: 0.9330\n",
            "Epoch 37: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9350 - val_loss: 1.0815 - val_accuracy: 0.7342\n",
            "Epoch 38/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.2110 - accuracy: 0.9350\n",
            "Epoch 38: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9365 - val_loss: 1.2258 - val_accuracy: 0.7280\n",
            "Epoch 39/200\n",
            "119/160 [=====================>........] - ETA: 0s - loss: 0.2171 - accuracy: 0.9301\n",
            "Epoch 39: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9342 - val_loss: 1.1730 - val_accuracy: 0.7002\n",
            "Epoch 40/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.2066 - accuracy: 0.9329\n",
            "Epoch 40: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9322 - val_loss: 1.1598 - val_accuracy: 0.7370\n",
            "Epoch 41/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.2070 - accuracy: 0.9303\n",
            "Epoch 41: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9279 - val_loss: 1.0809 - val_accuracy: 0.7245\n",
            "Epoch 42/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.2233 - accuracy: 0.9274\n",
            "Epoch 42: val_accuracy did not improve from 0.74046\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9307 - val_loss: 1.1920 - val_accuracy: 0.7224\n",
            "Epoch 43/200\n",
            "105/160 [==================>...........] - ETA: 0s - loss: 0.2046 - accuracy: 0.9375\n",
            "Epoch 43: val_accuracy improved from 0.74046 to 0.74393, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9365 - val_loss: 1.0988 - val_accuracy: 0.7439\n",
            "Epoch 44/200\n",
            "109/160 [===================>..........] - ETA: 0s - loss: 0.2023 - accuracy: 0.9358\n",
            "Epoch 44: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9354 - val_loss: 1.1913 - val_accuracy: 0.7328\n",
            "Epoch 45/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.2121 - accuracy: 0.9365\n",
            "Epoch 45: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9385 - val_loss: 1.1915 - val_accuracy: 0.7307\n",
            "Epoch 46/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.2020 - accuracy: 0.9334\n",
            "Epoch 46: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9362 - val_loss: 1.3466 - val_accuracy: 0.7294\n",
            "Epoch 47/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1881 - accuracy: 0.9400\n",
            "Epoch 47: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9432 - val_loss: 1.3466 - val_accuracy: 0.7363\n",
            "Epoch 48/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.1881 - accuracy: 0.9445\n",
            "Epoch 48: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9456 - val_loss: 1.0481 - val_accuracy: 0.7127\n",
            "Epoch 49/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.2041 - accuracy: 0.9350\n",
            "Epoch 49: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2014 - accuracy: 0.9369 - val_loss: 1.3287 - val_accuracy: 0.7335\n",
            "Epoch 50/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 0.2067 - accuracy: 0.9229\n",
            "Epoch 50: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9279 - val_loss: 1.3199 - val_accuracy: 0.7425\n",
            "Epoch 51/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1910 - accuracy: 0.9360\n",
            "Epoch 51: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9362 - val_loss: 1.2549 - val_accuracy: 0.7203\n",
            "Epoch 52/200\n",
            "126/160 [======================>.......] - ETA: 0s - loss: 0.1735 - accuracy: 0.9504\n",
            "Epoch 52: val_accuracy did not improve from 0.74393\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9491 - val_loss: 1.4953 - val_accuracy: 0.7245\n",
            "Epoch 53/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.1937 - accuracy: 0.9426\n",
            "Epoch 53: val_accuracy improved from 0.74393 to 0.74462, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9428 - val_loss: 1.3300 - val_accuracy: 0.7446\n",
            "Epoch 54/200\n",
            "114/160 [====================>.........] - ETA: 0s - loss: 0.1760 - accuracy: 0.9402\n",
            "Epoch 54: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9428 - val_loss: 1.5017 - val_accuracy: 0.7342\n",
            "Epoch 55/200\n",
            "126/160 [======================>.......] - ETA: 0s - loss: 0.1641 - accuracy: 0.9489\n",
            "Epoch 55: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9467 - val_loss: 1.3861 - val_accuracy: 0.7432\n",
            "Epoch 56/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1791 - accuracy: 0.9441\n",
            "Epoch 56: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9424 - val_loss: 1.4303 - val_accuracy: 0.7363\n",
            "Epoch 57/200\n",
            "115/160 [====================>.........] - ETA: 0s - loss: 0.1897 - accuracy: 0.9310\n",
            "Epoch 57: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.9346 - val_loss: 1.4567 - val_accuracy: 0.7349\n",
            "Epoch 58/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1617 - accuracy: 0.9478\n",
            "Epoch 58: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9475 - val_loss: 1.4008 - val_accuracy: 0.7307\n",
            "Epoch 59/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1689 - accuracy: 0.9451\n",
            "Epoch 59: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9416 - val_loss: 1.4326 - val_accuracy: 0.7398\n",
            "Epoch 60/200\n",
            "119/160 [=====================>........] - ETA: 0s - loss: 0.1633 - accuracy: 0.9475\n",
            "Epoch 60: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9479 - val_loss: 1.4342 - val_accuracy: 0.7377\n",
            "Epoch 61/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.1701 - accuracy: 0.9465\n",
            "Epoch 61: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9471 - val_loss: 1.3196 - val_accuracy: 0.7176\n",
            "Epoch 62/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.1560 - accuracy: 0.9486\n",
            "Epoch 62: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9467 - val_loss: 1.6067 - val_accuracy: 0.7280\n",
            "Epoch 63/200\n",
            "126/160 [======================>.......] - ETA: 0s - loss: 0.1917 - accuracy: 0.9360\n",
            "Epoch 63: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9393 - val_loss: 1.4785 - val_accuracy: 0.7439\n",
            "Epoch 64/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1748 - accuracy: 0.9406\n",
            "Epoch 64: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9448 - val_loss: 1.7425 - val_accuracy: 0.7363\n",
            "Epoch 65/200\n",
            "119/160 [=====================>........] - ETA: 0s - loss: 0.1520 - accuracy: 0.9564\n",
            "Epoch 65: val_accuracy did not improve from 0.74462\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9550 - val_loss: 1.5502 - val_accuracy: 0.7398\n",
            "Epoch 66/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1640 - accuracy: 0.9492\n",
            "Epoch 66: val_accuracy improved from 0.74462 to 0.75503, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9526 - val_loss: 1.4697 - val_accuracy: 0.7550\n",
            "Epoch 67/200\n",
            "111/160 [===================>..........] - ETA: 0s - loss: 0.1438 - accuracy: 0.9555\n",
            "Epoch 67: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9542 - val_loss: 1.5589 - val_accuracy: 0.7467\n",
            "Epoch 68/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.1587 - accuracy: 0.9503\n",
            "Epoch 68: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.9518 - val_loss: 1.4839 - val_accuracy: 0.7516\n",
            "Epoch 69/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1478 - accuracy: 0.9535\n",
            "Epoch 69: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9530 - val_loss: 1.2839 - val_accuracy: 0.7196\n",
            "Epoch 70/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.1480 - accuracy: 0.9525\n",
            "Epoch 70: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9538 - val_loss: 1.6427 - val_accuracy: 0.7335\n",
            "Epoch 71/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 0.1426 - accuracy: 0.9583\n",
            "Epoch 71: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9557 - val_loss: 1.7563 - val_accuracy: 0.7259\n",
            "Epoch 72/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1337 - accuracy: 0.9604\n",
            "Epoch 72: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9604 - val_loss: 1.6673 - val_accuracy: 0.7398\n",
            "Epoch 73/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.1356 - accuracy: 0.9541\n",
            "Epoch 73: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9534 - val_loss: 1.5575 - val_accuracy: 0.7398\n",
            "Epoch 74/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1487 - accuracy: 0.9497\n",
            "Epoch 74: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9522 - val_loss: 1.7201 - val_accuracy: 0.7529\n",
            "Epoch 75/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1390 - accuracy: 0.9593\n",
            "Epoch 75: val_accuracy did not improve from 0.75503\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9581 - val_loss: 1.7347 - val_accuracy: 0.7391\n",
            "Epoch 76/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1484 - accuracy: 0.9533\n",
            "Epoch 76: val_accuracy improved from 0.75503 to 0.75850, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9503 - val_loss: 1.5946 - val_accuracy: 0.7585\n",
            "Epoch 77/200\n",
            "111/160 [===================>..........] - ETA: 0s - loss: 0.1488 - accuracy: 0.9510\n",
            "Epoch 77: val_accuracy did not improve from 0.75850\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9530 - val_loss: 1.9367 - val_accuracy: 0.7453\n",
            "Epoch 78/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1477 - accuracy: 0.9487\n",
            "Epoch 78: val_accuracy did not improve from 0.75850\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9499 - val_loss: 1.7686 - val_accuracy: 0.7502\n",
            "Epoch 79/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1480 - accuracy: 0.9566\n",
            "Epoch 79: val_accuracy did not improve from 0.75850\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9518 - val_loss: 1.8146 - val_accuracy: 0.7564\n",
            "Epoch 80/200\n",
            "127/160 [======================>.......] - ETA: 0s - loss: 0.1307 - accuracy: 0.9601\n",
            "Epoch 80: val_accuracy did not improve from 0.75850\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9581 - val_loss: 1.6395 - val_accuracy: 0.7148\n",
            "Epoch 81/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.1468 - accuracy: 0.9555\n",
            "Epoch 81: val_accuracy improved from 0.75850 to 0.76336, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9530 - val_loss: 1.5975 - val_accuracy: 0.7634\n",
            "Epoch 82/200\n",
            "111/160 [===================>..........] - ETA: 0s - loss: 0.1492 - accuracy: 0.9488\n",
            "Epoch 82: val_accuracy did not improve from 0.76336\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9514 - val_loss: 1.7640 - val_accuracy: 0.7412\n",
            "Epoch 83/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.1233 - accuracy: 0.9611\n",
            "Epoch 83: val_accuracy improved from 0.76336 to 0.76405, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9600 - val_loss: 2.0663 - val_accuracy: 0.7641\n",
            "Epoch 84/200\n",
            "112/160 [====================>.........] - ETA: 0s - loss: 0.1194 - accuracy: 0.9565\n",
            "Epoch 84: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9530 - val_loss: 1.7404 - val_accuracy: 0.7488\n",
            "Epoch 85/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1303 - accuracy: 0.9582\n",
            "Epoch 85: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9542 - val_loss: 2.1065 - val_accuracy: 0.7453\n",
            "Epoch 86/200\n",
            "105/160 [==================>...........] - ETA: 0s - loss: 0.1281 - accuracy: 0.9613\n",
            "Epoch 86: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9577 - val_loss: 1.8661 - val_accuracy: 0.7536\n",
            "Epoch 87/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 0.1191 - accuracy: 0.9594\n",
            "Epoch 87: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9612 - val_loss: 1.7584 - val_accuracy: 0.7523\n",
            "Epoch 88/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.1374 - accuracy: 0.9600\n",
            "Epoch 88: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9581 - val_loss: 1.4785 - val_accuracy: 0.7405\n",
            "Epoch 89/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1165 - accuracy: 0.9628\n",
            "Epoch 89: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9659 - val_loss: 1.7903 - val_accuracy: 0.7488\n",
            "Epoch 90/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 0.1300 - accuracy: 0.9583\n",
            "Epoch 90: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9600 - val_loss: 2.0325 - val_accuracy: 0.7481\n",
            "Epoch 91/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1545 - accuracy: 0.9509\n",
            "Epoch 91: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9499 - val_loss: 1.6430 - val_accuracy: 0.7543\n",
            "Epoch 92/200\n",
            "118/160 [=====================>........] - ETA: 0s - loss: 0.1385 - accuracy: 0.9587\n",
            "Epoch 92: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9557 - val_loss: 1.7852 - val_accuracy: 0.7606\n",
            "Epoch 93/200\n",
            "117/160 [====================>.........] - ETA: 0s - loss: 0.1193 - accuracy: 0.9599\n",
            "Epoch 93: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9553 - val_loss: 1.9207 - val_accuracy: 0.7634\n",
            "Epoch 94/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1372 - accuracy: 0.9556\n",
            "Epoch 94: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9577 - val_loss: 1.5399 - val_accuracy: 0.7523\n",
            "Epoch 95/200\n",
            "123/160 [======================>.......] - ETA: 0s - loss: 0.1311 - accuracy: 0.9593\n",
            "Epoch 95: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9581 - val_loss: 1.9892 - val_accuracy: 0.7356\n",
            "Epoch 96/200\n",
            "126/160 [======================>.......] - ETA: 0s - loss: 0.1445 - accuracy: 0.9489\n",
            "Epoch 96: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9499 - val_loss: 2.2435 - val_accuracy: 0.7474\n",
            "Epoch 97/200\n",
            "117/160 [====================>.........] - ETA: 0s - loss: 0.1210 - accuracy: 0.9658\n",
            "Epoch 97: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9612 - val_loss: 2.1074 - val_accuracy: 0.7328\n",
            "Epoch 98/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 0.1168 - accuracy: 0.9646\n",
            "Epoch 98: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9651 - val_loss: 2.2117 - val_accuracy: 0.7481\n",
            "Epoch 99/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.1187 - accuracy: 0.9602\n",
            "Epoch 99: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9597 - val_loss: 2.0611 - val_accuracy: 0.7620\n",
            "Epoch 100/200\n",
            "107/160 [===================>..........] - ETA: 0s - loss: 0.1310 - accuracy: 0.9626\n",
            "Epoch 100: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9612 - val_loss: 2.0096 - val_accuracy: 0.7481\n",
            "Epoch 101/200\n",
            "107/160 [===================>..........] - ETA: 0s - loss: 0.1304 - accuracy: 0.9579\n",
            "Epoch 101: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9569 - val_loss: 2.4305 - val_accuracy: 0.7412\n",
            "Epoch 102/200\n",
            "116/160 [====================>.........] - ETA: 0s - loss: 0.1328 - accuracy: 0.9617\n",
            "Epoch 102: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9593 - val_loss: 1.8618 - val_accuracy: 0.7446\n",
            "Epoch 103/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.1096 - accuracy: 0.9597\n",
            "Epoch 103: val_accuracy did not improve from 0.76405\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9597 - val_loss: 2.0303 - val_accuracy: 0.7529\n",
            "52/52 [==============================] - 0s 736us/step - loss: 1.1744 - accuracy: 0.8025\n",
            "[0.9596552848815918, 0.7529493570327759, 0.8025362491607666]\n",
            "H\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "168/174 [===========================>..] - ETA: 0s - loss: 1.9391 - accuracy: 0.2191\n",
            "Epoch 1: val_accuracy improved from -inf to 0.42271, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 1s 2ms/step - loss: 1.9333 - accuracy: 0.2222 - val_loss: 1.6810 - val_accuracy: 0.4227\n",
            "Epoch 2/200\n",
            "140/174 [=======================>......] - ETA: 0s - loss: 1.5597 - accuracy: 0.3906\n",
            "Epoch 2: val_accuracy improved from 0.42271 to 0.57186, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 2ms/step - loss: 1.5159 - accuracy: 0.4098 - val_loss: 1.1094 - val_accuracy: 0.5719\n",
            "Epoch 3/200\n",
            "113/174 [==================>...........] - ETA: 0s - loss: 1.2313 - accuracy: 0.5066\n",
            "Epoch 3: val_accuracy improved from 0.57186 to 0.66063, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 1.1808 - accuracy: 0.5283 - val_loss: 0.9389 - val_accuracy: 0.6606\n",
            "Epoch 4/200\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.9819 - accuracy: 0.5952\n",
            "Epoch 4: val_accuracy improved from 0.66063 to 0.68176, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.9819 - accuracy: 0.5952 - val_loss: 0.8869 - val_accuracy: 0.6818\n",
            "Epoch 5/200\n",
            "173/174 [============================>.] - ETA: 0s - loss: 0.8880 - accuracy: 0.6449\n",
            "Epoch 5: val_accuracy did not improve from 0.68176\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.8874 - accuracy: 0.6446 - val_loss: 0.8577 - val_accuracy: 0.6636\n",
            "Epoch 6/200\n",
            "124/174 [====================>.........] - ETA: 0s - loss: 0.8121 - accuracy: 0.6613\n",
            "Epoch 6: val_accuracy did not improve from 0.68176\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.8098 - accuracy: 0.6626 - val_loss: 0.8789 - val_accuracy: 0.6745\n",
            "Epoch 7/200\n",
            "125/174 [====================>.........] - ETA: 0s - loss: 0.7527 - accuracy: 0.7015\n",
            "Epoch 7: val_accuracy improved from 0.68176 to 0.74155, saving model to weights.best.hdf5\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.7384 - accuracy: 0.7108 - val_loss: 0.8075 - val_accuracy: 0.7415\n",
            "Epoch 8/200\n",
            "171/174 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.7240\n",
            "Epoch 8: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.6869 - accuracy: 0.7249 - val_loss: 0.9166 - val_accuracy: 0.6679\n",
            "Epoch 9/200\n",
            "124/174 [====================>.........] - ETA: 0s - loss: 0.6519 - accuracy: 0.7429\n",
            "Epoch 9: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.6505 - accuracy: 0.7429 - val_loss: 0.9947 - val_accuracy: 0.6679\n",
            "Epoch 10/200\n",
            "125/174 [====================>.........] - ETA: 0s - loss: 0.6422 - accuracy: 0.7535\n",
            "Epoch 10: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.6322 - accuracy: 0.7587 - val_loss: 0.9248 - val_accuracy: 0.6739\n",
            "Epoch 11/200\n",
            "122/174 [====================>.........] - ETA: 0s - loss: 0.5895 - accuracy: 0.7777\n",
            "Epoch 11: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.5866 - accuracy: 0.7775 - val_loss: 1.0462 - val_accuracy: 0.6667\n",
            "Epoch 12/200\n",
            "122/174 [====================>.........] - ETA: 0s - loss: 0.5747 - accuracy: 0.7772\n",
            "Epoch 12: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.7753 - val_loss: 1.0764 - val_accuracy: 0.6552\n",
            "Epoch 13/200\n",
            "114/174 [==================>...........] - ETA: 0s - loss: 0.5474 - accuracy: 0.7911\n",
            "Epoch 13: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.5528 - accuracy: 0.7843 - val_loss: 0.9184 - val_accuracy: 0.6908\n",
            "Epoch 14/200\n",
            "169/174 [============================>.] - ETA: 0s - loss: 0.5080 - accuracy: 0.8084\n",
            "Epoch 14: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.5103 - accuracy: 0.8073 - val_loss: 1.1085 - val_accuracy: 0.6679\n",
            "Epoch 15/200\n",
            "123/174 [====================>.........] - ETA: 0s - loss: 0.5063 - accuracy: 0.8039\n",
            "Epoch 15: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.5046 - accuracy: 0.8019 - val_loss: 1.1515 - val_accuracy: 0.6510\n",
            "Epoch 16/200\n",
            "122/174 [====================>.........] - ETA: 0s - loss: 0.4928 - accuracy: 0.8079\n",
            "Epoch 16: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4890 - accuracy: 0.8127 - val_loss: 1.1097 - val_accuracy: 0.6552\n",
            "Epoch 17/200\n",
            "121/174 [===================>..........] - ETA: 0s - loss: 0.4846 - accuracy: 0.8259\n",
            "Epoch 17: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.8279 - val_loss: 1.3666 - val_accuracy: 0.6582\n",
            "Epoch 18/200\n",
            "124/174 [====================>.........] - ETA: 0s - loss: 0.4325 - accuracy: 0.8372\n",
            "Epoch 18: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.8347 - val_loss: 1.4034 - val_accuracy: 0.6310\n",
            "Epoch 19/200\n",
            "123/174 [====================>.........] - ETA: 0s - loss: 0.4698 - accuracy: 0.8237\n",
            "Epoch 19: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.8282 - val_loss: 1.2938 - val_accuracy: 0.6522\n",
            "Epoch 20/200\n",
            "127/174 [====================>.........] - ETA: 0s - loss: 0.4211 - accuracy: 0.8474\n",
            "Epoch 20: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4151 - accuracy: 0.8484 - val_loss: 1.4055 - val_accuracy: 0.6389\n",
            "Epoch 21/200\n",
            "127/174 [====================>.........] - ETA: 0s - loss: 0.4037 - accuracy: 0.8548\n",
            "Epoch 21: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.4107 - accuracy: 0.8534 - val_loss: 1.1655 - val_accuracy: 0.6643\n",
            "Epoch 22/200\n",
            "125/174 [====================>.........] - ETA: 0s - loss: 0.3908 - accuracy: 0.8555\n",
            "Epoch 22: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3877 - accuracy: 0.8614 - val_loss: 1.4224 - val_accuracy: 0.6075\n",
            "Epoch 23/200\n",
            "122/174 [====================>.........] - ETA: 0s - loss: 0.4005 - accuracy: 0.8525\n",
            "Epoch 23: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3920 - accuracy: 0.8563 - val_loss: 1.4420 - val_accuracy: 0.6606\n",
            "Epoch 24/200\n",
            "173/174 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8548\n",
            "Epoch 24: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3942 - accuracy: 0.8545 - val_loss: 1.1666 - val_accuracy: 0.6733\n",
            "Epoch 25/200\n",
            "122/174 [====================>.........] - ETA: 0s - loss: 0.3833 - accuracy: 0.8586\n",
            "Epoch 25: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8675 - val_loss: 1.3288 - val_accuracy: 0.6667\n",
            "Epoch 26/200\n",
            "123/174 [====================>.........] - ETA: 0s - loss: 0.3950 - accuracy: 0.8679\n",
            "Epoch 26: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8700 - val_loss: 1.2691 - val_accuracy: 0.6673\n",
            "Epoch 27/200\n",
            "117/174 [===================>..........] - ETA: 0s - loss: 0.3579 - accuracy: 0.8771\n",
            "Epoch 27: val_accuracy did not improve from 0.74155\n",
            "174/174 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8714 - val_loss: 1.2311 - val_accuracy: 0.6787\n",
            "39/39 [==============================] - 0s 763us/step - loss: 2.7798 - accuracy: 0.6664\n",
            "[0.8714439868927002, 0.6787439584732056, 0.6663927435874939]\n",
            "H\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "179/194 [==========================>...] - ETA: 0s - loss: 1.8634 - accuracy: 0.2647\n",
            "Epoch 1: val_accuracy improved from -inf to 0.52470, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 1s 2ms/step - loss: 1.8336 - accuracy: 0.2770 - val_loss: 1.4079 - val_accuracy: 0.5247\n",
            "Epoch 2/200\n",
            "176/194 [==========================>...] - ETA: 0s - loss: 1.3223 - accuracy: 0.4819\n",
            "Epoch 2: val_accuracy improved from 0.52470 to 0.74401, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 1.3155 - accuracy: 0.4834 - val_loss: 1.0429 - val_accuracy: 0.7440\n",
            "Epoch 3/200\n",
            "176/194 [==========================>...] - ETA: 0s - loss: 1.1001 - accuracy: 0.5565\n",
            "Epoch 3: val_accuracy improved from 0.74401 to 0.78443, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 1.0824 - accuracy: 0.5673 - val_loss: 0.8289 - val_accuracy: 0.7844\n",
            "Epoch 4/200\n",
            "172/194 [=========================>....] - ETA: 0s - loss: 0.9557 - accuracy: 0.6225\n",
            "Epoch 4: val_accuracy improved from 0.78443 to 0.88473, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.9482 - accuracy: 0.6277 - val_loss: 0.7683 - val_accuracy: 0.8847\n",
            "Epoch 5/200\n",
            "157/194 [=======================>......] - ETA: 0s - loss: 0.8535 - accuracy: 0.6712\n",
            "Epoch 5: val_accuracy did not improve from 0.88473\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.8418 - accuracy: 0.6755 - val_loss: 0.6867 - val_accuracy: 0.8481\n",
            "Epoch 6/200\n",
            "188/194 [============================>.] - ETA: 0s - loss: 0.7367 - accuracy: 0.7314\n",
            "Epoch 6: val_accuracy did not improve from 0.88473\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.7358 - accuracy: 0.7314 - val_loss: 0.6455 - val_accuracy: 0.7979\n",
            "Epoch 7/200\n",
            "188/194 [============================>.] - ETA: 0s - loss: 0.6719 - accuracy: 0.7460\n",
            "Epoch 7: val_accuracy did not improve from 0.88473\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.6697 - accuracy: 0.7459 - val_loss: 0.6325 - val_accuracy: 0.8555\n",
            "Epoch 8/200\n",
            "187/194 [===========================>..] - ETA: 0s - loss: 0.6065 - accuracy: 0.7771\n",
            "Epoch 8: val_accuracy did not improve from 0.88473\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.6046 - accuracy: 0.7769 - val_loss: 0.5912 - val_accuracy: 0.8683\n",
            "Epoch 9/200\n",
            "187/194 [===========================>..] - ETA: 0s - loss: 0.5702 - accuracy: 0.7888\n",
            "Epoch 9: val_accuracy did not improve from 0.88473\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.5697 - accuracy: 0.7895 - val_loss: 0.5910 - val_accuracy: 0.8256\n",
            "Epoch 10/200\n",
            "175/194 [==========================>...] - ETA: 0s - loss: 0.5144 - accuracy: 0.8111\n",
            "Epoch 10: val_accuracy did not improve from 0.88473\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.8101 - val_loss: 0.5372 - val_accuracy: 0.8847\n",
            "Epoch 11/200\n",
            "186/194 [===========================>..] - ETA: 0s - loss: 0.4931 - accuracy: 0.8155\n",
            "Epoch 11: val_accuracy did not improve from 0.88473\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4928 - accuracy: 0.8156 - val_loss: 0.6778 - val_accuracy: 0.8234\n",
            "Epoch 12/200\n",
            "187/194 [===========================>..] - ETA: 0s - loss: 0.4484 - accuracy: 0.8356\n",
            "Epoch 12: val_accuracy improved from 0.88473 to 0.88772, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4494 - accuracy: 0.8347 - val_loss: 0.6042 - val_accuracy: 0.8877\n",
            "Epoch 13/200\n",
            "138/194 [====================>.........] - ETA: 0s - loss: 0.4356 - accuracy: 0.8469\n",
            "Epoch 13: val_accuracy improved from 0.88772 to 0.89671, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4339 - accuracy: 0.8482 - val_loss: 0.5619 - val_accuracy: 0.8967\n",
            "Epoch 14/200\n",
            "172/194 [=========================>....] - ETA: 0s - loss: 0.4022 - accuracy: 0.8430\n",
            "Epoch 14: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4008 - accuracy: 0.8463 - val_loss: 0.6316 - val_accuracy: 0.8638\n",
            "Epoch 15/200\n",
            "180/194 [==========================>...] - ETA: 0s - loss: 0.3937 - accuracy: 0.8594\n",
            "Epoch 15: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3965 - accuracy: 0.8570 - val_loss: 0.6135 - val_accuracy: 0.8922\n",
            "Epoch 16/200\n",
            "181/194 [==========================>...] - ETA: 0s - loss: 0.3774 - accuracy: 0.8615\n",
            "Epoch 16: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8608 - val_loss: 0.6082 - val_accuracy: 0.8383\n",
            "Epoch 17/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.3551 - accuracy: 0.8767\n",
            "Epoch 17: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8757 - val_loss: 0.7525 - val_accuracy: 0.7807\n",
            "Epoch 18/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.3426 - accuracy: 0.8798\n",
            "Epoch 18: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8799 - val_loss: 0.6861 - val_accuracy: 0.8563\n",
            "Epoch 19/200\n",
            "183/194 [===========================>..] - ETA: 0s - loss: 0.3396 - accuracy: 0.8777\n",
            "Epoch 19: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3433 - accuracy: 0.8760 - val_loss: 0.7960 - val_accuracy: 0.7657\n",
            "Epoch 20/200\n",
            "185/194 [===========================>..] - ETA: 0s - loss: 0.3201 - accuracy: 0.8831\n",
            "Epoch 20: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8831 - val_loss: 0.5797 - val_accuracy: 0.8533\n",
            "Epoch 21/200\n",
            "171/194 [=========================>....] - ETA: 0s - loss: 0.3073 - accuracy: 0.8900\n",
            "Epoch 21: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3084 - accuracy: 0.8902 - val_loss: 0.6817 - val_accuracy: 0.8698\n",
            "Epoch 22/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.2990 - accuracy: 0.9004\n",
            "Epoch 22: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3048 - accuracy: 0.8986 - val_loss: 0.5810 - val_accuracy: 0.8413\n",
            "Epoch 23/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.3061 - accuracy: 0.8901\n",
            "Epoch 23: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3095 - accuracy: 0.8886 - val_loss: 0.6951 - val_accuracy: 0.7732\n",
            "Epoch 24/200\n",
            "186/194 [===========================>..] - ETA: 0s - loss: 0.2798 - accuracy: 0.8989\n",
            "Epoch 24: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2796 - accuracy: 0.8976 - val_loss: 0.6096 - val_accuracy: 0.8436\n",
            "Epoch 25/200\n",
            "181/194 [==========================>...] - ETA: 0s - loss: 0.2867 - accuracy: 0.9012\n",
            "Epoch 25: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.9015 - val_loss: 0.5660 - val_accuracy: 0.8638\n",
            "Epoch 26/200\n",
            "187/194 [===========================>..] - ETA: 0s - loss: 0.2776 - accuracy: 0.9001\n",
            "Epoch 26: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.9005 - val_loss: 0.7276 - val_accuracy: 0.8780\n",
            "Epoch 27/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.2820 - accuracy: 0.9008\n",
            "Epoch 27: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.9035 - val_loss: 0.7333 - val_accuracy: 0.8301\n",
            "Epoch 28/200\n",
            "184/194 [===========================>..] - ETA: 0s - loss: 0.2499 - accuracy: 0.9158\n",
            "Epoch 28: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.9151 - val_loss: 0.7059 - val_accuracy: 0.8563\n",
            "Epoch 29/200\n",
            "163/194 [========================>.....] - ETA: 0s - loss: 0.2786 - accuracy: 0.9057\n",
            "Epoch 29: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2730 - accuracy: 0.9077 - val_loss: 0.7357 - val_accuracy: 0.8510\n",
            "Epoch 30/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.2598 - accuracy: 0.9179\n",
            "Epoch 30: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.9180 - val_loss: 0.7215 - val_accuracy: 0.8376\n",
            "Epoch 31/200\n",
            "161/194 [=======================>......] - ETA: 0s - loss: 0.2495 - accuracy: 0.9099\n",
            "Epoch 31: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2436 - accuracy: 0.9135 - val_loss: 0.7977 - val_accuracy: 0.7799\n",
            "Epoch 32/200\n",
            "184/194 [===========================>..] - ETA: 0s - loss: 0.2536 - accuracy: 0.9093\n",
            "Epoch 32: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9096 - val_loss: 0.7509 - val_accuracy: 0.8331\n",
            "Epoch 33/200\n",
            "181/194 [==========================>...] - ETA: 0s - loss: 0.2329 - accuracy: 0.9192\n",
            "Epoch 33: val_accuracy did not improve from 0.89671\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9202 - val_loss: 0.7368 - val_accuracy: 0.8540\n",
            "39/39 [==============================] - 0s 763us/step - loss: 0.3464 - accuracy: 0.8463\n",
            "[0.9202454090118408, 0.8540419340133667, 0.846343457698822]\n",
            "K\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "121/187 [==================>...........] - ETA: 0s - loss: 1.9483 - accuracy: 0.2335\n",
            "Epoch 1: val_accuracy improved from -inf to 0.55378, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 1s 2ms/step - loss: 1.8406 - accuracy: 0.2781 - val_loss: 1.4813 - val_accuracy: 0.5538\n",
            "Epoch 2/200\n",
            "167/187 [=========================>....] - ETA: 0s - loss: 1.3167 - accuracy: 0.4910\n",
            "Epoch 2: val_accuracy improved from 0.55378 to 0.70298, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 1.2960 - accuracy: 0.5023 - val_loss: 1.0950 - val_accuracy: 0.7030\n",
            "Epoch 3/200\n",
            "124/187 [==================>...........] - ETA: 0s - loss: 1.0238 - accuracy: 0.5953\n",
            "Epoch 3: val_accuracy improved from 0.70298 to 0.74462, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.9902 - accuracy: 0.6116 - val_loss: 0.9686 - val_accuracy: 0.7446\n",
            "Epoch 4/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.8243 - accuracy: 0.6855\n",
            "Epoch 4: val_accuracy did not improve from 0.74462\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.8243 - accuracy: 0.6855 - val_loss: 0.9194 - val_accuracy: 0.7432\n",
            "Epoch 5/200\n",
            "133/187 [====================>.........] - ETA: 0s - loss: 0.6744 - accuracy: 0.7627\n",
            "Epoch 5: val_accuracy did not improve from 0.74462\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.6726 - accuracy: 0.7610 - val_loss: 0.9796 - val_accuracy: 0.7363\n",
            "Epoch 6/200\n",
            "131/187 [====================>.........] - ETA: 0s - loss: 0.5927 - accuracy: 0.7901\n",
            "Epoch 6: val_accuracy improved from 0.74462 to 0.76822, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.7991 - val_loss: 0.7841 - val_accuracy: 0.7682\n",
            "Epoch 7/200\n",
            "120/187 [==================>...........] - ETA: 0s - loss: 0.5142 - accuracy: 0.8188\n",
            "Epoch 7: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.5133 - accuracy: 0.8178 - val_loss: 0.9442 - val_accuracy: 0.6884\n",
            "Epoch 8/200\n",
            "133/187 [====================>.........] - ETA: 0s - loss: 0.4624 - accuracy: 0.8407\n",
            "Epoch 8: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.4475 - accuracy: 0.8486 - val_loss: 0.9122 - val_accuracy: 0.6155\n",
            "Epoch 9/200\n",
            "134/187 [====================>.........] - ETA: 0s - loss: 0.4152 - accuracy: 0.8503\n",
            "Epoch 9: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.4161 - accuracy: 0.8553 - val_loss: 0.8763 - val_accuracy: 0.5940\n",
            "Epoch 10/200\n",
            "178/187 [===========================>..] - ETA: 0s - loss: 0.3814 - accuracy: 0.8736\n",
            "Epoch 10: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8747 - val_loss: 0.9359 - val_accuracy: 0.6093\n",
            "Epoch 11/200\n",
            "135/187 [====================>.........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8954\n",
            "Epoch 11: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3421 - accuracy: 0.8910 - val_loss: 1.0876 - val_accuracy: 0.5628\n",
            "Epoch 12/200\n",
            "134/187 [====================>.........] - ETA: 0s - loss: 0.2974 - accuracy: 0.9086\n",
            "Epoch 12: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3090 - accuracy: 0.9001 - val_loss: 0.9581 - val_accuracy: 0.5815\n",
            "Epoch 13/200\n",
            "133/187 [====================>.........] - ETA: 0s - loss: 0.2746 - accuracy: 0.9154\n",
            "Epoch 13: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2792 - accuracy: 0.9134 - val_loss: 0.8342 - val_accuracy: 0.6773\n",
            "Epoch 14/200\n",
            "133/187 [====================>.........] - ETA: 0s - loss: 0.2599 - accuracy: 0.9182\n",
            "Epoch 14: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.9158 - val_loss: 1.0063 - val_accuracy: 0.5913\n",
            "Epoch 15/200\n",
            "135/187 [====================>.........] - ETA: 0s - loss: 0.2607 - accuracy: 0.9250\n",
            "Epoch 15: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.9265 - val_loss: 1.2109 - val_accuracy: 0.5406\n",
            "Epoch 16/200\n",
            "134/187 [====================>.........] - ETA: 0s - loss: 0.2384 - accuracy: 0.9338\n",
            "Epoch 16: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9305 - val_loss: 1.2487 - val_accuracy: 0.5600\n",
            "Epoch 17/200\n",
            "121/187 [==================>...........] - ETA: 0s - loss: 0.2344 - accuracy: 0.9329\n",
            "Epoch 17: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2313 - accuracy: 0.9332 - val_loss: 1.1351 - val_accuracy: 0.6003\n",
            "Epoch 18/200\n",
            "180/187 [===========================>..] - ETA: 0s - loss: 0.2169 - accuracy: 0.9399\n",
            "Epoch 18: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9382 - val_loss: 1.2528 - val_accuracy: 0.5920\n",
            "Epoch 19/200\n",
            "132/187 [====================>.........] - ETA: 0s - loss: 0.1943 - accuracy: 0.9479\n",
            "Epoch 19: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9462 - val_loss: 1.4189 - val_accuracy: 0.5517\n",
            "Epoch 20/200\n",
            "136/187 [====================>.........] - ETA: 0s - loss: 0.2063 - accuracy: 0.9288\n",
            "Epoch 20: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9338 - val_loss: 1.4523 - val_accuracy: 0.5635\n",
            "Epoch 21/200\n",
            "135/187 [====================>.........] - ETA: 0s - loss: 0.1872 - accuracy: 0.9481\n",
            "Epoch 21: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9475 - val_loss: 1.2198 - val_accuracy: 0.5864\n",
            "Epoch 22/200\n",
            "131/187 [====================>.........] - ETA: 0s - loss: 0.2033 - accuracy: 0.9447\n",
            "Epoch 22: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9485 - val_loss: 1.3143 - val_accuracy: 0.5933\n",
            "Epoch 23/200\n",
            "136/187 [====================>.........] - ETA: 0s - loss: 0.2062 - accuracy: 0.9416\n",
            "Epoch 23: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9422 - val_loss: 1.3788 - val_accuracy: 0.5774\n",
            "Epoch 24/200\n",
            "135/187 [====================>.........] - ETA: 0s - loss: 0.1830 - accuracy: 0.9481\n",
            "Epoch 24: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9485 - val_loss: 1.2166 - val_accuracy: 0.6246\n",
            "Epoch 25/200\n",
            "131/187 [====================>.........] - ETA: 0s - loss: 0.1769 - accuracy: 0.9490\n",
            "Epoch 25: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9469 - val_loss: 1.2099 - val_accuracy: 0.6051\n",
            "Epoch 26/200\n",
            "182/187 [============================>.] - ETA: 0s - loss: 0.1770 - accuracy: 0.9461\n",
            "Epoch 26: val_accuracy did not improve from 0.76822\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9462 - val_loss: 1.2056 - val_accuracy: 0.6371\n",
            "39/39 [==============================] - 0s 740us/step - loss: 1.5159 - accuracy: 0.7198\n",
            "[0.946189820766449, 0.6370576024055481, 0.7198027968406677]\n",
            "H\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 1.9189 - accuracy: 0.2193\n",
            "Epoch 1: val_accuracy improved from -inf to 0.43659, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 1s 2ms/step - loss: 1.8565 - accuracy: 0.2517 - val_loss: 1.5115 - val_accuracy: 0.4366\n",
            "Epoch 2/200\n",
            "111/167 [==================>...........] - ETA: 0s - loss: 1.5054 - accuracy: 0.3930\n",
            "Epoch 2: val_accuracy improved from 0.43659 to 0.62621, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 1.4576 - accuracy: 0.4063 - val_loss: 1.1626 - val_accuracy: 0.6262\n",
            "Epoch 3/200\n",
            "167/167 [==============================] - ETA: 0s - loss: 1.2201 - accuracy: 0.4816\n",
            "Epoch 3: val_accuracy improved from 0.62621 to 0.64070, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 1.2201 - accuracy: 0.4816 - val_loss: 0.9817 - val_accuracy: 0.6407\n",
            "Epoch 4/200\n",
            "109/167 [==================>...........] - ETA: 0s - loss: 1.0606 - accuracy: 0.5751\n",
            "Epoch 4: val_accuracy improved from 0.64070 to 0.72585, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 1.0402 - accuracy: 0.5794 - val_loss: 0.8755 - val_accuracy: 0.7258\n",
            "Epoch 5/200\n",
            "115/167 [===================>..........] - ETA: 0s - loss: 0.9512 - accuracy: 0.6087\n",
            "Epoch 5: val_accuracy improved from 0.72585 to 0.76147, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.9342 - accuracy: 0.6163 - val_loss: 0.8278 - val_accuracy: 0.7615\n",
            "Epoch 6/200\n",
            "113/167 [===================>..........] - ETA: 0s - loss: 0.8480 - accuracy: 0.6521\n",
            "Epoch 6: val_accuracy improved from 0.76147 to 0.76691, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.8410 - accuracy: 0.6561 - val_loss: 0.8084 - val_accuracy: 0.7669\n",
            "Epoch 7/200\n",
            "124/167 [=====================>........] - ETA: 0s - loss: 0.7855 - accuracy: 0.6764\n",
            "Epoch 7: val_accuracy did not improve from 0.76691\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.7758 - accuracy: 0.6817 - val_loss: 0.7550 - val_accuracy: 0.7548\n",
            "Epoch 8/200\n",
            "118/167 [====================>.........] - ETA: 0s - loss: 0.7021 - accuracy: 0.7150\n",
            "Epoch 8: val_accuracy improved from 0.76691 to 0.78442, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.6985 - accuracy: 0.7167 - val_loss: 0.7676 - val_accuracy: 0.7844\n",
            "Epoch 9/200\n",
            "110/167 [==================>...........] - ETA: 0s - loss: 0.6607 - accuracy: 0.7278\n",
            "Epoch 9: val_accuracy did not improve from 0.78442\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.6717 - accuracy: 0.7220 - val_loss: 0.7639 - val_accuracy: 0.7041\n",
            "Epoch 10/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.6181 - accuracy: 0.7490\n",
            "Epoch 10: val_accuracy improved from 0.78442 to 0.80556, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.6172 - accuracy: 0.7509 - val_loss: 0.6956 - val_accuracy: 0.8056\n",
            "Epoch 11/200\n",
            "116/167 [===================>..........] - ETA: 0s - loss: 0.5632 - accuracy: 0.7726\n",
            "Epoch 11: val_accuracy did not improve from 0.80556\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.7622 - val_loss: 0.7126 - val_accuracy: 0.7899\n",
            "Epoch 12/200\n",
            "117/167 [====================>.........] - ETA: 0s - loss: 0.5677 - accuracy: 0.7730\n",
            "Epoch 12: val_accuracy did not improve from 0.80556\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.5601 - accuracy: 0.7724 - val_loss: 0.6053 - val_accuracy: 0.7989\n",
            "Epoch 13/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.5497 - accuracy: 0.7705\n",
            "Epoch 13: val_accuracy improved from 0.80556 to 0.82729, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.5445 - accuracy: 0.7735 - val_loss: 0.6446 - val_accuracy: 0.8273\n",
            "Epoch 14/200\n",
            "164/167 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.7900\n",
            "Epoch 14: val_accuracy did not improve from 0.82729\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.5195 - accuracy: 0.7901 - val_loss: 0.6010 - val_accuracy: 0.8255\n",
            "Epoch 15/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.4962 - accuracy: 0.8017\n",
            "Epoch 15: val_accuracy did not improve from 0.82729\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.8036 - val_loss: 0.7530 - val_accuracy: 0.7886\n",
            "Epoch 16/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.4967 - accuracy: 0.7988\n",
            "Epoch 16: val_accuracy did not improve from 0.82729\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4952 - accuracy: 0.7976 - val_loss: 0.6681 - val_accuracy: 0.8213\n",
            "Epoch 17/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.4704 - accuracy: 0.8069\n",
            "Epoch 17: val_accuracy did not improve from 0.82729\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.8040 - val_loss: 0.6724 - val_accuracy: 0.7826\n",
            "Epoch 18/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.4806 - accuracy: 0.7990\n",
            "Epoch 18: val_accuracy did not improve from 0.82729\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.8044 - val_loss: 0.8111 - val_accuracy: 0.8098\n",
            "Epoch 19/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.4462 - accuracy: 0.8162\n",
            "Epoch 19: val_accuracy did not improve from 0.82729\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.8100 - val_loss: 0.6212 - val_accuracy: 0.8267\n",
            "Epoch 20/200\n",
            "118/167 [====================>.........] - ETA: 0s - loss: 0.4143 - accuracy: 0.8316\n",
            "Epoch 20: val_accuracy improved from 0.82729 to 0.83213, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4264 - accuracy: 0.8266 - val_loss: 0.6157 - val_accuracy: 0.8321\n",
            "Epoch 21/200\n",
            "159/167 [===========================>..] - ETA: 0s - loss: 0.4245 - accuracy: 0.8314\n",
            "Epoch 21: val_accuracy improved from 0.83213 to 0.83816, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8315 - val_loss: 0.5794 - val_accuracy: 0.8382\n",
            "Epoch 22/200\n",
            "113/167 [===================>..........] - ETA: 0s - loss: 0.4157 - accuracy: 0.8324\n",
            "Epoch 22: val_accuracy did not improve from 0.83816\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.8337 - val_loss: 0.6274 - val_accuracy: 0.8267\n",
            "Epoch 23/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.4217 - accuracy: 0.8365\n",
            "Epoch 23: val_accuracy improved from 0.83816 to 0.84360, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4150 - accuracy: 0.8397 - val_loss: 0.5499 - val_accuracy: 0.8436\n",
            "Epoch 24/200\n",
            "164/167 [============================>.] - ETA: 0s - loss: 0.4023 - accuracy: 0.8338\n",
            "Epoch 24: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4015 - accuracy: 0.8337 - val_loss: 0.5926 - val_accuracy: 0.8382\n",
            "Epoch 25/200\n",
            "124/167 [=====================>........] - ETA: 0s - loss: 0.3923 - accuracy: 0.8337\n",
            "Epoch 25: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.4038 - accuracy: 0.8288 - val_loss: 0.5552 - val_accuracy: 0.8382\n",
            "Epoch 26/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3841 - accuracy: 0.8491\n",
            "Epoch 26: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.8495 - val_loss: 0.6600 - val_accuracy: 0.8182\n",
            "Epoch 27/200\n",
            "118/167 [====================>.........] - ETA: 0s - loss: 0.3867 - accuracy: 0.8411\n",
            "Epoch 27: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3866 - accuracy: 0.8439 - val_loss: 0.6486 - val_accuracy: 0.8267\n",
            "Epoch 28/200\n",
            "124/167 [=====================>........] - ETA: 0s - loss: 0.3566 - accuracy: 0.8564\n",
            "Epoch 28: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3655 - accuracy: 0.8567 - val_loss: 0.5929 - val_accuracy: 0.8424\n",
            "Epoch 29/200\n",
            "114/167 [===================>..........] - ETA: 0s - loss: 0.3706 - accuracy: 0.8531\n",
            "Epoch 29: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3653 - accuracy: 0.8585 - val_loss: 0.6454 - val_accuracy: 0.8406\n",
            "Epoch 30/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3744 - accuracy: 0.8613\n",
            "Epoch 30: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8638 - val_loss: 0.6222 - val_accuracy: 0.8303\n",
            "Epoch 31/200\n",
            "124/167 [=====================>........] - ETA: 0s - loss: 0.3447 - accuracy: 0.8654\n",
            "Epoch 31: val_accuracy did not improve from 0.84360\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8683 - val_loss: 0.6429 - val_accuracy: 0.8231\n",
            "Epoch 32/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8603\n",
            "Epoch 32: val_accuracy improved from 0.84360 to 0.85688, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8585 - val_loss: 0.4617 - val_accuracy: 0.8569\n",
            "Epoch 33/200\n",
            "113/167 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8628\n",
            "Epoch 33: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3602 - accuracy: 0.8600 - val_loss: 0.6355 - val_accuracy: 0.8279\n",
            "Epoch 34/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.3666 - accuracy: 0.8619\n",
            "Epoch 34: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.8661 - val_loss: 0.5396 - val_accuracy: 0.8430\n",
            "Epoch 35/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3272 - accuracy: 0.8786\n",
            "Epoch 35: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8774 - val_loss: 0.5948 - val_accuracy: 0.8454\n",
            "Epoch 36/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8729\n",
            "Epoch 36: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.8732 - val_loss: 0.5588 - val_accuracy: 0.8484\n",
            "Epoch 37/200\n",
            "115/167 [===================>..........] - ETA: 0s - loss: 0.3463 - accuracy: 0.8668\n",
            "Epoch 37: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3447 - accuracy: 0.8664 - val_loss: 0.6077 - val_accuracy: 0.8508\n",
            "Epoch 38/200\n",
            "124/167 [=====================>........] - ETA: 0s - loss: 0.3232 - accuracy: 0.8896\n",
            "Epoch 38: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3302 - accuracy: 0.8845 - val_loss: 0.6623 - val_accuracy: 0.8406\n",
            "Epoch 39/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.3269 - accuracy: 0.8771\n",
            "Epoch 39: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3262 - accuracy: 0.8777 - val_loss: 0.6327 - val_accuracy: 0.8339\n",
            "Epoch 40/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.3004 - accuracy: 0.8859\n",
            "Epoch 40: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.8864 - val_loss: 0.6662 - val_accuracy: 0.8466\n",
            "Epoch 41/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.2953 - accuracy: 0.8976\n",
            "Epoch 41: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8965 - val_loss: 0.5994 - val_accuracy: 0.8472\n",
            "Epoch 42/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.3007 - accuracy: 0.8899\n",
            "Epoch 42: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3023 - accuracy: 0.8864 - val_loss: 0.5149 - val_accuracy: 0.8514\n",
            "Epoch 43/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2932 - accuracy: 0.8951\n",
            "Epoch 43: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3004 - accuracy: 0.8928 - val_loss: 0.6158 - val_accuracy: 0.8527\n",
            "Epoch 44/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3024 - accuracy: 0.8928\n",
            "Epoch 44: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8909 - val_loss: 0.5806 - val_accuracy: 0.8345\n",
            "Epoch 45/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.3074 - accuracy: 0.8857\n",
            "Epoch 45: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.8871 - val_loss: 0.6575 - val_accuracy: 0.8472\n",
            "Epoch 46/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.2925 - accuracy: 0.8953\n",
            "Epoch 46: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8883 - val_loss: 0.6694 - val_accuracy: 0.8394\n",
            "Epoch 47/200\n",
            "117/167 [====================>.........] - ETA: 0s - loss: 0.2921 - accuracy: 0.8974\n",
            "Epoch 47: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.9003 - val_loss: 0.6017 - val_accuracy: 0.8557\n",
            "Epoch 48/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.2740 - accuracy: 0.9037\n",
            "Epoch 48: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2789 - accuracy: 0.9011 - val_loss: 0.6421 - val_accuracy: 0.8194\n",
            "Epoch 49/200\n",
            "117/167 [====================>.........] - ETA: 0s - loss: 0.2822 - accuracy: 0.9017\n",
            "Epoch 49: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2670 - accuracy: 0.9074 - val_loss: 0.7190 - val_accuracy: 0.8357\n",
            "Epoch 50/200\n",
            "117/167 [====================>.........] - ETA: 0s - loss: 0.2839 - accuracy: 0.9022\n",
            "Epoch 50: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.9048 - val_loss: 0.6931 - val_accuracy: 0.8303\n",
            "Epoch 51/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.2674 - accuracy: 0.9010\n",
            "Epoch 51: val_accuracy did not improve from 0.85688\n",
            "167/167 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.9003 - val_loss: 0.7259 - val_accuracy: 0.8406\n",
            "Epoch 52/200\n",
            "114/167 [===================>..........] - ETA: 0s - loss: 0.2671 - accuracy: 0.9008\n",
            "Epoch 52: val_accuracy improved from 0.85688 to 0.86836, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.9029 - val_loss: 0.5299 - val_accuracy: 0.8684\n",
            "Epoch 53/200\n",
            "115/167 [===================>..........] - ETA: 0s - loss: 0.2797 - accuracy: 0.9022\n",
            "Epoch 53: val_accuracy did not improve from 0.86836\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2887 - accuracy: 0.8992 - val_loss: 0.7779 - val_accuracy: 0.8237\n",
            "Epoch 54/200\n",
            "118/167 [====================>.........] - ETA: 0s - loss: 0.2859 - accuracy: 0.9041\n",
            "Epoch 54: val_accuracy improved from 0.86836 to 0.87379, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2728 - accuracy: 0.9078 - val_loss: 0.5377 - val_accuracy: 0.8738\n",
            "Epoch 55/200\n",
            "116/167 [===================>..........] - ETA: 0s - loss: 0.2995 - accuracy: 0.8917\n",
            "Epoch 55: val_accuracy did not improve from 0.87379\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.8992 - val_loss: 0.6293 - val_accuracy: 0.8478\n",
            "Epoch 56/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2522 - accuracy: 0.9086\n",
            "Epoch 56: val_accuracy did not improve from 0.87379\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.9074 - val_loss: 0.5105 - val_accuracy: 0.8647\n",
            "Epoch 57/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.2517 - accuracy: 0.9134\n",
            "Epoch 57: val_accuracy did not improve from 0.87379\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.9135 - val_loss: 0.5748 - val_accuracy: 0.8635\n",
            "Epoch 58/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.2584 - accuracy: 0.9065\n",
            "Epoch 58: val_accuracy did not improve from 0.87379\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.9097 - val_loss: 0.6472 - val_accuracy: 0.8575\n",
            "Epoch 59/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2328 - accuracy: 0.9261\n",
            "Epoch 59: val_accuracy improved from 0.87379 to 0.88104, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.9206 - val_loss: 0.5348 - val_accuracy: 0.8810\n",
            "Epoch 60/200\n",
            "113/167 [===================>..........] - ETA: 0s - loss: 0.2451 - accuracy: 0.9148\n",
            "Epoch 60: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.9116 - val_loss: 0.6336 - val_accuracy: 0.8587\n",
            "Epoch 61/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.2511 - accuracy: 0.9177\n",
            "Epoch 61: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.9090 - val_loss: 0.4994 - val_accuracy: 0.8786\n",
            "Epoch 62/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.2650 - accuracy: 0.9031\n",
            "Epoch 62: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.9033 - val_loss: 0.7902 - val_accuracy: 0.8321\n",
            "Epoch 63/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.2662 - accuracy: 0.9102\n",
            "Epoch 63: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.9127 - val_loss: 0.7747 - val_accuracy: 0.8321\n",
            "Epoch 64/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.2486 - accuracy: 0.9125\n",
            "Epoch 64: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2378 - accuracy: 0.9150 - val_loss: 0.6811 - val_accuracy: 0.8508\n",
            "Epoch 65/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2568 - accuracy: 0.9143\n",
            "Epoch 65: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.9131 - val_loss: 0.6790 - val_accuracy: 0.8653\n",
            "Epoch 66/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.2207 - accuracy: 0.9216\n",
            "Epoch 66: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9251 - val_loss: 0.7804 - val_accuracy: 0.8472\n",
            "Epoch 67/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2419 - accuracy: 0.9137\n",
            "Epoch 67: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9157 - val_loss: 0.6787 - val_accuracy: 0.8569\n",
            "Epoch 68/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2100 - accuracy: 0.9267\n",
            "Epoch 68: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9165 - val_loss: 0.9577 - val_accuracy: 0.8321\n",
            "Epoch 69/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.2500 - accuracy: 0.9060\n",
            "Epoch 69: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9138 - val_loss: 0.7704 - val_accuracy: 0.8424\n",
            "Epoch 70/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2364 - accuracy: 0.9236\n",
            "Epoch 70: val_accuracy did not improve from 0.88104\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9229 - val_loss: 0.7052 - val_accuracy: 0.8623\n",
            "Epoch 71/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2400 - accuracy: 0.9153\n",
            "Epoch 71: val_accuracy improved from 0.88104 to 0.88345, saving model to weights.best.hdf5\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9172 - val_loss: 0.5617 - val_accuracy: 0.8835\n",
            "Epoch 72/200\n",
            "161/167 [===========================>..] - ETA: 0s - loss: 0.2407 - accuracy: 0.9165\n",
            "Epoch 72: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2377 - accuracy: 0.9172 - val_loss: 0.6886 - val_accuracy: 0.8702\n",
            "Epoch 73/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.2267 - accuracy: 0.9242\n",
            "Epoch 73: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2183 - accuracy: 0.9266 - val_loss: 0.6831 - val_accuracy: 0.8575\n",
            "Epoch 74/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2422 - accuracy: 0.9194\n",
            "Epoch 74: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9214 - val_loss: 1.0450 - val_accuracy: 0.8200\n",
            "Epoch 75/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2384 - accuracy: 0.9174\n",
            "Epoch 75: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9153 - val_loss: 0.7556 - val_accuracy: 0.8569\n",
            "Epoch 76/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.2413 - accuracy: 0.9170\n",
            "Epoch 76: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2374 - accuracy: 0.9195 - val_loss: 0.8746 - val_accuracy: 0.8472\n",
            "Epoch 77/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.2412 - accuracy: 0.9182\n",
            "Epoch 77: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9236 - val_loss: 0.9667 - val_accuracy: 0.8448\n",
            "Epoch 78/200\n",
            "119/167 [====================>.........] - ETA: 0s - loss: 0.2399 - accuracy: 0.9107\n",
            "Epoch 78: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9161 - val_loss: 0.7072 - val_accuracy: 0.8472\n",
            "Epoch 79/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.2369 - accuracy: 0.9151\n",
            "Epoch 79: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.9161 - val_loss: 0.6608 - val_accuracy: 0.8653\n",
            "Epoch 80/200\n",
            "118/167 [====================>.........] - ETA: 0s - loss: 0.2331 - accuracy: 0.9264\n",
            "Epoch 80: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2260 - accuracy: 0.9289 - val_loss: 0.8131 - val_accuracy: 0.8394\n",
            "Epoch 81/200\n",
            "121/167 [====================>.........] - ETA: 0s - loss: 0.2095 - accuracy: 0.9287\n",
            "Epoch 81: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.9236 - val_loss: 0.7885 - val_accuracy: 0.8448\n",
            "Epoch 82/200\n",
            "123/167 [=====================>........] - ETA: 0s - loss: 0.2004 - accuracy: 0.9299\n",
            "Epoch 82: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9308 - val_loss: 0.7621 - val_accuracy: 0.8496\n",
            "Epoch 83/200\n",
            "125/167 [=====================>........] - ETA: 0s - loss: 0.2238 - accuracy: 0.9270\n",
            "Epoch 83: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9296 - val_loss: 0.7215 - val_accuracy: 0.8527\n",
            "Epoch 84/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.2232 - accuracy: 0.9252\n",
            "Epoch 84: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9270 - val_loss: 1.0810 - val_accuracy: 0.8454\n",
            "Epoch 85/200\n",
            "116/167 [===================>..........] - ETA: 0s - loss: 0.2355 - accuracy: 0.9213\n",
            "Epoch 85: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9278 - val_loss: 0.9794 - val_accuracy: 0.8442\n",
            "Epoch 86/200\n",
            "160/167 [===========================>..] - ETA: 0s - loss: 0.2027 - accuracy: 0.9301\n",
            "Epoch 86: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9296 - val_loss: 1.0690 - val_accuracy: 0.8249\n",
            "Epoch 87/200\n",
            "120/167 [====================>.........] - ETA: 0s - loss: 0.2071 - accuracy: 0.9385\n",
            "Epoch 87: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9383 - val_loss: 0.8288 - val_accuracy: 0.8533\n",
            "Epoch 88/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.2341 - accuracy: 0.9155\n",
            "Epoch 88: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2230 - accuracy: 0.9199 - val_loss: 0.9394 - val_accuracy: 0.8400\n",
            "Epoch 89/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.2148 - accuracy: 0.9232\n",
            "Epoch 89: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2216 - accuracy: 0.9206 - val_loss: 0.9987 - val_accuracy: 0.8357\n",
            "Epoch 90/200\n",
            "122/167 [====================>.........] - ETA: 0s - loss: 0.1929 - accuracy: 0.9355\n",
            "Epoch 90: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9342 - val_loss: 1.0130 - val_accuracy: 0.8388\n",
            "Epoch 91/200\n",
            "118/167 [====================>.........] - ETA: 0s - loss: 0.2235 - accuracy: 0.9253\n",
            "Epoch 91: val_accuracy did not improve from 0.88345\n",
            "167/167 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9217 - val_loss: 0.9081 - val_accuracy: 0.8454\n",
            "42/42 [==============================] - 0s 729us/step - loss: 2.0919 - accuracy: 0.6984\n",
            "[0.9217456579208374, 0.8454106450080872, 0.6983532905578613]\n",
            "H\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "184/194 [===========================>..] - ETA: 0s - loss: 1.8648 - accuracy: 0.2663\n",
            "Epoch 1: val_accuracy improved from -inf to 0.44700, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 1s 2ms/step - loss: 1.8527 - accuracy: 0.2722 - val_loss: 1.5737 - val_accuracy: 0.4470\n",
            "Epoch 2/200\n",
            "176/194 [==========================>...] - ETA: 0s - loss: 1.4242 - accuracy: 0.4403\n",
            "Epoch 2: val_accuracy improved from 0.44700 to 0.52095, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 1.4111 - accuracy: 0.4433 - val_loss: 1.2270 - val_accuracy: 0.5210\n",
            "Epoch 3/200\n",
            "175/194 [==========================>...] - ETA: 0s - loss: 1.1639 - accuracy: 0.5364\n",
            "Epoch 3: val_accuracy improved from 0.52095 to 0.57190, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 1.1545 - accuracy: 0.5396 - val_loss: 1.1413 - val_accuracy: 0.5719\n",
            "Epoch 4/200\n",
            "170/194 [=========================>....] - ETA: 0s - loss: 0.9967 - accuracy: 0.5960\n",
            "Epoch 4: val_accuracy improved from 0.57190 to 0.70008, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.9920 - accuracy: 0.6006 - val_loss: 1.0681 - val_accuracy: 0.7001\n",
            "Epoch 5/200\n",
            "164/194 [========================>.....] - ETA: 0s - loss: 0.8765 - accuracy: 0.6570\n",
            "Epoch 5: val_accuracy improved from 0.70008 to 0.74281, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.8690 - accuracy: 0.6606 - val_loss: 1.0498 - val_accuracy: 0.7428\n",
            "Epoch 6/200\n",
            "179/194 [==========================>...] - ETA: 0s - loss: 0.7946 - accuracy: 0.6952\n",
            "Epoch 6: val_accuracy improved from 0.74281 to 0.84224, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.7910 - accuracy: 0.6984 - val_loss: 0.8427 - val_accuracy: 0.8422\n",
            "Epoch 7/200\n",
            "179/194 [==========================>...] - ETA: 0s - loss: 0.7197 - accuracy: 0.7242\n",
            "Epoch 7: val_accuracy improved from 0.84224 to 0.84470, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.7188 - accuracy: 0.7259 - val_loss: 0.8042 - val_accuracy: 0.8447\n",
            "Epoch 8/200\n",
            "165/194 [========================>.....] - ETA: 0s - loss: 0.6494 - accuracy: 0.7598\n",
            "Epoch 8: val_accuracy improved from 0.84470 to 0.84634, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.6428 - accuracy: 0.7611 - val_loss: 0.8633 - val_accuracy: 0.8463\n",
            "Epoch 9/200\n",
            "184/194 [===========================>..] - ETA: 0s - loss: 0.5945 - accuracy: 0.7850\n",
            "Epoch 9: val_accuracy did not improve from 0.84634\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.5974 - accuracy: 0.7840 - val_loss: 0.7310 - val_accuracy: 0.8389\n",
            "Epoch 10/200\n",
            "173/194 [=========================>....] - ETA: 0s - loss: 0.5636 - accuracy: 0.7923\n",
            "Epoch 10: val_accuracy improved from 0.84634 to 0.84881, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.7933 - val_loss: 0.7549 - val_accuracy: 0.8488\n",
            "Epoch 11/200\n",
            "178/194 [==========================>...] - ETA: 0s - loss: 0.5249 - accuracy: 0.8097\n",
            "Epoch 11: val_accuracy did not improve from 0.84881\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.8095 - val_loss: 0.5455 - val_accuracy: 0.8365\n",
            "Epoch 12/200\n",
            "185/194 [===========================>..] - ETA: 0s - loss: 0.5151 - accuracy: 0.8081\n",
            "Epoch 12: val_accuracy improved from 0.84881 to 0.88496, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.5135 - accuracy: 0.8095 - val_loss: 0.4880 - val_accuracy: 0.8850\n",
            "Epoch 13/200\n",
            "178/194 [==========================>...] - ETA: 0s - loss: 0.4765 - accuracy: 0.8244\n",
            "Epoch 13: val_accuracy improved from 0.88496 to 0.88661, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4732 - accuracy: 0.8263 - val_loss: 0.4637 - val_accuracy: 0.8866\n",
            "Epoch 14/200\n",
            "178/194 [==========================>...] - ETA: 0s - loss: 0.4501 - accuracy: 0.8308\n",
            "Epoch 14: val_accuracy did not improve from 0.88661\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.8308 - val_loss: 0.4793 - val_accuracy: 0.8463\n",
            "Epoch 15/200\n",
            "166/194 [========================>.....] - ETA: 0s - loss: 0.4384 - accuracy: 0.8396\n",
            "Epoch 15: val_accuracy improved from 0.88661 to 0.89154, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8402 - val_loss: 0.3781 - val_accuracy: 0.8915\n",
            "Epoch 16/200\n",
            "179/194 [==========================>...] - ETA: 0s - loss: 0.4329 - accuracy: 0.8331\n",
            "Epoch 16: val_accuracy did not improve from 0.89154\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8344 - val_loss: 0.4618 - val_accuracy: 0.8644\n",
            "Epoch 17/200\n",
            "185/194 [===========================>..] - ETA: 0s - loss: 0.3987 - accuracy: 0.8527\n",
            "Epoch 17: val_accuracy improved from 0.89154 to 0.93837, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3964 - accuracy: 0.8544 - val_loss: 0.3363 - val_accuracy: 0.9384\n",
            "Epoch 18/200\n",
            "172/194 [=========================>....] - ETA: 0s - loss: 0.3910 - accuracy: 0.8634\n",
            "Epoch 18: val_accuracy did not improve from 0.93837\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3891 - accuracy: 0.8608 - val_loss: 0.4465 - val_accuracy: 0.8743\n",
            "Epoch 19/200\n",
            "186/194 [===========================>..] - ETA: 0s - loss: 0.3874 - accuracy: 0.8535\n",
            "Epoch 19: val_accuracy did not improve from 0.93837\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.8541 - val_loss: 0.2838 - val_accuracy: 0.8981\n",
            "Epoch 20/200\n",
            "161/194 [=======================>......] - ETA: 0s - loss: 0.3621 - accuracy: 0.8703\n",
            "Epoch 20: val_accuracy did not improve from 0.93837\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8731 - val_loss: 0.3099 - val_accuracy: 0.9039\n",
            "Epoch 21/200\n",
            "183/194 [===========================>..] - ETA: 0s - loss: 0.3501 - accuracy: 0.8689\n",
            "Epoch 21: val_accuracy improved from 0.93837 to 0.95974, saving model to weights.best.hdf5\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3497 - accuracy: 0.8699 - val_loss: 0.2484 - val_accuracy: 0.9597\n",
            "Epoch 22/200\n",
            "177/194 [==========================>...] - ETA: 0s - loss: 0.3549 - accuracy: 0.8718\n",
            "Epoch 22: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.8721 - val_loss: 0.2791 - val_accuracy: 0.9187\n",
            "Epoch 23/200\n",
            "183/194 [===========================>..] - ETA: 0s - loss: 0.3523 - accuracy: 0.8709\n",
            "Epoch 23: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3493 - accuracy: 0.8721 - val_loss: 0.2555 - val_accuracy: 0.9022\n",
            "Epoch 24/200\n",
            "181/194 [==========================>...] - ETA: 0s - loss: 0.3283 - accuracy: 0.8743\n",
            "Epoch 24: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.8747 - val_loss: 0.3223 - val_accuracy: 0.8316\n",
            "Epoch 25/200\n",
            "162/194 [========================>.....] - ETA: 0s - loss: 0.3224 - accuracy: 0.8800\n",
            "Epoch 25: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.8821 - val_loss: 0.3238 - val_accuracy: 0.8546\n",
            "Epoch 26/200\n",
            "176/194 [==========================>...] - ETA: 0s - loss: 0.3449 - accuracy: 0.8722\n",
            "Epoch 26: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8734 - val_loss: 0.3039 - val_accuracy: 0.8661\n",
            "Epoch 27/200\n",
            "180/194 [==========================>...] - ETA: 0s - loss: 0.3086 - accuracy: 0.8819\n",
            "Epoch 27: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8841 - val_loss: 0.2685 - val_accuracy: 0.8578\n",
            "Epoch 28/200\n",
            "181/194 [==========================>...] - ETA: 0s - loss: 0.3139 - accuracy: 0.8840\n",
            "Epoch 28: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8825 - val_loss: 0.2317 - val_accuracy: 0.9047\n",
            "Epoch 29/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.3255 - accuracy: 0.8812\n",
            "Epoch 29: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8828 - val_loss: 0.2410 - val_accuracy: 0.8858\n",
            "Epoch 30/200\n",
            "164/194 [========================>.....] - ETA: 0s - loss: 0.2929 - accuracy: 0.8857\n",
            "Epoch 30: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8860 - val_loss: 0.2751 - val_accuracy: 0.8710\n",
            "Epoch 31/200\n",
            "184/194 [===========================>..] - ETA: 0s - loss: 0.2908 - accuracy: 0.8937\n",
            "Epoch 31: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.8941 - val_loss: 0.1869 - val_accuracy: 0.9523\n",
            "Epoch 32/200\n",
            "179/194 [==========================>...] - ETA: 0s - loss: 0.2887 - accuracy: 0.8865\n",
            "Epoch 32: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2891 - accuracy: 0.8854 - val_loss: 0.2527 - val_accuracy: 0.8825\n",
            "Epoch 33/200\n",
            "174/194 [=========================>....] - ETA: 0s - loss: 0.2702 - accuracy: 0.8994\n",
            "Epoch 33: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.9005 - val_loss: 0.2763 - val_accuracy: 0.8570\n",
            "Epoch 34/200\n",
            "182/194 [===========================>..] - ETA: 0s - loss: 0.2874 - accuracy: 0.8905\n",
            "Epoch 34: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8899 - val_loss: 0.2038 - val_accuracy: 0.8948\n",
            "Epoch 35/200\n",
            "166/194 [========================>.....] - ETA: 0s - loss: 0.2774 - accuracy: 0.8998\n",
            "Epoch 35: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.8970 - val_loss: 0.3288 - val_accuracy: 0.8521\n",
            "Epoch 36/200\n",
            "180/194 [==========================>...] - ETA: 0s - loss: 0.2638 - accuracy: 0.9028\n",
            "Epoch 36: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2620 - accuracy: 0.9041 - val_loss: 0.2032 - val_accuracy: 0.9137\n",
            "Epoch 37/200\n",
            "183/194 [===========================>..] - ETA: 0s - loss: 0.2591 - accuracy: 0.9044\n",
            "Epoch 37: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.9057 - val_loss: 0.2606 - val_accuracy: 0.8570\n",
            "Epoch 38/200\n",
            "178/194 [==========================>...] - ETA: 0s - loss: 0.2631 - accuracy: 0.9031\n",
            "Epoch 38: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.9044 - val_loss: 0.2297 - val_accuracy: 0.9055\n",
            "Epoch 39/200\n",
            "186/194 [===========================>..] - ETA: 0s - loss: 0.2358 - accuracy: 0.9116\n",
            "Epoch 39: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2358 - accuracy: 0.9122 - val_loss: 0.2514 - val_accuracy: 0.8932\n",
            "Epoch 40/200\n",
            "153/194 [======================>.......] - ETA: 0s - loss: 0.2596 - accuracy: 0.9085\n",
            "Epoch 40: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.9064 - val_loss: 0.2742 - val_accuracy: 0.8595\n",
            "Epoch 41/200\n",
            "184/194 [===========================>..] - ETA: 0s - loss: 0.2724 - accuracy: 0.9042\n",
            "Epoch 41: val_accuracy did not improve from 0.95974\n",
            "194/194 [==============================] - 0s 1ms/step - loss: 0.2730 - accuracy: 0.9041 - val_loss: 0.2561 - val_accuracy: 0.8850\n",
            "42/42 [==============================] - 0s 788us/step - loss: 0.6422 - accuracy: 0.8675\n",
            "[0.9041007161140442, 0.8849630355834961, 0.867514967918396]\n",
            "NH\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 2.0524 - accuracy: 0.1737\n",
            "Epoch 1: val_accuracy improved from -inf to 0.57113, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 1s 2ms/step - loss: 1.9584 - accuracy: 0.2144 - val_loss: 1.6915 - val_accuracy: 0.5711\n",
            "Epoch 2/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 1.4356 - accuracy: 0.4445\n",
            "Epoch 2: val_accuracy did not improve from 0.57113\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 1.4359 - accuracy: 0.4448 - val_loss: 1.2712 - val_accuracy: 0.5573\n",
            "Epoch 3/200\n",
            "169/180 [===========================>..] - ETA: 0s - loss: 1.1446 - accuracy: 0.5518\n",
            "Epoch 3: val_accuracy improved from 0.57113 to 0.62804, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 1.1369 - accuracy: 0.5552 - val_loss: 1.1048 - val_accuracy: 0.6280\n",
            "Epoch 4/200\n",
            "175/180 [============================>.] - ETA: 0s - loss: 0.9392 - accuracy: 0.6379\n",
            "Epoch 4: val_accuracy improved from 0.62804 to 0.63914, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.9358 - accuracy: 0.6391 - val_loss: 1.0932 - val_accuracy: 0.6391\n",
            "Epoch 5/200\n",
            "177/180 [============================>.] - ETA: 0s - loss: 0.7595 - accuracy: 0.7327\n",
            "Epoch 5: val_accuracy did not improve from 0.63914\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.7615 - accuracy: 0.7330 - val_loss: 1.0859 - val_accuracy: 0.6384\n",
            "Epoch 6/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.6648 - accuracy: 0.7623\n",
            "Epoch 6: val_accuracy improved from 0.63914 to 0.65441, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.6550 - accuracy: 0.7689 - val_loss: 1.0342 - val_accuracy: 0.6544\n",
            "Epoch 7/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.8104\n",
            "Epoch 7: val_accuracy improved from 0.65441 to 0.65718, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.5651 - accuracy: 0.8096 - val_loss: 0.9857 - val_accuracy: 0.6572\n",
            "Epoch 8/200\n",
            "173/180 [===========================>..] - ETA: 0s - loss: 0.4796 - accuracy: 0.8363\n",
            "Epoch 8: val_accuracy did not improve from 0.65718\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.8364 - val_loss: 1.0288 - val_accuracy: 0.6364\n",
            "Epoch 9/200\n",
            "167/180 [==========================>...] - ETA: 0s - loss: 0.4389 - accuracy: 0.8514\n",
            "Epoch 9: val_accuracy improved from 0.65718 to 0.66273, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.4428 - accuracy: 0.8496 - val_loss: 0.9643 - val_accuracy: 0.6627\n",
            "Epoch 10/200\n",
            "174/180 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8667\n",
            "Epoch 10: val_accuracy improved from 0.66273 to 0.68286, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8663 - val_loss: 0.9319 - val_accuracy: 0.6829\n",
            "Epoch 11/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8739\n",
            "Epoch 11: val_accuracy did not improve from 0.68286\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8740 - val_loss: 1.0966 - val_accuracy: 0.6600\n",
            "Epoch 12/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.3409 - accuracy: 0.8868\n",
            "Epoch 12: val_accuracy did not improve from 0.68286\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.3498 - accuracy: 0.8768 - val_loss: 1.1293 - val_accuracy: 0.6426\n",
            "Epoch 13/200\n",
            "176/180 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.9073\n",
            "Epoch 13: val_accuracy did not improve from 0.68286\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.9078 - val_loss: 1.1212 - val_accuracy: 0.6579\n",
            "Epoch 14/200\n",
            "171/180 [===========================>..] - ETA: 0s - loss: 0.2780 - accuracy: 0.9108\n",
            "Epoch 14: val_accuracy did not improve from 0.68286\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2878 - accuracy: 0.9085 - val_loss: 1.1824 - val_accuracy: 0.6495\n",
            "Epoch 15/200\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9022\n",
            "Epoch 15: val_accuracy did not improve from 0.68286\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.9022 - val_loss: 1.1052 - val_accuracy: 0.6808\n",
            "Epoch 16/200\n",
            "175/180 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9154\n",
            "Epoch 16: val_accuracy did not improve from 0.68286\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.9161 - val_loss: 1.2485 - val_accuracy: 0.6419\n",
            "Epoch 17/200\n",
            "123/180 [===================>..........] - ETA: 0s - loss: 0.2399 - accuracy: 0.9273\n",
            "Epoch 17: val_accuracy did not improve from 0.68286\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9276 - val_loss: 1.2993 - val_accuracy: 0.6579\n",
            "Epoch 18/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9279\n",
            "Epoch 18: val_accuracy improved from 0.68286 to 0.68980, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9279 - val_loss: 1.3155 - val_accuracy: 0.6898\n",
            "Epoch 19/200\n",
            "171/180 [===========================>..] - ETA: 0s - loss: 0.2276 - accuracy: 0.9309\n",
            "Epoch 19: val_accuracy improved from 0.68980 to 0.69049, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9321 - val_loss: 1.3624 - val_accuracy: 0.6905\n",
            "Epoch 20/200\n",
            "169/180 [===========================>..] - ETA: 0s - loss: 0.2090 - accuracy: 0.9294\n",
            "Epoch 20: val_accuracy did not improve from 0.69049\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9297 - val_loss: 1.4598 - val_accuracy: 0.6482\n",
            "Epoch 21/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.9300\n",
            "Epoch 21: val_accuracy did not improve from 0.69049\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9300 - val_loss: 1.5349 - val_accuracy: 0.6454\n",
            "Epoch 22/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.1874 - accuracy: 0.9365\n",
            "Epoch 22: val_accuracy improved from 0.69049 to 0.69188, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1910 - accuracy: 0.9412 - val_loss: 1.4020 - val_accuracy: 0.6919\n",
            "Epoch 23/200\n",
            "166/180 [==========================>...] - ETA: 0s - loss: 0.1934 - accuracy: 0.9409\n",
            "Epoch 23: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9398 - val_loss: 1.5600 - val_accuracy: 0.6662\n",
            "Epoch 24/200\n",
            "125/180 [===================>..........] - ETA: 0s - loss: 0.1917 - accuracy: 0.9370\n",
            "Epoch 24: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9342 - val_loss: 1.3525 - val_accuracy: 0.6426\n",
            "Epoch 25/200\n",
            "124/180 [===================>..........] - ETA: 0s - loss: 0.1677 - accuracy: 0.9501\n",
            "Epoch 25: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9460 - val_loss: 1.5601 - val_accuracy: 0.6905\n",
            "Epoch 26/200\n",
            "174/180 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9479\n",
            "Epoch 26: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9478 - val_loss: 1.5285 - val_accuracy: 0.6530\n",
            "Epoch 27/200\n",
            "120/180 [===================>..........] - ETA: 0s - loss: 0.1661 - accuracy: 0.9422\n",
            "Epoch 27: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9474 - val_loss: 1.6807 - val_accuracy: 0.6419\n",
            "Epoch 28/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9523\n",
            "Epoch 28: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9523 - val_loss: 1.6230 - val_accuracy: 0.6593\n",
            "Epoch 29/200\n",
            "165/180 [==========================>...] - ETA: 0s - loss: 0.1606 - accuracy: 0.9420\n",
            "Epoch 29: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9429 - val_loss: 1.7171 - val_accuracy: 0.6544\n",
            "Epoch 30/200\n",
            "121/180 [===================>..........] - ETA: 0s - loss: 0.1355 - accuracy: 0.9545\n",
            "Epoch 30: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9575 - val_loss: 1.8287 - val_accuracy: 0.6718\n",
            "Epoch 31/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.1433 - accuracy: 0.9559\n",
            "Epoch 31: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9582 - val_loss: 1.9287 - val_accuracy: 0.6676\n",
            "Epoch 32/200\n",
            "124/180 [===================>..........] - ETA: 0s - loss: 0.1417 - accuracy: 0.9511\n",
            "Epoch 32: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9513 - val_loss: 1.8591 - val_accuracy: 0.6731\n",
            "Epoch 33/200\n",
            "135/180 [=====================>........] - ETA: 0s - loss: 0.1523 - accuracy: 0.9528\n",
            "Epoch 33: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9499 - val_loss: 1.8787 - val_accuracy: 0.6128\n",
            "Epoch 34/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.1532 - accuracy: 0.9457\n",
            "Epoch 34: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9513 - val_loss: 1.9188 - val_accuracy: 0.6072\n",
            "Epoch 35/200\n",
            "121/180 [===================>..........] - ETA: 0s - loss: 0.1388 - accuracy: 0.9566\n",
            "Epoch 35: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9561 - val_loss: 1.9150 - val_accuracy: 0.6461\n",
            "Epoch 36/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.1328 - accuracy: 0.9606\n",
            "Epoch 36: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9579 - val_loss: 1.8919 - val_accuracy: 0.5378\n",
            "Epoch 37/200\n",
            "175/180 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9557\n",
            "Epoch 37: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9561 - val_loss: 1.7842 - val_accuracy: 0.6919\n",
            "Epoch 38/200\n",
            "171/180 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9591\n",
            "Epoch 38: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9586 - val_loss: 2.0467 - val_accuracy: 0.6662\n",
            "Epoch 39/200\n",
            "123/180 [===================>..........] - ETA: 0s - loss: 0.1211 - accuracy: 0.9644\n",
            "Epoch 39: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9596 - val_loss: 1.8971 - val_accuracy: 0.5663\n",
            "Epoch 40/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.1324 - accuracy: 0.9616\n",
            "Epoch 40: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9652 - val_loss: 1.9836 - val_accuracy: 0.6634\n",
            "Epoch 41/200\n",
            "121/180 [===================>..........] - ETA: 0s - loss: 0.1278 - accuracy: 0.9607\n",
            "Epoch 41: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9610 - val_loss: 2.1952 - val_accuracy: 0.5455\n",
            "Epoch 42/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.1103 - accuracy: 0.9631\n",
            "Epoch 42: val_accuracy did not improve from 0.69188\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9603 - val_loss: 2.0403 - val_accuracy: 0.6655\n",
            "42/42 [==============================] - 0s 809us/step - loss: 1.4844 - accuracy: 0.6175\n",
            "[0.960320234298706, 0.6655100584030151, 0.617514967918396]\n",
            "K\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "120/160 [=====================>........] - ETA: 0s - loss: 1.8989 - accuracy: 0.2292\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51027, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 1s 2ms/step - loss: 1.8392 - accuracy: 0.2523 - val_loss: 1.4421 - val_accuracy: 0.5103\n",
            "Epoch 2/200\n",
            "105/160 [==================>...........] - ETA: 0s - loss: 1.4834 - accuracy: 0.4202\n",
            "Epoch 2: val_accuracy improved from 0.51027 to 0.71014, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4317 - accuracy: 0.4293 - val_loss: 1.0716 - val_accuracy: 0.7101\n",
            "Epoch 3/200\n",
            "137/160 [========================>.....] - ETA: 0s - loss: 1.2033 - accuracy: 0.5196\n",
            "Epoch 3: val_accuracy improved from 0.71014 to 0.77717, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.1944 - accuracy: 0.5206 - val_loss: 0.8895 - val_accuracy: 0.7772\n",
            "Epoch 4/200\n",
            "116/160 [====================>.........] - ETA: 0s - loss: 1.0753 - accuracy: 0.5663\n",
            "Epoch 4: val_accuracy improved from 0.77717 to 0.86171, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 1.0555 - accuracy: 0.5840 - val_loss: 0.7241 - val_accuracy: 0.8617\n",
            "Epoch 5/200\n",
            "155/160 [============================>.] - ETA: 0s - loss: 0.9207 - accuracy: 0.6448\n",
            "Epoch 5: val_accuracy improved from 0.86171 to 0.92693, saving model to weights.best.hdf5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.9193 - accuracy: 0.6455 - val_loss: 0.5900 - val_accuracy: 0.9269\n",
            "Epoch 6/200\n",
            "114/160 [====================>.........] - ETA: 0s - loss: 0.8304 - accuracy: 0.6848\n",
            "Epoch 6: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.8273 - accuracy: 0.6866 - val_loss: 0.5130 - val_accuracy: 0.9118\n",
            "Epoch 7/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.7485 - accuracy: 0.7367\n",
            "Epoch 7: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.7256 - accuracy: 0.7450 - val_loss: 0.4718 - val_accuracy: 0.8750\n",
            "Epoch 8/200\n",
            "145/160 [==========================>...] - ETA: 0s - loss: 0.6794 - accuracy: 0.7578\n",
            "Epoch 8: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.6782 - accuracy: 0.7587 - val_loss: 0.4197 - val_accuracy: 0.9052\n",
            "Epoch 9/200\n",
            "124/160 [======================>.......] - ETA: 0s - loss: 0.5767 - accuracy: 0.7903\n",
            "Epoch 9: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5843 - accuracy: 0.7881 - val_loss: 0.3589 - val_accuracy: 0.9245\n",
            "Epoch 10/200\n",
            "115/160 [====================>.........] - ETA: 0s - loss: 0.5487 - accuracy: 0.8087\n",
            "Epoch 10: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5539 - accuracy: 0.8104 - val_loss: 0.3670 - val_accuracy: 0.8750\n",
            "Epoch 11/200\n",
            "121/160 [=====================>........] - ETA: 0s - loss: 0.5149 - accuracy: 0.8311\n",
            "Epoch 11: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5037 - accuracy: 0.8347 - val_loss: 0.3178 - val_accuracy: 0.8955\n",
            "Epoch 12/200\n",
            "118/160 [=====================>........] - ETA: 0s - loss: 0.4918 - accuracy: 0.8326\n",
            "Epoch 12: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4864 - accuracy: 0.8320 - val_loss: 0.3147 - val_accuracy: 0.8877\n",
            "Epoch 13/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.4514 - accuracy: 0.8430\n",
            "Epoch 13: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.8433 - val_loss: 0.3284 - val_accuracy: 0.8816\n",
            "Epoch 14/200\n",
            "127/160 [======================>.......] - ETA: 0s - loss: 0.4520 - accuracy: 0.8450\n",
            "Epoch 14: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4428 - accuracy: 0.8500 - val_loss: 0.3215 - val_accuracy: 0.8714\n",
            "Epoch 15/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.3952 - accuracy: 0.8785\n",
            "Epoch 15: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3972 - accuracy: 0.8743 - val_loss: 0.3213 - val_accuracy: 0.8822\n",
            "Epoch 16/200\n",
            "127/160 [======================>.......] - ETA: 0s - loss: 0.4139 - accuracy: 0.8656\n",
            "Epoch 16: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4101 - accuracy: 0.8660 - val_loss: 0.3120 - val_accuracy: 0.8979\n",
            "Epoch 17/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.4241 - accuracy: 0.8585\n",
            "Epoch 17: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4087 - accuracy: 0.8621 - val_loss: 0.3053 - val_accuracy: 0.8998\n",
            "Epoch 18/200\n",
            "118/160 [=====================>........] - ETA: 0s - loss: 0.3587 - accuracy: 0.8867\n",
            "Epoch 18: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8817 - val_loss: 0.3632 - val_accuracy: 0.8593\n",
            "Epoch 19/200\n",
            "110/160 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8864\n",
            "Epoch 19: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3597 - accuracy: 0.8809 - val_loss: 0.2970 - val_accuracy: 0.9130\n",
            "Epoch 20/200\n",
            "126/160 [======================>.......] - ETA: 0s - loss: 0.3568 - accuracy: 0.8780\n",
            "Epoch 20: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3668 - accuracy: 0.8750 - val_loss: 0.3291 - val_accuracy: 0.8901\n",
            "Epoch 21/200\n",
            "119/160 [=====================>........] - ETA: 0s - loss: 0.3283 - accuracy: 0.8944\n",
            "Epoch 21: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8907 - val_loss: 0.3619 - val_accuracy: 0.8641\n",
            "Epoch 22/200\n",
            "125/160 [======================>.......] - ETA: 0s - loss: 0.3410 - accuracy: 0.8790\n",
            "Epoch 22: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8841 - val_loss: 0.3377 - val_accuracy: 0.8798\n",
            "Epoch 23/200\n",
            "129/160 [=======================>......] - ETA: 0s - loss: 0.3172 - accuracy: 0.9002\n",
            "Epoch 23: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.8986 - val_loss: 0.4131 - val_accuracy: 0.8146\n",
            "Epoch 24/200\n",
            "122/160 [=====================>........] - ETA: 0s - loss: 0.3025 - accuracy: 0.9057\n",
            "Epoch 24: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.9013 - val_loss: 0.4156 - val_accuracy: 0.8188\n",
            "Epoch 25/200\n",
            "101/160 [=================>............] - ETA: 0s - loss: 0.2722 - accuracy: 0.9189\n",
            "Epoch 25: val_accuracy did not improve from 0.92693\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.9166 - val_loss: 0.3829 - val_accuracy: 0.8478\n",
            "46/46 [==============================] - 0s 655us/step - loss: 0.8200 - accuracy: 0.6960\n",
            "[0.9165687561035156, 0.8478260636329651, 0.696044385433197]\n",
            "K\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "179/187 [===========================>..] - ETA: 0s - loss: 1.8702 - accuracy: 0.2692\n",
            "Epoch 1: val_accuracy improved from -inf to 0.52835, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 1s 2ms/step - loss: 1.8567 - accuracy: 0.2751 - val_loss: 1.5080 - val_accuracy: 0.5283\n",
            "Epoch 2/200\n",
            "146/187 [======================>.......] - ETA: 0s - loss: 1.3507 - accuracy: 0.4837\n",
            "Epoch 2: val_accuracy improved from 0.52835 to 0.62120, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 1.3144 - accuracy: 0.4977 - val_loss: 1.2879 - val_accuracy: 0.6212\n",
            "Epoch 3/200\n",
            "180/187 [===========================>..] - ETA: 0s - loss: 1.0372 - accuracy: 0.6038\n",
            "Epoch 3: val_accuracy improved from 0.62120 to 0.71076, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 1.0311 - accuracy: 0.6080 - val_loss: 1.2200 - val_accuracy: 0.7108\n",
            "Epoch 4/200\n",
            "129/187 [===================>..........] - ETA: 0s - loss: 0.8583 - accuracy: 0.6797\n",
            "Epoch 4: val_accuracy did not improve from 0.71076\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.8369 - accuracy: 0.6922 - val_loss: 1.1768 - val_accuracy: 0.6713\n",
            "Epoch 5/200\n",
            "128/187 [===================>..........] - ETA: 0s - loss: 0.7366 - accuracy: 0.7451\n",
            "Epoch 5: val_accuracy did not improve from 0.71076\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.7326 - accuracy: 0.7460 - val_loss: 1.1268 - val_accuracy: 0.6261\n",
            "Epoch 6/200\n",
            "132/187 [====================>.........] - ETA: 0s - loss: 0.6508 - accuracy: 0.7779\n",
            "Epoch 6: val_accuracy did not improve from 0.71076\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.6330 - accuracy: 0.7821 - val_loss: 1.1625 - val_accuracy: 0.6919\n",
            "Epoch 7/200\n",
            "179/187 [===========================>..] - ETA: 0s - loss: 0.5650 - accuracy: 0.8125\n",
            "Epoch 7: val_accuracy improved from 0.71076 to 0.71898, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.8145 - val_loss: 1.1258 - val_accuracy: 0.7190\n",
            "Epoch 8/200\n",
            "122/187 [==================>...........] - ETA: 0s - loss: 0.5289 - accuracy: 0.8248\n",
            "Epoch 8: val_accuracy did not improve from 0.71898\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.8332 - val_loss: 1.1503 - val_accuracy: 0.7190\n",
            "Epoch 9/200\n",
            "133/187 [====================>.........] - ETA: 0s - loss: 0.4661 - accuracy: 0.8567\n",
            "Epoch 9: val_accuracy improved from 0.71898 to 0.71980, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.4507 - accuracy: 0.8583 - val_loss: 1.0840 - val_accuracy: 0.7198\n",
            "Epoch 10/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8777\n",
            "Epoch 10: val_accuracy did not improve from 0.71980\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8777 - val_loss: 1.1936 - val_accuracy: 0.7173\n",
            "Epoch 11/200\n",
            "127/187 [===================>..........] - ETA: 0s - loss: 0.3896 - accuracy: 0.8780\n",
            "Epoch 11: val_accuracy improved from 0.71980 to 0.74856, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3789 - accuracy: 0.8783 - val_loss: 1.0553 - val_accuracy: 0.7486\n",
            "Epoch 12/200\n",
            "141/187 [=====================>........] - ETA: 0s - loss: 0.3822 - accuracy: 0.8790\n",
            "Epoch 12: val_accuracy did not improve from 0.74856\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8820 - val_loss: 0.9818 - val_accuracy: 0.7329\n",
            "Epoch 13/200\n",
            "132/187 [====================>.........] - ETA: 0s - loss: 0.3352 - accuracy: 0.8949\n",
            "Epoch 13: val_accuracy did not improve from 0.74856\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.9017 - val_loss: 1.0717 - val_accuracy: 0.7198\n",
            "Epoch 14/200\n",
            "132/187 [====================>.........] - ETA: 0s - loss: 0.3174 - accuracy: 0.9020\n",
            "Epoch 14: val_accuracy improved from 0.74856 to 0.76335, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3042 - accuracy: 0.9071 - val_loss: 1.0646 - val_accuracy: 0.7634\n",
            "Epoch 15/200\n",
            "127/187 [===================>..........] - ETA: 0s - loss: 0.3175 - accuracy: 0.9094\n",
            "Epoch 15: val_accuracy did not improve from 0.76335\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.3058 - accuracy: 0.9037 - val_loss: 1.2673 - val_accuracy: 0.7601\n",
            "Epoch 16/200\n",
            "129/187 [===================>..........] - ETA: 0s - loss: 0.2967 - accuracy: 0.9026\n",
            "Epoch 16: val_accuracy improved from 0.76335 to 0.78718, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2958 - accuracy: 0.9051 - val_loss: 0.8073 - val_accuracy: 0.7872\n",
            "Epoch 17/200\n",
            "171/187 [==========================>...] - ETA: 0s - loss: 0.2672 - accuracy: 0.9225\n",
            "Epoch 17: val_accuracy did not improve from 0.78718\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.9225 - val_loss: 1.1415 - val_accuracy: 0.7313\n",
            "Epoch 18/200\n",
            "135/187 [====================>.........] - ETA: 0s - loss: 0.2673 - accuracy: 0.9144\n",
            "Epoch 18: val_accuracy did not improve from 0.78718\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.9144 - val_loss: 1.2567 - val_accuracy: 0.7543\n",
            "Epoch 19/200\n",
            "132/187 [====================>.........] - ETA: 0s - loss: 0.2398 - accuracy: 0.9276\n",
            "Epoch 19: val_accuracy did not improve from 0.78718\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9285 - val_loss: 1.0275 - val_accuracy: 0.7642\n",
            "Epoch 20/200\n",
            "127/187 [===================>..........] - ETA: 0s - loss: 0.2287 - accuracy: 0.9341\n",
            "Epoch 20: val_accuracy improved from 0.78718 to 0.82909, saving model to weights.best.hdf5\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.9311 - val_loss: 0.6994 - val_accuracy: 0.8291\n",
            "Epoch 21/200\n",
            "144/187 [======================>.......] - ETA: 0s - loss: 0.2487 - accuracy: 0.9232\n",
            "Epoch 21: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.9261 - val_loss: 1.1131 - val_accuracy: 0.7634\n",
            "Epoch 22/200\n",
            "134/187 [====================>.........] - ETA: 0s - loss: 0.2168 - accuracy: 0.9417\n",
            "Epoch 22: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9385 - val_loss: 1.1027 - val_accuracy: 0.7765\n",
            "Epoch 23/200\n",
            "129/187 [===================>..........] - ETA: 0s - loss: 0.1985 - accuracy: 0.9462\n",
            "Epoch 23: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9439 - val_loss: 1.1244 - val_accuracy: 0.7666\n",
            "Epoch 24/200\n",
            "128/187 [===================>..........] - ETA: 0s - loss: 0.1996 - accuracy: 0.9341\n",
            "Epoch 24: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9385 - val_loss: 0.9964 - val_accuracy: 0.7798\n",
            "Epoch 25/200\n",
            "131/187 [====================>.........] - ETA: 0s - loss: 0.1947 - accuracy: 0.9394\n",
            "Epoch 25: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9402 - val_loss: 1.1356 - val_accuracy: 0.7625\n",
            "Epoch 26/200\n",
            "160/187 [========================>.....] - ETA: 0s - loss: 0.1912 - accuracy: 0.9461\n",
            "Epoch 26: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9455 - val_loss: 1.3179 - val_accuracy: 0.7486\n",
            "Epoch 27/200\n",
            "132/187 [====================>.........] - ETA: 0s - loss: 0.1926 - accuracy: 0.9437\n",
            "Epoch 27: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9415 - val_loss: 1.3738 - val_accuracy: 0.7683\n",
            "Epoch 28/200\n",
            "134/187 [====================>.........] - ETA: 0s - loss: 0.1808 - accuracy: 0.9426\n",
            "Epoch 28: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9435 - val_loss: 1.4939 - val_accuracy: 0.7453\n",
            "Epoch 29/200\n",
            "136/187 [====================>.........] - ETA: 0s - loss: 0.1925 - accuracy: 0.9389\n",
            "Epoch 29: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9385 - val_loss: 1.1729 - val_accuracy: 0.7518\n",
            "Epoch 30/200\n",
            "131/187 [====================>.........] - ETA: 0s - loss: 0.1878 - accuracy: 0.9461\n",
            "Epoch 30: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9449 - val_loss: 1.3295 - val_accuracy: 0.7288\n",
            "Epoch 31/200\n",
            "131/187 [====================>.........] - ETA: 0s - loss: 0.1667 - accuracy: 0.9499\n",
            "Epoch 31: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9482 - val_loss: 1.3622 - val_accuracy: 0.7568\n",
            "Epoch 32/200\n",
            "134/187 [====================>.........] - ETA: 0s - loss: 0.1623 - accuracy: 0.9454\n",
            "Epoch 32: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9505 - val_loss: 1.4416 - val_accuracy: 0.7477\n",
            "Epoch 33/200\n",
            "186/187 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9462\n",
            "Epoch 33: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9462 - val_loss: 0.9641 - val_accuracy: 0.7913\n",
            "Epoch 34/200\n",
            "128/187 [===================>..........] - ETA: 0s - loss: 0.1743 - accuracy: 0.9531\n",
            "Epoch 34: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9525 - val_loss: 1.2966 - val_accuracy: 0.7584\n",
            "Epoch 35/200\n",
            "135/187 [====================>.........] - ETA: 0s - loss: 0.1681 - accuracy: 0.9472\n",
            "Epoch 35: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.9512 - val_loss: 1.2613 - val_accuracy: 0.7691\n",
            "Epoch 36/200\n",
            "134/187 [====================>.........] - ETA: 0s - loss: 0.1597 - accuracy: 0.9524\n",
            "Epoch 36: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9549 - val_loss: 1.6009 - val_accuracy: 0.7494\n",
            "Epoch 37/200\n",
            "181/187 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9534\n",
            "Epoch 37: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9532 - val_loss: 1.5573 - val_accuracy: 0.7157\n",
            "Epoch 38/200\n",
            "133/187 [====================>.........] - ETA: 0s - loss: 0.1599 - accuracy: 0.9530\n",
            "Epoch 38: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9522 - val_loss: 1.5628 - val_accuracy: 0.7601\n",
            "Epoch 39/200\n",
            "131/187 [====================>.........] - ETA: 0s - loss: 0.1427 - accuracy: 0.9594\n",
            "Epoch 39: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9566 - val_loss: 1.4800 - val_accuracy: 0.7617\n",
            "Epoch 40/200\n",
            "132/187 [====================>.........] - ETA: 0s - loss: 0.1367 - accuracy: 0.9564\n",
            "Epoch 40: val_accuracy did not improve from 0.82909\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9555 - val_loss: 1.3787 - val_accuracy: 0.7675\n",
            "46/46 [==============================] - 0s 721us/step - loss: 1.1319 - accuracy: 0.5968\n",
            "[0.955548107624054, 0.7674609422683716, 0.5968077778816223]\n",
            "NH\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 64)                2176      \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,487\n",
            "Trainable params: 4,487\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 1.7310 - accuracy: 0.3104\n",
            "Epoch 1: val_accuracy improved from -inf to 0.45210, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 1s 2ms/step - loss: 1.7295 - accuracy: 0.3108 - val_loss: 1.4644 - val_accuracy: 0.4521\n",
            "Epoch 2/200\n",
            "172/180 [===========================>..] - ETA: 0s - loss: 1.3029 - accuracy: 0.4604\n",
            "Epoch 2: val_accuracy improved from 0.45210 to 0.69835, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 1.2989 - accuracy: 0.4612 - val_loss: 1.2109 - val_accuracy: 0.6984\n",
            "Epoch 3/200\n",
            "169/180 [===========================>..] - ETA: 0s - loss: 1.0554 - accuracy: 0.5780\n",
            "Epoch 3: val_accuracy improved from 0.69835 to 0.71332, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 1.0484 - accuracy: 0.5799 - val_loss: 1.0413 - val_accuracy: 0.7133\n",
            "Epoch 4/200\n",
            "133/180 [=====================>........] - ETA: 0s - loss: 0.8996 - accuracy: 0.6424\n",
            "Epoch 4: val_accuracy improved from 0.71332 to 0.72081, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 2ms/step - loss: 0.8929 - accuracy: 0.6478 - val_loss: 0.9648 - val_accuracy: 0.7208\n",
            "Epoch 5/200\n",
            "170/180 [===========================>..] - ETA: 0s - loss: 0.7588 - accuracy: 0.7202\n",
            "Epoch 5: val_accuracy improved from 0.72081 to 0.72605, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.7628 - accuracy: 0.7174 - val_loss: 1.0180 - val_accuracy: 0.7260\n",
            "Epoch 6/200\n",
            "167/180 [==========================>...] - ETA: 0s - loss: 0.6399 - accuracy: 0.7736\n",
            "Epoch 6: val_accuracy improved from 0.72605 to 0.72680, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.6293 - accuracy: 0.7772 - val_loss: 1.0216 - val_accuracy: 0.7268\n",
            "Epoch 7/200\n",
            "168/180 [===========================>..] - ETA: 0s - loss: 0.5433 - accuracy: 0.8103\n",
            "Epoch 7: val_accuracy improved from 0.72680 to 0.74701, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.8107 - val_loss: 0.9527 - val_accuracy: 0.7470\n",
            "Epoch 8/200\n",
            "142/180 [======================>.......] - ETA: 0s - loss: 0.4963 - accuracy: 0.8341\n",
            "Epoch 8: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.8357 - val_loss: 1.0082 - val_accuracy: 0.6811\n",
            "Epoch 9/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8617\n",
            "Epoch 9: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8615 - val_loss: 1.0450 - val_accuracy: 0.6564\n",
            "Epoch 10/200\n",
            "123/180 [===================>..........] - ETA: 0s - loss: 0.4115 - accuracy: 0.8679\n",
            "Epoch 10: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.4040 - accuracy: 0.8646 - val_loss: 1.0329 - val_accuracy: 0.7403\n",
            "Epoch 11/200\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.8932\n",
            "Epoch 11: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8935 - val_loss: 1.0975 - val_accuracy: 0.7111\n",
            "Epoch 12/200\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3285 - accuracy: 0.8977\n",
            "Epoch 12: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.3286 - accuracy: 0.8973 - val_loss: 1.1199 - val_accuracy: 0.6168\n",
            "Epoch 13/200\n",
            "167/180 [==========================>...] - ETA: 0s - loss: 0.3047 - accuracy: 0.9068\n",
            "Epoch 13: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.3038 - accuracy: 0.9060 - val_loss: 1.2169 - val_accuracy: 0.6205\n",
            "Epoch 14/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8992\n",
            "Epoch 14: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.8991 - val_loss: 1.2366 - val_accuracy: 0.6332\n",
            "Epoch 15/200\n",
            "124/180 [===================>..........] - ETA: 0s - loss: 0.2700 - accuracy: 0.9128\n",
            "Epoch 15: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2694 - accuracy: 0.9105 - val_loss: 1.2042 - val_accuracy: 0.6407\n",
            "Epoch 16/200\n",
            "166/180 [==========================>...] - ETA: 0s - loss: 0.2662 - accuracy: 0.9127\n",
            "Epoch 16: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2668 - accuracy: 0.9109 - val_loss: 1.2829 - val_accuracy: 0.6325\n",
            "Epoch 17/200\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9337\n",
            "Epoch 17: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9335 - val_loss: 1.2552 - val_accuracy: 0.6362\n",
            "Epoch 18/200\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2508 - accuracy: 0.9186\n",
            "Epoch 18: val_accuracy did not improve from 0.74701\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9186 - val_loss: 1.3704 - val_accuracy: 0.7238\n",
            "Epoch 19/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.2254 - accuracy: 0.9232\n",
            "Epoch 19: val_accuracy improved from 0.74701 to 0.76123, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9283 - val_loss: 1.3833 - val_accuracy: 0.7612\n",
            "Epoch 20/200\n",
            "143/180 [======================>.......] - ETA: 0s - loss: 0.2230 - accuracy: 0.9331\n",
            "Epoch 20: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9318 - val_loss: 1.3554 - val_accuracy: 0.6624\n",
            "Epoch 21/200\n",
            "171/180 [===========================>..] - ETA: 0s - loss: 0.2107 - accuracy: 0.9357\n",
            "Epoch 21: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9363 - val_loss: 1.5328 - val_accuracy: 0.6123\n",
            "Epoch 22/200\n",
            "121/180 [===================>..........] - ETA: 0s - loss: 0.2162 - accuracy: 0.9267\n",
            "Epoch 22: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9293 - val_loss: 1.3386 - val_accuracy: 0.6490\n",
            "Epoch 23/200\n",
            "123/180 [===================>..........] - ETA: 0s - loss: 0.2036 - accuracy: 0.9375\n",
            "Epoch 23: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9377 - val_loss: 1.2739 - val_accuracy: 0.7223\n",
            "Epoch 24/200\n",
            "174/180 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.9511\n",
            "Epoch 24: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9516 - val_loss: 1.3532 - val_accuracy: 0.6407\n",
            "Epoch 25/200\n",
            "120/180 [===================>..........] - ETA: 0s - loss: 0.1718 - accuracy: 0.9484\n",
            "Epoch 25: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9488 - val_loss: 1.6293 - val_accuracy: 0.6302\n",
            "Epoch 26/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.1807 - accuracy: 0.9428\n",
            "Epoch 26: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9433 - val_loss: 1.6165 - val_accuracy: 0.6355\n",
            "Epoch 27/200\n",
            "121/180 [===================>..........] - ETA: 0s - loss: 0.1766 - accuracy: 0.9452\n",
            "Epoch 27: val_accuracy did not improve from 0.76123\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9450 - val_loss: 1.5050 - val_accuracy: 0.6886\n",
            "Epoch 28/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9422\n",
            "Epoch 28: val_accuracy improved from 0.76123 to 0.77769, saving model to weights.best.hdf5\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9422 - val_loss: 1.3047 - val_accuracy: 0.7777\n",
            "Epoch 29/200\n",
            "145/180 [=======================>......] - ETA: 0s - loss: 0.1701 - accuracy: 0.9530\n",
            "Epoch 29: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9516 - val_loss: 1.6808 - val_accuracy: 0.6400\n",
            "Epoch 30/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9449\n",
            "Epoch 30: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9450 - val_loss: 1.6853 - val_accuracy: 0.6437\n",
            "Epoch 31/200\n",
            "125/180 [===================>..........] - ETA: 0s - loss: 0.1570 - accuracy: 0.9540\n",
            "Epoch 31: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9558 - val_loss: 1.6996 - val_accuracy: 0.6168\n",
            "Epoch 32/200\n",
            "119/180 [==================>...........] - ETA: 0s - loss: 0.1477 - accuracy: 0.9522\n",
            "Epoch 32: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9537 - val_loss: 1.9537 - val_accuracy: 0.6272\n",
            "Epoch 33/200\n",
            "122/180 [===================>..........] - ETA: 0s - loss: 0.1494 - accuracy: 0.9611\n",
            "Epoch 33: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9582 - val_loss: 1.8805 - val_accuracy: 0.6100\n",
            "Epoch 34/200\n",
            "177/180 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9552\n",
            "Epoch 34: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9551 - val_loss: 1.8957 - val_accuracy: 0.6235\n",
            "Epoch 35/200\n",
            "168/180 [===========================>..] - ETA: 0s - loss: 0.1323 - accuracy: 0.9565\n",
            "Epoch 35: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9558 - val_loss: 1.5041 - val_accuracy: 0.7328\n",
            "Epoch 36/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9603\n",
            "Epoch 36: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9603 - val_loss: 1.7898 - val_accuracy: 0.6235\n",
            "Epoch 37/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9589\n",
            "Epoch 37: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9589 - val_loss: 1.9107 - val_accuracy: 0.6220\n",
            "Epoch 38/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9537\n",
            "Epoch 38: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9541 - val_loss: 1.7740 - val_accuracy: 0.6347\n",
            "Epoch 39/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9603\n",
            "Epoch 39: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9603 - val_loss: 1.9509 - val_accuracy: 0.6205\n",
            "Epoch 40/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9635\n",
            "Epoch 40: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9635 - val_loss: 2.1100 - val_accuracy: 0.6153\n",
            "Epoch 41/200\n",
            "180/180 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9610\n",
            "Epoch 41: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9610 - val_loss: 1.7920 - val_accuracy: 0.6385\n",
            "Epoch 42/200\n",
            "166/180 [==========================>...] - ETA: 0s - loss: 0.1185 - accuracy: 0.9605\n",
            "Epoch 42: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9603 - val_loss: 1.8574 - val_accuracy: 0.6347\n",
            "Epoch 43/200\n",
            "176/180 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9592\n",
            "Epoch 43: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9593 - val_loss: 1.9482 - val_accuracy: 0.6257\n",
            "Epoch 44/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9561\n",
            "Epoch 44: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9565 - val_loss: 1.8890 - val_accuracy: 0.6662\n",
            "Epoch 45/200\n",
            "178/180 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9631\n",
            "Epoch 45: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9628 - val_loss: 2.1270 - val_accuracy: 0.6280\n",
            "Epoch 46/200\n",
            "159/180 [=========================>....] - ETA: 0s - loss: 0.1279 - accuracy: 0.9548\n",
            "Epoch 46: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9565 - val_loss: 2.2870 - val_accuracy: 0.6310\n",
            "Epoch 47/200\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9651\n",
            "Epoch 47: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9652 - val_loss: 2.2780 - val_accuracy: 0.6063\n",
            "Epoch 48/200\n",
            "177/180 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9615\n",
            "Epoch 48: val_accuracy did not improve from 0.77769\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9614 - val_loss: 2.0409 - val_accuracy: 0.6415\n",
            "46/46 [==============================] - 0s 733us/step - loss: 2.8387 - accuracy: 0.6794\n",
            "[0.9613644480705261, 0.6414670944213867, 0.6793892979621887]\n",
            "                                                  0   \\\n",
            "0  [0.9020525813102722, 0.6647493839263916, 0.664...   \n",
            "\n",
            "                                                  1   \\\n",
            "0  [0.8611738085746765, 0.7305389046669006, 0.845...   \n",
            "\n",
            "                                                  2   \\\n",
            "0  [0.9596552848815918, 0.7529493570327759, 0.802...   \n",
            "\n",
            "                                                  3   \\\n",
            "0  [0.8714439868927002, 0.6787439584732056, 0.666...   \n",
            "\n",
            "                                                  4   \\\n",
            "0  [0.9202454090118408, 0.8540419340133667, 0.846...   \n",
            "\n",
            "                                                  5   \\\n",
            "0  [0.946189820766449, 0.6370576024055481, 0.7198...   \n",
            "\n",
            "                                                  6   \\\n",
            "0  [0.9217456579208374, 0.8454106450080872, 0.698...   \n",
            "\n",
            "                                                  7   \\\n",
            "0  [0.9041007161140442, 0.8849630355834961, 0.867...   \n",
            "\n",
            "                                                  8   \\\n",
            "0  [0.960320234298706, 0.6655100584030151, 0.6175...   \n",
            "\n",
            "                                                  9   \\\n",
            "0  [0.9165687561035156, 0.8478260636329651, 0.696...   \n",
            "\n",
            "                                                  10  \\\n",
            "0  [0.955548107624054, 0.7674609422683716, 0.5968...   \n",
            "\n",
            "                                                  11  \n",
            "0  [0.9613644480705261, 0.6414670944213867, 0.679...  \n"
          ]
        }
      ],
      "source": [
        "# START CROSS-VALIDATION\n",
        "lstCsv = np.array(['N', 'NH', 'K', 'H'])\n",
        "lstAcc = []\n",
        "for test in lstCsv:\n",
        "    for validate in lstCsv[lstCsv != test]:\n",
        "        train = lstCsv[(lstCsv != test) & (lstCsv != validate)]\n",
        "        X_train = pd.concat([data[train[0]][0], data[train[1]][0]])\n",
        "        y1 = data[train[0]][1]\n",
        "        y2 = data[train[1]][1]\n",
        "        y_train = y1.tolist()\n",
        "        y_train.extend(y2.tolist())\n",
        "        y_train = np.array(y_train)\n",
        "        X_val, y_val, _ = data[validate]\n",
        "        X_test, y_test, df_test = data[test]\n",
        "\n",
        "        train_acc, val_acc, test_acc = trainModel(\n",
        "            X_train, y_train, X_val, y_val, X_test, y_test, class_names)\n",
        "            \n",
        "        lstAcc.append([train_acc, val_acc, test_acc])\n",
        "        print([train_acc, val_acc, test_acc])\n",
        "\n",
        "print(pd.DataFrame([lstAcc]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.9020525813102722, 0.6647493839263916, 0.6642512083053589],\n",
              " [0.8611738085746765, 0.7305389046669006, 0.8454106450080872],\n",
              " [0.9596552848815918, 0.7529493570327759, 0.8025362491607666],\n",
              " [0.8714439868927002, 0.6787439584732056, 0.6663927435874939],\n",
              " [0.9202454090118408, 0.8540419340133667, 0.846343457698822],\n",
              " [0.946189820766449, 0.6370576024055481, 0.7198027968406677],\n",
              " [0.9217456579208374, 0.8454106450080872, 0.6983532905578613],\n",
              " [0.9041007161140442, 0.8849630355834961, 0.867514967918396],\n",
              " [0.960320234298706, 0.6655100584030151, 0.617514967918396],\n",
              " [0.9165687561035156, 0.8478260636329651, 0.696044385433197],\n",
              " [0.955548107624054, 0.7674609422683716, 0.5968077778816223],\n",
              " [0.9613644480705261, 0.6414670944213867, 0.6793892979621887]]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstAcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.array(['K.csv' 'H.csv'])\n",
        "type(a) is np.ndarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "headers = []\n",
        "for test in lstCsv:\n",
        "    for validate in lstCsv[lstCsv != test]:\n",
        "        train = lstCsv[(lstCsv != test) & (lstCsv != validate)]\n",
        "\n",
        "        l = train[:].tolist()\n",
        "        l.append(validate)\n",
        "        l.append(test)\n",
        "        header = \"\\n\".join(l)\n",
        "        headers.append(header)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['K H NH N',\n",
              " 'NH H K N',\n",
              " 'NH K H N',\n",
              " 'K H N NH',\n",
              " 'N H K NH',\n",
              " 'N K H NH',\n",
              " 'NH H N K',\n",
              " 'N H NH K',\n",
              " 'N NH H K',\n",
              " 'NH K N H',\n",
              " 'N K NH H',\n",
              " 'N NH K H']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K H NH N</th>\n",
              "      <th>NH H K N</th>\n",
              "      <th>NH K H N</th>\n",
              "      <th>K H N NH</th>\n",
              "      <th>N H K NH</th>\n",
              "      <th>N K H NH</th>\n",
              "      <th>NH H N K</th>\n",
              "      <th>N H NH K</th>\n",
              "      <th>N NH H K</th>\n",
              "      <th>NH K N H</th>\n",
              "      <th>N K NH H</th>\n",
              "      <th>N NH K H</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>0.902053</td>\n",
              "      <td>0.861174</td>\n",
              "      <td>0.959655</td>\n",
              "      <td>0.871444</td>\n",
              "      <td>0.920245</td>\n",
              "      <td>0.946190</td>\n",
              "      <td>0.921746</td>\n",
              "      <td>0.904101</td>\n",
              "      <td>0.960320</td>\n",
              "      <td>0.916569</td>\n",
              "      <td>0.955548</td>\n",
              "      <td>0.961364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>validate</th>\n",
              "      <td>0.664749</td>\n",
              "      <td>0.730539</td>\n",
              "      <td>0.752949</td>\n",
              "      <td>0.678744</td>\n",
              "      <td>0.854042</td>\n",
              "      <td>0.637058</td>\n",
              "      <td>0.845411</td>\n",
              "      <td>0.884963</td>\n",
              "      <td>0.665510</td>\n",
              "      <td>0.847826</td>\n",
              "      <td>0.767461</td>\n",
              "      <td>0.641467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>0.664251</td>\n",
              "      <td>0.845411</td>\n",
              "      <td>0.802536</td>\n",
              "      <td>0.666393</td>\n",
              "      <td>0.846343</td>\n",
              "      <td>0.719803</td>\n",
              "      <td>0.698353</td>\n",
              "      <td>0.867515</td>\n",
              "      <td>0.617515</td>\n",
              "      <td>0.696044</td>\n",
              "      <td>0.596808</td>\n",
              "      <td>0.679389</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          K H NH N  NH H K N  NH K H N  K H N NH  N H K NH  N K H NH  \\\n",
              "train     0.902053  0.861174  0.959655  0.871444  0.920245  0.946190   \n",
              "validate  0.664749  0.730539  0.752949  0.678744  0.854042  0.637058   \n",
              "test      0.664251  0.845411  0.802536  0.666393  0.846343  0.719803   \n",
              "\n",
              "          NH H N K  N H NH K  N NH H K  NH K N H  N K NH H  N NH K H  \n",
              "train     0.921746  0.904101  0.960320  0.916569  0.955548  0.961364  \n",
              "validate  0.845411  0.884963  0.665510  0.847826  0.767461  0.641467  \n",
              "test      0.698353  0.867515  0.617515  0.696044  0.596808  0.679389  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(lstAcc, index=headers, columns=[\n",
        "                  'train', 'validate', 'test']).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(lstAcc)\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['TRAIN', 'VALIDATE', 'TEST'], loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from matplotlib.pyplot import figure\n",
        "# figure(figsize=(9, 5), dpi=80)\n",
        "plt.plot(df)\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Case')\n",
        "plt.text(-1.75, 0.39, 'train\\ntrain\\nvalidate\\ntest')\n",
        "plt.legend(['TRAIN', 'VALIDATE', 'TEST'], loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# set width of bar\n",
        "barWidth = 0.25\n",
        "fig = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "# set height of bar\n",
        "trainAcc = df['TRAIN']\n",
        "validateAcc = df['VALIDATE']\n",
        "testAcc = df['TEST']\n",
        "# Set position of bar on X axis\n",
        "br1 = np.arange(len(trainAcc))\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "\n",
        "# Make the plot\n",
        "plt.bar(br1, trainAcc, color='r', width=barWidth,\n",
        "        edgecolor='grey', label='TRAIN')\n",
        "plt.bar(br2, validateAcc, color='g', width=barWidth,\n",
        "        edgecolor='grey', label='VALIDATE')\n",
        "plt.bar(br3, testAcc, color='b', width=barWidth,\n",
        "        edgecolor='grey', label='TEST')\n",
        "\n",
        "# Adding Xticks\n",
        "plt.xlabel('Case', fontweight='bold', fontsize=15)\n",
        "plt.text(-0.9, -0.182, 'train\\ntrain\\nvalidate\\ntest')\n",
        "plt.ylabel('Accuracy', fontweight='bold', fontsize=15)\n",
        "plt.xticks([r + barWidth for r in range(len(trainAcc))],\n",
        "           df.index)\n",
        "plt.gca().set_yticks(np.linspace(0, 1, 11))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = pd.concat([X_N, X_H, X_NH, X_K])\n",
        "\n",
        "y = y_N.tolist()\n",
        "y2 = y_H.tolist()\n",
        "y3 = y_NH.tolist()\n",
        "y4 = y_K.tolist()\n",
        "\n",
        "y.extend(y2)\n",
        "y.extend(y3)\n",
        "y.extend(y4)\n",
        "\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 33)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               4352      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,063\n",
            "Trainable params: 13,063\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = tf.keras.Input(shape=(33))\n",
        "\n",
        "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(inputs)\n",
        "layer = keras.layers.Dropout(0.5)(layer)\n",
        "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
        "layer = keras.layers.Dropout(0.5)(layer)\n",
        "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "216/248 [=========================>....] - ETA: 0s - loss: 1.6026 - accuracy: 0.3666\n",
            "Epoch 1: val_accuracy improved from -inf to 0.76342, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 2s 4ms/step - loss: 1.5541 - accuracy: 0.3861 - val_loss: 0.9555 - val_accuracy: 0.7634\n",
            "Epoch 2/200\n",
            "221/248 [=========================>....] - ETA: 0s - loss: 1.0179 - accuracy: 0.6024\n",
            "Epoch 2: val_accuracy improved from 0.76342 to 0.84602, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 1.0067 - accuracy: 0.6078 - val_loss: 0.6206 - val_accuracy: 0.8460\n",
            "Epoch 3/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.7618 - accuracy: 0.7182\n",
            "Epoch 3: val_accuracy improved from 0.84602 to 0.89853, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.7574 - accuracy: 0.7201 - val_loss: 0.4119 - val_accuracy: 0.8985\n",
            "Epoch 4/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.5965 - accuracy: 0.7889\n",
            "Epoch 4: val_accuracy did not improve from 0.89853\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5937 - accuracy: 0.7914 - val_loss: 0.3349 - val_accuracy: 0.8903\n",
            "Epoch 5/200\n",
            "225/248 [==========================>...] - ETA: 0s - loss: 0.4873 - accuracy: 0.8333\n",
            "Epoch 5: val_accuracy improved from 0.89853 to 0.91150, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.8336 - val_loss: 0.2717 - val_accuracy: 0.9115\n",
            "Epoch 6/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.4462 - accuracy: 0.8487\n",
            "Epoch 6: val_accuracy did not improve from 0.91150\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.4440 - accuracy: 0.8493 - val_loss: 0.2524 - val_accuracy: 0.9097\n",
            "Epoch 7/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.3964 - accuracy: 0.8566\n",
            "Epoch 7: val_accuracy improved from 0.91150 to 0.92035, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3982 - accuracy: 0.8556 - val_loss: 0.2225 - val_accuracy: 0.9204\n",
            "Epoch 8/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.3455 - accuracy: 0.8810\n",
            "Epoch 8: val_accuracy improved from 0.92035 to 0.93274, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8809 - val_loss: 0.1916 - val_accuracy: 0.9327\n",
            "Epoch 9/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8827\n",
            "Epoch 9: val_accuracy improved from 0.93274 to 0.93392, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3428 - accuracy: 0.8827 - val_loss: 0.1783 - val_accuracy: 0.9339\n",
            "Epoch 10/200\n",
            "234/248 [===========================>..] - ETA: 0s - loss: 0.3155 - accuracy: 0.8913\n",
            "Epoch 10: val_accuracy improved from 0.93392 to 0.93451, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3115 - accuracy: 0.8928 - val_loss: 0.1666 - val_accuracy: 0.9345\n",
            "Epoch 11/200\n",
            "222/248 [=========================>....] - ETA: 0s - loss: 0.3002 - accuracy: 0.8919\n",
            "Epoch 11: val_accuracy did not improve from 0.93451\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2954 - accuracy: 0.8938 - val_loss: 0.1701 - val_accuracy: 0.9221\n",
            "Epoch 12/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2675 - accuracy: 0.9065\n",
            "Epoch 12: val_accuracy improved from 0.93451 to 0.94218, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.9059 - val_loss: 0.1406 - val_accuracy: 0.9422\n",
            "Epoch 13/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9052\n",
            "Epoch 13: val_accuracy improved from 0.94218 to 0.95103, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2663 - accuracy: 0.9052 - val_loss: 0.1404 - val_accuracy: 0.9510\n",
            "Epoch 14/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.2538 - accuracy: 0.9134\n",
            "Epoch 14: val_accuracy improved from 0.95103 to 0.95929, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2564 - accuracy: 0.9128 - val_loss: 0.1354 - val_accuracy: 0.9593\n",
            "Epoch 15/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.2469 - accuracy: 0.9061\n",
            "Epoch 15: val_accuracy improved from 0.95929 to 0.95988, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2431 - accuracy: 0.9090 - val_loss: 0.1200 - val_accuracy: 0.9599\n",
            "Epoch 16/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9147\n",
            "Epoch 16: val_accuracy did not improve from 0.95988\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2441 - accuracy: 0.9148 - val_loss: 0.1206 - val_accuracy: 0.9528\n",
            "Epoch 17/200\n",
            "221/248 [=========================>....] - ETA: 0s - loss: 0.2434 - accuracy: 0.9146\n",
            "Epoch 17: val_accuracy did not improve from 0.95988\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2393 - accuracy: 0.9176 - val_loss: 0.1287 - val_accuracy: 0.9463\n",
            "Epoch 18/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.2209 - accuracy: 0.9241\n",
            "Epoch 18: val_accuracy improved from 0.95988 to 0.96047, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2195 - accuracy: 0.9244 - val_loss: 0.1113 - val_accuracy: 0.9605\n",
            "Epoch 19/200\n",
            "223/248 [=========================>....] - ETA: 0s - loss: 0.2214 - accuracy: 0.9257\n",
            "Epoch 19: val_accuracy improved from 0.96047 to 0.96637, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2179 - accuracy: 0.9269 - val_loss: 0.0999 - val_accuracy: 0.9664\n",
            "Epoch 20/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1962 - accuracy: 0.9322\n",
            "Epoch 20: val_accuracy did not improve from 0.96637\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1973 - accuracy: 0.9317 - val_loss: 0.0964 - val_accuracy: 0.9640\n",
            "Epoch 21/200\n",
            "219/248 [=========================>....] - ETA: 0s - loss: 0.1972 - accuracy: 0.9275\n",
            "Epoch 21: val_accuracy improved from 0.96637 to 0.97345, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1933 - accuracy: 0.9302 - val_loss: 0.0927 - val_accuracy: 0.9735\n",
            "Epoch 22/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1996 - accuracy: 0.9325\n",
            "Epoch 22: val_accuracy did not improve from 0.97345\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2008 - accuracy: 0.9327 - val_loss: 0.0915 - val_accuracy: 0.9652\n",
            "Epoch 23/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9281\n",
            "Epoch 23: val_accuracy did not improve from 0.97345\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2021 - accuracy: 0.9282 - val_loss: 0.0854 - val_accuracy: 0.9705\n",
            "Epoch 24/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1923 - accuracy: 0.9312\n",
            "Epoch 24: val_accuracy did not improve from 0.97345\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9320 - val_loss: 0.0902 - val_accuracy: 0.9693\n",
            "Epoch 25/200\n",
            "215/248 [=========================>....] - ETA: 0s - loss: 0.1894 - accuracy: 0.9378\n",
            "Epoch 25: val_accuracy improved from 0.97345 to 0.97581, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9383 - val_loss: 0.0832 - val_accuracy: 0.9758\n",
            "Epoch 26/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.1739 - accuracy: 0.9402\n",
            "Epoch 26: val_accuracy did not improve from 0.97581\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.9393 - val_loss: 0.0983 - val_accuracy: 0.9617\n",
            "Epoch 27/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1734 - accuracy: 0.9443\n",
            "Epoch 27: val_accuracy did not improve from 0.97581\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9418 - val_loss: 0.0788 - val_accuracy: 0.9740\n",
            "Epoch 28/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1787 - accuracy: 0.9421\n",
            "Epoch 28: val_accuracy improved from 0.97581 to 0.97817, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1772 - accuracy: 0.9426 - val_loss: 0.0750 - val_accuracy: 0.9782\n",
            "Epoch 29/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1747 - accuracy: 0.9400\n",
            "Epoch 29: val_accuracy improved from 0.97817 to 0.97994, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1734 - accuracy: 0.9401 - val_loss: 0.0683 - val_accuracy: 0.9799\n",
            "Epoch 30/200\n",
            "220/248 [=========================>....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9449\n",
            "Epoch 30: val_accuracy did not improve from 0.97994\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1745 - accuracy: 0.9464 - val_loss: 0.0720 - val_accuracy: 0.9752\n",
            "Epoch 31/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.1682 - accuracy: 0.9456\n",
            "Epoch 31: val_accuracy did not improve from 0.97994\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1663 - accuracy: 0.9459 - val_loss: 0.0678 - val_accuracy: 0.9776\n",
            "Epoch 32/200\n",
            "210/248 [========================>.....] - ETA: 0s - loss: 0.1575 - accuracy: 0.9476\n",
            "Epoch 32: val_accuracy did not improve from 0.97994\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1604 - accuracy: 0.9464 - val_loss: 0.0739 - val_accuracy: 0.9740\n",
            "Epoch 33/200\n",
            "234/248 [===========================>..] - ETA: 0s - loss: 0.1631 - accuracy: 0.9458\n",
            "Epoch 33: val_accuracy did not improve from 0.97994\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1605 - accuracy: 0.9469 - val_loss: 0.0705 - val_accuracy: 0.9764\n",
            "Epoch 34/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 0.9497\n",
            "Epoch 34: val_accuracy improved from 0.97994 to 0.98289, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9494 - val_loss: 0.0663 - val_accuracy: 0.9829\n",
            "Epoch 35/200\n",
            "219/248 [=========================>....] - ETA: 0s - loss: 0.1609 - accuracy: 0.9446\n",
            "Epoch 35: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1632 - accuracy: 0.9444 - val_loss: 0.0668 - val_accuracy: 0.9746\n",
            "Epoch 36/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.1517 - accuracy: 0.9480\n",
            "Epoch 36: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9494 - val_loss: 0.0801 - val_accuracy: 0.9664\n",
            "Epoch 37/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1587 - accuracy: 0.9457\n",
            "Epoch 37: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1563 - accuracy: 0.9464 - val_loss: 0.0636 - val_accuracy: 0.9782\n",
            "Epoch 38/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9497\n",
            "Epoch 38: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9502 - val_loss: 0.0639 - val_accuracy: 0.9817\n",
            "Epoch 39/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9563\n",
            "Epoch 39: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1316 - accuracy: 0.9563 - val_loss: 0.0683 - val_accuracy: 0.9764\n",
            "Epoch 40/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1490 - accuracy: 0.9512\n",
            "Epoch 40: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1497 - accuracy: 0.9507 - val_loss: 0.0655 - val_accuracy: 0.9817\n",
            "Epoch 41/200\n",
            "225/248 [==========================>...] - ETA: 0s - loss: 0.1447 - accuracy: 0.9561\n",
            "Epoch 41: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1470 - accuracy: 0.9545 - val_loss: 0.0699 - val_accuracy: 0.9782\n",
            "Epoch 42/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1457 - accuracy: 0.9537\n",
            "Epoch 42: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1472 - accuracy: 0.9535 - val_loss: 0.0643 - val_accuracy: 0.9723\n",
            "Epoch 43/200\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.1481 - accuracy: 0.9508\n",
            "Epoch 43: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1464 - accuracy: 0.9509 - val_loss: 0.0653 - val_accuracy: 0.9782\n",
            "Epoch 44/200\n",
            "221/248 [=========================>....] - ETA: 0s - loss: 0.1421 - accuracy: 0.9548\n",
            "Epoch 44: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1469 - accuracy: 0.9540 - val_loss: 0.0580 - val_accuracy: 0.9805\n",
            "Epoch 45/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9521\n",
            "Epoch 45: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1437 - accuracy: 0.9522 - val_loss: 0.0565 - val_accuracy: 0.9829\n",
            "Epoch 46/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1302 - accuracy: 0.9561\n",
            "Epoch 46: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1308 - accuracy: 0.9560 - val_loss: 0.0740 - val_accuracy: 0.9740\n",
            "Epoch 47/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.9511\n",
            "Epoch 47: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1436 - accuracy: 0.9509 - val_loss: 0.0572 - val_accuracy: 0.9788\n",
            "Epoch 48/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9495\n",
            "Epoch 48: val_accuracy did not improve from 0.98289\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1363 - accuracy: 0.9497 - val_loss: 0.0674 - val_accuracy: 0.9752\n",
            "Epoch 49/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1384 - accuracy: 0.9559\n",
            "Epoch 49: val_accuracy improved from 0.98289 to 0.98525, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1349 - accuracy: 0.9573 - val_loss: 0.0490 - val_accuracy: 0.9853\n",
            "Epoch 50/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1346 - accuracy: 0.9548\n",
            "Epoch 50: val_accuracy did not improve from 0.98525\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1332 - accuracy: 0.9552 - val_loss: 0.0510 - val_accuracy: 0.9811\n",
            "Epoch 51/200\n",
            "207/248 [========================>.....] - ETA: 0s - loss: 0.1214 - accuracy: 0.9583\n",
            "Epoch 51: val_accuracy did not improve from 0.98525\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9585 - val_loss: 0.0505 - val_accuracy: 0.9841\n",
            "Epoch 52/200\n",
            "222/248 [=========================>....] - ETA: 0s - loss: 0.1323 - accuracy: 0.9561\n",
            "Epoch 52: val_accuracy did not improve from 0.98525\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1374 - accuracy: 0.9537 - val_loss: 0.0668 - val_accuracy: 0.9711\n",
            "Epoch 53/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1290 - accuracy: 0.9566\n",
            "Epoch 53: val_accuracy did not improve from 0.98525\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9575 - val_loss: 0.0477 - val_accuracy: 0.9853\n",
            "Epoch 54/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9500\n",
            "Epoch 54: val_accuracy did not improve from 0.98525\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9502 - val_loss: 0.0555 - val_accuracy: 0.9782\n",
            "Epoch 55/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9595\n",
            "Epoch 55: val_accuracy did not improve from 0.98525\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1304 - accuracy: 0.9595 - val_loss: 0.0613 - val_accuracy: 0.9829\n",
            "Epoch 56/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1349 - accuracy: 0.9545\n",
            "Epoch 56: val_accuracy did not improve from 0.98525\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1346 - accuracy: 0.9547 - val_loss: 0.0568 - val_accuracy: 0.9817\n",
            "Epoch 57/200\n",
            "211/248 [========================>.....] - ETA: 0s - loss: 0.1259 - accuracy: 0.9591\n",
            "Epoch 57: val_accuracy improved from 0.98525 to 0.98584, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9603 - val_loss: 0.0434 - val_accuracy: 0.9858\n",
            "Epoch 58/200\n",
            "216/248 [=========================>....] - ETA: 0s - loss: 0.1281 - accuracy: 0.9578\n",
            "Epoch 58: val_accuracy did not improve from 0.98584\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1300 - accuracy: 0.9578 - val_loss: 0.0415 - val_accuracy: 0.9858\n",
            "Epoch 59/200\n",
            "216/248 [=========================>....] - ETA: 0s - loss: 0.1179 - accuracy: 0.9580\n",
            "Epoch 59: val_accuracy improved from 0.98584 to 0.98761, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1212 - accuracy: 0.9570 - val_loss: 0.0497 - val_accuracy: 0.9876\n",
            "Epoch 60/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1186 - accuracy: 0.9611\n",
            "Epoch 60: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1188 - accuracy: 0.9606 - val_loss: 0.0468 - val_accuracy: 0.9829\n",
            "Epoch 61/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9604\n",
            "Epoch 61: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1253 - accuracy: 0.9603 - val_loss: 0.0492 - val_accuracy: 0.9835\n",
            "Epoch 62/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1131 - accuracy: 0.9637\n",
            "Epoch 62: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1137 - accuracy: 0.9636 - val_loss: 0.0449 - val_accuracy: 0.9853\n",
            "Epoch 63/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9559\n",
            "Epoch 63: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1299 - accuracy: 0.9560 - val_loss: 0.0459 - val_accuracy: 0.9847\n",
            "Epoch 64/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9566\n",
            "Epoch 64: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1348 - accuracy: 0.9565 - val_loss: 0.0498 - val_accuracy: 0.9841\n",
            "Epoch 65/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.1159 - accuracy: 0.9629\n",
            "Epoch 65: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1157 - accuracy: 0.9633 - val_loss: 0.0413 - val_accuracy: 0.9870\n",
            "Epoch 66/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9585\n",
            "Epoch 66: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1155 - accuracy: 0.9578 - val_loss: 0.0450 - val_accuracy: 0.9847\n",
            "Epoch 67/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9582\n",
            "Epoch 67: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.9583 - val_loss: 0.0447 - val_accuracy: 0.9858\n",
            "Epoch 68/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9638\n",
            "Epoch 68: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1149 - accuracy: 0.9638 - val_loss: 0.0474 - val_accuracy: 0.9864\n",
            "Epoch 69/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.1155 - accuracy: 0.9622\n",
            "Epoch 69: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1177 - accuracy: 0.9621 - val_loss: 0.0456 - val_accuracy: 0.9841\n",
            "Epoch 70/200\n",
            "221/248 [=========================>....] - ETA: 0s - loss: 0.1161 - accuracy: 0.9601\n",
            "Epoch 70: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1175 - accuracy: 0.9613 - val_loss: 0.0512 - val_accuracy: 0.9811\n",
            "Epoch 71/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.1078 - accuracy: 0.9654\n",
            "Epoch 71: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1050 - accuracy: 0.9664 - val_loss: 0.0389 - val_accuracy: 0.9858\n",
            "Epoch 72/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9649\n",
            "Epoch 72: val_accuracy did not improve from 0.98761\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1134 - accuracy: 0.9649 - val_loss: 0.0466 - val_accuracy: 0.9841\n",
            "Epoch 73/200\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.1254 - accuracy: 0.9599\n",
            "Epoch 73: val_accuracy improved from 0.98761 to 0.98820, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1240 - accuracy: 0.9601 - val_loss: 0.0381 - val_accuracy: 0.9882\n",
            "Epoch 74/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9664\n",
            "Epoch 74: val_accuracy improved from 0.98820 to 0.98997, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1084 - accuracy: 0.9661 - val_loss: 0.0359 - val_accuracy: 0.9900\n",
            "Epoch 75/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1238 - accuracy: 0.9589\n",
            "Epoch 75: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1211 - accuracy: 0.9601 - val_loss: 0.0435 - val_accuracy: 0.9841\n",
            "Epoch 76/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9651\n",
            "Epoch 76: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1147 - accuracy: 0.9651 - val_loss: 0.0609 - val_accuracy: 0.9799\n",
            "Epoch 77/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1128 - accuracy: 0.9620\n",
            "Epoch 77: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1110 - accuracy: 0.9626 - val_loss: 0.0476 - val_accuracy: 0.9811\n",
            "Epoch 78/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1178 - accuracy: 0.9593\n",
            "Epoch 78: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1171 - accuracy: 0.9590 - val_loss: 0.0368 - val_accuracy: 0.9864\n",
            "Epoch 79/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9612\n",
            "Epoch 79: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9616 - val_loss: 0.0434 - val_accuracy: 0.9870\n",
            "Epoch 80/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1056 - accuracy: 0.9633\n",
            "Epoch 80: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1074 - accuracy: 0.9623 - val_loss: 0.0482 - val_accuracy: 0.9847\n",
            "Epoch 81/200\n",
            "221/248 [=========================>....] - ETA: 0s - loss: 0.1109 - accuracy: 0.9615\n",
            "Epoch 81: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1082 - accuracy: 0.9626 - val_loss: 0.0480 - val_accuracy: 0.9841\n",
            "Epoch 82/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9678\n",
            "Epoch 82: val_accuracy did not improve from 0.98997\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1098 - accuracy: 0.9684 - val_loss: 0.0422 - val_accuracy: 0.9864\n",
            "Epoch 83/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1185 - accuracy: 0.9626\n",
            "Epoch 83: val_accuracy improved from 0.98997 to 0.99115, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1166 - accuracy: 0.9623 - val_loss: 0.0346 - val_accuracy: 0.9912\n",
            "Epoch 84/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.1333 - accuracy: 0.9546\n",
            "Epoch 84: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1302 - accuracy: 0.9563 - val_loss: 0.0432 - val_accuracy: 0.9876\n",
            "Epoch 85/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1161 - accuracy: 0.9627\n",
            "Epoch 85: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1151 - accuracy: 0.9633 - val_loss: 0.0380 - val_accuracy: 0.9864\n",
            "Epoch 86/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9622\n",
            "Epoch 86: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1100 - accuracy: 0.9626 - val_loss: 0.0413 - val_accuracy: 0.9853\n",
            "Epoch 87/200\n",
            "220/248 [=========================>....] - ETA: 0s - loss: 0.1018 - accuracy: 0.9682\n",
            "Epoch 87: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.0987 - accuracy: 0.9694 - val_loss: 0.0330 - val_accuracy: 0.9894\n",
            "Epoch 88/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.1157 - accuracy: 0.9624\n",
            "Epoch 88: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9623 - val_loss: 0.0380 - val_accuracy: 0.9876\n",
            "Epoch 89/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.1144 - accuracy: 0.9607\n",
            "Epoch 89: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1140 - accuracy: 0.9608 - val_loss: 0.0420 - val_accuracy: 0.9864\n",
            "Epoch 90/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1129 - accuracy: 0.9642\n",
            "Epoch 90: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1116 - accuracy: 0.9646 - val_loss: 0.0450 - val_accuracy: 0.9853\n",
            "Epoch 91/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9628\n",
            "Epoch 91: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1085 - accuracy: 0.9628 - val_loss: 0.0360 - val_accuracy: 0.9882\n",
            "Epoch 92/200\n",
            "222/248 [=========================>....] - ETA: 0s - loss: 0.1038 - accuracy: 0.9659\n",
            "Epoch 92: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1068 - accuracy: 0.9659 - val_loss: 0.0395 - val_accuracy: 0.9870\n",
            "Epoch 93/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9623\n",
            "Epoch 93: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1168 - accuracy: 0.9623 - val_loss: 0.0391 - val_accuracy: 0.9841\n",
            "Epoch 94/200\n",
            "217/248 [=========================>....] - ETA: 0s - loss: 0.1165 - accuracy: 0.9617\n",
            "Epoch 94: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1169 - accuracy: 0.9613 - val_loss: 0.0481 - val_accuracy: 0.9858\n",
            "Epoch 95/200\n",
            "220/248 [=========================>....] - ETA: 0s - loss: 0.1129 - accuracy: 0.9571\n",
            "Epoch 95: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9575 - val_loss: 0.0401 - val_accuracy: 0.9841\n",
            "Epoch 96/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1023 - accuracy: 0.9661\n",
            "Epoch 96: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9661 - val_loss: 0.0407 - val_accuracy: 0.9864\n",
            "Epoch 97/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.0999 - accuracy: 0.9674\n",
            "Epoch 97: val_accuracy did not improve from 0.99115\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1044 - accuracy: 0.9666 - val_loss: 0.0574 - val_accuracy: 0.9858\n",
            "Epoch 98/200\n",
            "222/248 [=========================>....] - ETA: 0s - loss: 0.1134 - accuracy: 0.9611\n",
            "Epoch 98: val_accuracy improved from 0.99115 to 0.99233, saving model to weights.best.hdf5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1191 - accuracy: 0.9588 - val_loss: 0.0410 - val_accuracy: 0.9923\n",
            "Epoch 99/200\n",
            "217/248 [=========================>....] - ETA: 0s - loss: 0.1008 - accuracy: 0.9677\n",
            "Epoch 99: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1015 - accuracy: 0.9669 - val_loss: 0.0467 - val_accuracy: 0.9817\n",
            "Epoch 100/200\n",
            "215/248 [=========================>....] - ETA: 0s - loss: 0.1095 - accuracy: 0.9669\n",
            "Epoch 100: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1118 - accuracy: 0.9664 - val_loss: 0.0342 - val_accuracy: 0.9917\n",
            "Epoch 101/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.0989 - accuracy: 0.9691\n",
            "Epoch 101: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.0978 - accuracy: 0.9692 - val_loss: 0.0360 - val_accuracy: 0.9876\n",
            "Epoch 102/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9675\n",
            "Epoch 102: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1040 - accuracy: 0.9676 - val_loss: 0.0409 - val_accuracy: 0.9847\n",
            "Epoch 103/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1088 - accuracy: 0.9658\n",
            "Epoch 103: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9664 - val_loss: 0.0405 - val_accuracy: 0.9853\n",
            "Epoch 104/200\n",
            "214/248 [========================>.....] - ETA: 0s - loss: 0.0965 - accuracy: 0.9685\n",
            "Epoch 104: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.0964 - accuracy: 0.9666 - val_loss: 0.0351 - val_accuracy: 0.9888\n",
            "Epoch 105/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1013 - accuracy: 0.9647\n",
            "Epoch 105: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1025 - accuracy: 0.9646 - val_loss: 0.0498 - val_accuracy: 0.9829\n",
            "Epoch 106/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.0887 - accuracy: 0.9695\n",
            "Epoch 106: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9692 - val_loss: 0.0288 - val_accuracy: 0.9917\n",
            "Epoch 107/200\n",
            "209/248 [========================>.....] - ETA: 0s - loss: 0.0975 - accuracy: 0.9674\n",
            "Epoch 107: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.0984 - accuracy: 0.9664 - val_loss: 0.0312 - val_accuracy: 0.9906\n",
            "Epoch 108/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1179 - accuracy: 0.9637\n",
            "Epoch 108: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1158 - accuracy: 0.9646 - val_loss: 0.0432 - val_accuracy: 0.9841\n",
            "Epoch 109/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1014 - accuracy: 0.9668\n",
            "Epoch 109: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.0992 - accuracy: 0.9669 - val_loss: 0.0345 - val_accuracy: 0.9858\n",
            "Epoch 110/200\n",
            "220/248 [=========================>....] - ETA: 0s - loss: 0.0995 - accuracy: 0.9685\n",
            "Epoch 110: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.0964 - accuracy: 0.9694 - val_loss: 0.0370 - val_accuracy: 0.9847\n",
            "Epoch 111/200\n",
            "218/248 [=========================>....] - ETA: 0s - loss: 0.0958 - accuracy: 0.9690\n",
            "Epoch 111: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1037 - accuracy: 0.9659 - val_loss: 0.0475 - val_accuracy: 0.9847\n",
            "Epoch 112/200\n",
            "212/248 [========================>.....] - ETA: 0s - loss: 0.1039 - accuracy: 0.9640\n",
            "Epoch 112: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.0995 - accuracy: 0.9656 - val_loss: 0.0327 - val_accuracy: 0.9864\n",
            "Epoch 113/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9642\n",
            "Epoch 113: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1058 - accuracy: 0.9646 - val_loss: 0.0408 - val_accuracy: 0.9882\n",
            "Epoch 114/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1176 - accuracy: 0.9642\n",
            "Epoch 114: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1202 - accuracy: 0.9623 - val_loss: 0.0358 - val_accuracy: 0.9876\n",
            "Epoch 115/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1025 - accuracy: 0.9642\n",
            "Epoch 115: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1032 - accuracy: 0.9638 - val_loss: 0.0399 - val_accuracy: 0.9864\n",
            "Epoch 116/200\n",
            "205/248 [=======================>......] - ETA: 0s - loss: 0.1045 - accuracy: 0.9637\n",
            "Epoch 116: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1019 - accuracy: 0.9651 - val_loss: 0.0389 - val_accuracy: 0.9876\n",
            "Epoch 117/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1094 - accuracy: 0.9642\n",
            "Epoch 117: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1124 - accuracy: 0.9638 - val_loss: 0.0375 - val_accuracy: 0.9864\n",
            "Epoch 118/200\n",
            "217/248 [=========================>....] - ETA: 0s - loss: 0.0964 - accuracy: 0.9666\n",
            "Epoch 118: val_accuracy did not improve from 0.99233\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.0953 - accuracy: 0.9664 - val_loss: 0.0329 - val_accuracy: 0.9894\n"
          ]
        }
      ],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "checkpoint_path = \"weights.best.hdf5\"\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                             monitor='val_accuracy',\n",
        "                                             verbose=1,\n",
        "                                             save_best_only=True,\n",
        "                                             mode='max')\n",
        "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                              patience=20)\n",
        "\n",
        "# Start training\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=200,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[checkpoint, earlystopping])          # ';' discards the output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.9869\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate the model using the TEST dataset\n",
        "loss, accuracy = model.evaluate(X_NH, y_NH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 0s 907us/step - loss: 0.0739 - accuracy: 0.9771\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_H, y_H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9940\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_N, y_N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.9963\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_K, y_K)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Part 3: Convert the pose classification model to TensorFlow Lite**\n",
        "\n",
        "You'll convert the Keras pose classification model to the TensorFlow Lite format so that you can deploy it to mobile apps, web browsers and edge devices. When converting the model, you'll apply [dynamic range quantization](https://www.tensorflow.org/lite/performance/post_training_quant) to reduce the pose classification TensorFlow Lite model size by about 4 times with insignificant accuracy loss.\n",
        "\n",
        "Note: TensorFlow Lite supports multiple quantization schemes. See the [documentation](https://www.tensorflow.org/lite/performance/model_optimization) if you are interested to learn more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\minhn\\AppData\\Local\\Temp\\tmp1b_u7hj9\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\minhn\\AppData\\Local\\Temp\\tmp1b_u7hj9\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 16KB\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "print('Model size: %dKB' % (len(tflite_model) / 1024))\n",
        "\n",
        "with open('pose_classifier.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then you'll write the label file which contains mapping from the class indexes to the human readable class names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('pose_labels.txt', 'w') as f:\n",
        "  f.write('\\n'.join(class_names))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you've applied quantization to reduce the model size, let's evaluate the quantized TFLite model to check whether the accuracy drop is acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of TFLite model: 0.9887905604719764\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(interpreter, X, y_true):\n",
        "  \"\"\"Evaluates the given TFLite model and return its accuracy.\"\"\"\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on all given poses.\n",
        "  y_pred = []\n",
        "  for i in range(len(y_true)):\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = X[i: i + 1].astype('float32')\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the class with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    predicted_label = np.argmax(output()[0])\n",
        "    y_pred.append(predicted_label)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  y_pred = keras.utils.to_categorical(y_pred)\n",
        "  return accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Evaluate the accuracy of the converted TFLite model\n",
        "classifier_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "classifier_interpreter.allocate_tensors()\n",
        "print('Accuracy of TFLite model: %s' %\n",
        "      evaluate_model(classifier_interpreter, X_test, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IfQ3xP6-EY5r",
        "Jpy4A1Vpi9jH",
        "dH_yWnJ9QRLs",
        "aN-W7SEpnLoe",
        "pNiIeG5PnUHm",
        "dVn3fgInndqY",
        "YvA67LZDnijz",
        "L24GWhgo4WAl",
        "TJXSR2CQhm-z",
        "BoXaN5nUeU8d",
        "vxOkXvm-TvOZ",
        "cQtgAeHVT0UE",
        "UevEKViRT_6J",
        "E2D1czPJazvb",
        "iGMcoSwLwRSD",
        "ydb-bd_UWXMq",
        "PI7Wb3Bagau3",
        "JPnPmwjn9452",
        "uhY0VeDkFK7W"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
